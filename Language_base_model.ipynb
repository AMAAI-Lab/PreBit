{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Language base model",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOkiPpC8hdgJw7kj7OG5KLf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YanzhaoZ/PreBit/blob/main/Language_base_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87zKeMTmYZVM",
        "outputId": "1373ee79-99be-4251-976f-d4ad38e20fa5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import pandas as pd\n",
        "import glob\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('words')\n",
        "\n",
        "!pip install yfinance\n",
        "import yfinance as yf\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n",
            "Collecting yfinance\n",
            "  Downloading https://files.pythonhosted.org/packages/7a/e8/b9d7104d3a4bf39924799067592d9e59119fcfc900a425a12e80a3123ec8/yfinance-0.1.55.tar.gz\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.6/dist-packages (from yfinance) (1.1.4)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.6/dist-packages (from yfinance) (1.18.5)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.6/dist-packages (from yfinance) (2.23.0)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from yfinance) (0.0.9)\n",
            "Collecting lxml>=4.5.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/64/28/0b761b64ecbd63d272ed0e7a6ae6e4402fc37886b59181bfdf274424d693/lxml-4.6.1-cp36-cp36m-manylinux1_x86_64.whl (5.5MB)\n",
            "\u001b[K     |████████████████████████████████| 5.5MB 12.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.24->yfinance) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.24->yfinance) (2.8.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20->yfinance) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20->yfinance) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20->yfinance) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20->yfinance) (1.24.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.7.3->pandas>=0.24->yfinance) (1.15.0)\n",
            "Building wheels for collected packages: yfinance\n",
            "  Building wheel for yfinance (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for yfinance: filename=yfinance-0.1.55-py2.py3-none-any.whl size=22618 sha256=05517c76565814a57abda609273d62d01ca2c424b564c7b60715ed291508e30d\n",
            "  Stored in directory: /root/.cache/pip/wheels/04/98/cc/2702a4242d60bdc14f48b4557c427ded1fe92aedf257d4565c\n",
            "Successfully built yfinance\n",
            "Installing collected packages: lxml, yfinance\n",
            "  Found existing installation: lxml 4.2.6\n",
            "    Uninstalling lxml-4.2.6:\n",
            "      Successfully uninstalled lxml-4.2.6\n",
            "Successfully installed lxml-4.6.1 yfinance-0.1.55\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32tjIeFcYi8e"
      },
      "source": [
        "# Loading in data "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_107ymtYqLF",
        "outputId": "6cd7e88b-d109-455b-89b5-c46927417adb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aF9y_Fj5bCV7"
      },
      "source": [
        "#now that I have the whole string for the day, I need to do processing first, before spliting them into chunks. \n",
        "words = set(nltk.corpus.words.words())\n",
        "#I need to add in some crypto currency specific words. like Etherum.\n",
        "#bascailly all the symbols of crypto currencies, I should add a set here. \n",
        "cryptowords=(['crypto','cryptocurrency','bitcoin','btc','etherum','eos','eth','xrp','ltc'])\n",
        "words.update(cryptowords)\n",
        "\n",
        "exclusionli=['up','so','if','go'] #keep in view\n",
        "\n",
        "\n",
        "def processTweet(tweetFeed):\n",
        "    tweetFeed = tweetFeed.lower()\n",
        "    #Convert www.* or https?://* pic.twitter.com* to space\n",
        "    tweetFeed = re.sub('(https:\\/\\/www\\.[\\s][\\a-zA-Z]*)|(http:\\/\\/www\\.[\\s][\\a-zA-Z]*)|(https:\\/\\/[\\s][\\a-zA-Z]*)|(http:\\/\\/[\\s][\\a-zA-Z]*)|(www\\.[\\s][\\a-zA-Z]*)|(https:\\/\\/[\\a-zA-Z]*)|(http:\\/\\/[\\a-zA-Z]*)',' ',tweetFeed)\n",
        "    tweetFeed = re.sub('pic\\.[^\\s]*',' ',tweetFeed)\n",
        "    #removw @sth and #sth\n",
        "    tweetFeed = re.sub('(@\\ [^\\s]*)|(#\\ [^\\s]*)', ' ', tweetFeed)\n",
        "    #remove rt\n",
        "    tweetFeed = re.sub('rt[\\s]','',tweetFeed)\n",
        "\n",
        "    tweetFeed = \" \".join(w for w in nltk.wordpunct_tokenize(tweetFeed) if w.lower() in words)# or not w.isalpha())#this part takes in number as True.\n",
        "    #I'm not so sure about this language filter, but let's just roll with it first. \n",
        "    tweetFeed = re.sub(\"[^A-Za-z']+\", ' ', tweetFeed)\n",
        "\n",
        "    #filter out all the token left with len of 1. possibaly 2, defind a exclusion list: up, if, so, \n",
        "    tweetFeed = \" \".join(w for w in nltk.wordpunct_tokenize(tweetFeed) if len(w) >1 )\n",
        "    return tweetFeed\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fuaaoRnkIeKA"
      },
      "source": [
        "#now let's try to split the text into smaller chunks, which is just taking the tutorial's code\n",
        "def get_split(text1,length=200, overlap=50):\n",
        "  l_total = []\n",
        "  l_parcial = []\n",
        "  if len(text1.split())//(length-overlap) >0:\n",
        "    n = len(text1.split())//(length-overlap)\n",
        "  else: \n",
        "    n = 1\n",
        "  for w in range(n):\n",
        "    if w == 0:\n",
        "      l_parcial = text1.split()[:length]\n",
        "      l_total.append(\" \".join(l_parcial))\n",
        "    else:\n",
        "      l_parcial = text1.split()[w*(length-overlap):w*(length-overlap) + length]\n",
        "      l_total.append(\" \".join(l_parcial))\n",
        "  return l_total"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Hzv5dJJL73E"
      },
      "source": [
        "#next, tie them up, read in csv, combine into chunks, take the dates as the price? \n",
        "path = '/content/gdrive/My Drive/Crypto/tweets2019'\n",
        "all_files = glob.glob(path+'/*.csv')\n",
        "\n",
        "#let me try to use list1 to store string and list 2 to store dates\n",
        "li1=[]\n",
        "li2=[]\n",
        "for file in all_files:\n",
        "  df=pd.read_csv(file,sep=',',index_col=0)\n",
        "  df=df['text'].to_frame()\n",
        "  rawtxt=[]\n",
        "  for row in df.text:\n",
        "    rawtxt.append(row)\n",
        "  string = \" \".join(rawtxt)\n",
        "\n",
        "  date= file[-14:-4]\n",
        "  \n",
        "  li1.append(string)\n",
        "  li2.append(date)\n",
        "\n",
        "data=pd.DataFrame({'date':li2, 'text':li1})\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2pWPWDYMs1R",
        "outputId": "9f3ef1b7-5a78-40bb-d1b0-0dda106a4540",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#now, let's process the text with the pre-defined functin. and perhaps add in the label by looking at the price. \n",
        "data['clean_txt']=data['text'].apply(processTweet)\n",
        "\n",
        "#A note for the price, file date is the day after all the tweets inside. So the label should be created, as such that it reflex:\n",
        "#file date over previous day change. \n",
        "#What index should I use/should I use my own file (maybe not, too big/detailed for daily ticker). Let's go with Yahoofinance.\n",
        "#now, let's create the price change label, return diff, log return diff, and binary label \n",
        "start_date ='2018-12-28'\n",
        "end_date = '2019-04-02'\n",
        "price = yf.download(\"BTC-USD\", start=start_date, end=end_date)\n",
        "price_2= price.Close.to_frame()\n",
        "price['change']=price_2.apply(lambda x: x/x.shift(1)-1)\n",
        "price['log_change']=price_2.apply(lambda x: np.log(x)-np.log(x.shift(1)))\n",
        "price['change_label']=price['change'].apply (lambda x: x>0)\n",
        "price=price.reset_index()\n",
        "price['date']=price.Date.apply (lambda x : str(x)[:10])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UKo8j3GZMt49"
      },
      "source": [
        "#price.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BL6PdrjcY7Nt"
      },
      "source": [
        "#now that I have everything, I need to join the price set with my text set, by the date column. \n",
        "#I have a problem with datetime type on price frame vs string in text date, which converstion should I use, convert to string?  Let\n",
        "# us convert datetime to string first. if need to, we can do the otherway in the future. \n",
        "# import datetime\n",
        "# t = datetime.datetime(2012, 2, 23, 0, 0)\n",
        "# t.strftime('%Y-%m-%d')\n",
        "\n",
        "frame_x = data.merge(price, how='outer', on=['date'])\n",
        "#and drop any non-overlapping cells by dropna.\n",
        "frame_x = frame_x.dropna()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qSLwAXzK4cCl"
      },
      "source": [
        "#frame_x.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2LmoPhFLzngF"
      },
      "source": [
        "#now that I have the frame prepared, I think I should do the text splitting part? \n",
        "#I need to decide on the length to use, and embedding to use. \n",
        "#let's roll with the 200 length for now. \n",
        "\n",
        "frame_x['text_split']=frame_x.clean_txt.apply(get_split)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vN4t5acu1kvi",
        "outputId": "77aab3cf-a3d4-403e-a33c-29a630660d4a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 620
        }
      },
      "source": [
        "frame_x.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>date</th>\n",
              "      <th>text</th>\n",
              "      <th>clean_txt</th>\n",
              "      <th>Date</th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Close</th>\n",
              "      <th>Adj Close</th>\n",
              "      <th>Volume</th>\n",
              "      <th>change</th>\n",
              "      <th>log_change</th>\n",
              "      <th>change_label</th>\n",
              "      <th>text_split</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2019-01-22</td>\n",
              "      <td>仮想通貨の時価総額 $119,391,276,842 BTC 価格:$3577.41 ドミナ...</td>\n",
              "      <td>btc eth btc bitcoin current price more on btc ...</td>\n",
              "      <td>2019-01-22</td>\n",
              "      <td>3575.081299</td>\n",
              "      <td>3620.746582</td>\n",
              "      <td>3539.721436</td>\n",
              "      <td>3604.577148</td>\n",
              "      <td>3604.577148</td>\n",
              "      <td>5313623556</td>\n",
              "      <td>0.007982</td>\n",
              "      <td>0.007951</td>\n",
              "      <td>True</td>\n",
              "      <td>[btc eth btc bitcoin current price more on btc...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2019-03-26</td>\n",
              "      <td>@ WhatsApp to incorporate a # Bitcoin wallet ?...</td>\n",
              "      <td>to incorporate wallet another step towards ado...</td>\n",
              "      <td>2019-03-26</td>\n",
              "      <td>3969.228760</td>\n",
              "      <td>3985.080811</td>\n",
              "      <td>3944.753174</td>\n",
              "      <td>3985.080811</td>\n",
              "      <td>3985.080811</td>\n",
              "      <td>10707678814</td>\n",
              "      <td>0.005554</td>\n",
              "      <td>0.005538</td>\n",
              "      <td>True</td>\n",
              "      <td>[to incorporate wallet another step towards ad...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2019-03-27</td>\n",
              "      <td>I was having a chat the other day during a Bit...</td>\n",
              "      <td>was chat the other day during bitcoin beyond l...</td>\n",
              "      <td>2019-03-27</td>\n",
              "      <td>3984.244873</td>\n",
              "      <td>4087.066162</td>\n",
              "      <td>3977.810547</td>\n",
              "      <td>4087.066162</td>\n",
              "      <td>4087.066162</td>\n",
              "      <td>10897131934</td>\n",
              "      <td>0.025592</td>\n",
              "      <td>0.025270</td>\n",
              "      <td>True</td>\n",
              "      <td>[was chat the other day during bitcoin beyond ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2019-01-23</td>\n",
              "      <td>$2,000,000 worth of # Bitcoin sold at $3,576.4...</td>\n",
              "      <td>worth of sold at wait for the bounce then corn...</td>\n",
              "      <td>2019-01-23</td>\n",
              "      <td>3605.557129</td>\n",
              "      <td>3623.067871</td>\n",
              "      <td>3565.313965</td>\n",
              "      <td>3585.123047</td>\n",
              "      <td>3585.123047</td>\n",
              "      <td>5433755648</td>\n",
              "      <td>-0.005397</td>\n",
              "      <td>-0.005412</td>\n",
              "      <td>False</td>\n",
              "      <td>[worth of sold at wait for the bounce then cor...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2019-01-09</td>\n",
              "      <td>$ BTC | # BTC - bitcoin's Current Price: ▼ $40...</td>\n",
              "      <td>btc bitcoin current price more on stick to bit...</td>\n",
              "      <td>2019-01-09</td>\n",
              "      <td>4031.552002</td>\n",
              "      <td>4068.403564</td>\n",
              "      <td>4022.662842</td>\n",
              "      <td>4035.296387</td>\n",
              "      <td>4035.296387</td>\n",
              "      <td>5115905224</td>\n",
              "      <td>0.001104</td>\n",
              "      <td>0.001103</td>\n",
              "      <td>True</td>\n",
              "      <td>[btc bitcoin current price more on stick to bi...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         date  ...                                         text_split\n",
              "0  2019-01-22  ...  [btc eth btc bitcoin current price more on btc...\n",
              "1  2019-03-26  ...  [to incorporate wallet another step towards ad...\n",
              "2  2019-03-27  ...  [was chat the other day during bitcoin beyond ...\n",
              "3  2019-01-23  ...  [worth of sold at wait for the bounce then cor...\n",
              "4  2019-01-09  ...  [btc bitcoin current price more on stick to bi...\n",
              "\n",
              "[5 rows x 14 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1rY7lDLO4jEd",
        "outputId": "1652923c-e61e-4734-8f68-197f473a9c1c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "frame_x.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(90, 14)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yECEmzvEn6zw"
      },
      "source": [
        "#ok, i'm gonna rank the frame here by the time entry. \n",
        "frame_x= frame_x.sort_values('date',ascending=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JinMz2dU5rpS"
      },
      "source": [
        "#try to save the processed frame to load directly next time\n",
        "df_save=frame_x[['date','text_split','change_label']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LrJoT6sA48oF",
        "outputId": "4f59e205-7ff8-4079-f38a-d94cf940c85d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "df_save.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>date</th>\n",
              "      <th>text_split</th>\n",
              "      <th>change_label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>71</th>\n",
              "      <td>2019-01-01</td>\n",
              "      <td>[bitcoin could revolutionize governance accord...</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>62</th>\n",
              "      <td>2019-01-02</td>\n",
              "      <td>[btc over now bitcoin cash pay the up threaten...</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>56</th>\n",
              "      <td>2019-01-03</td>\n",
              "      <td>[btc bitcoin price in on resistance at will th...</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>81</th>\n",
              "      <td>2019-01-04</td>\n",
              "      <td>[na data de era do da btc san bitcoin how bitc...</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>73</th>\n",
              "      <td>2019-01-05</td>\n",
              "      <td>[tell my year old self to act less edgy and to...</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          date                                         text_split  change_label\n",
              "71  2019-01-01  [bitcoin could revolutionize governance accord...          True\n",
              "62  2019-01-02  [btc over now bitcoin cash pay the up threaten...          True\n",
              "56  2019-01-03  [btc bitcoin price in on resistance at will th...         False\n",
              "81  2019-01-04  [na data de era do da btc san bitcoin how bitc...          True\n",
              "73  2019-01-05  [tell my year old self to act less edgy and to...         False"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ME1imadJjAd",
        "outputId": "9474d891-cff5-4422-fb4e-e139816e8fc2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#ok, now, let's save another version, with every chunk as an entry. \n",
        "#this part, omits the dates signal for now. btw \n",
        "train_l = []\n",
        "label_l = []\n",
        "index_l =[]\n",
        "for idx,row in df_save.iterrows():\n",
        "  for l in row['text_split']:\n",
        "    train_l.append(l)\n",
        "    label_l.append(row['change_label'])\n",
        "    index_l.append(idx)\n",
        "len(train_l), len(label_l), len(index_l)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(23170, 23170, 23170)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "77tWanhgKfjX"
      },
      "source": [
        "#converting the label from True/False to 1/0. might be redundant, see how. \n",
        "label_l_num= [1 if row == True else 0 for row in label_l]\n",
        "\n",
        "train_df = pd.DataFrame({'text_split':train_l, 'change_label':label_l_num})\n",
        "\n",
        "#ok, I'm going to split my dataframe into train/validation by a percentage\n",
        "frame_len= train_df.shape[0]\n",
        "split_ratio= 0.8\n",
        "num = round(frame_len * split_ratio)\n",
        "\n",
        "train_df[:num].to_csv('/content/gdrive/My Drive/Crypto/train_save.csv',index=False)\n",
        "train_df[num:].to_csv('/content/gdrive/My Drive/Crypto/val_save.csv',index=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGAyoqwF8QgA",
        "outputId": "8eb2cb27-3015-4071-fde7-a3981ecfc21a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "train_df[:num].head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text_split</th>\n",
              "      <th>change_label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>bitcoin could revolutionize governance accordi...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>be sparkling clean with all the wash trading g...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>the in the last bitcoin price did not behave a...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>above it for bitcoin on track to replace gold ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>la de ser en global network in helping those w...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                          text_split  change_label\n",
              "0  bitcoin could revolutionize governance accordi...             1\n",
              "1  be sparkling clean with all the wash trading g...             1\n",
              "2  the in the last bitcoin price did not behave a...             1\n",
              "3  above it for bitcoin on track to replace gold ...             1\n",
              "4  la de ser en global network in helping those w...             1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8KWNugF4UBdX",
        "outputId": "b968663e-5c45-4233-f2de-0ed12c3ed6e3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#train_df[num:].change_label.value_counts()\n",
        "train_df[num:].change_label.sum()/len(train_df[num:])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5837289598618903"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "co7CLtSi779J",
        "outputId": "a2437075-05ca-4061-96bc-dee54e578543",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "!ls '/content/gdrive/My Drive/Crypto'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "df_save.csv\t      GoogleNews-vectors-negative300.bin.gz\n",
            "final_price_2019.csv  tweets2019\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-bNtwe-vPxt"
      },
      "source": [
        "# Get word embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NE_uDu-avV5N"
      },
      "source": [
        "#ok text and labels are somewhat prepared, time to bring out the big guns,get the embedding. \n",
        "#USE PRETRAINED ONES, what embeddings to use. doesn't have to be BERT. \n",
        "\n",
        "#in the BERT tutorial mode, there are a few steps:\n",
        "#1. prepare data for Bert\n",
        "#2. finetune Bert\n",
        "#3. extract Bert embedding\n",
        "#4. prepare the emedding to LSTM, which takes in variable input length, gotta do padding and masking and stuff. \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fb7XFWDhJUAJ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tFBUK65SvZvY"
      },
      "source": [
        "import torch\n",
        "from torchtext import data\n",
        "\n",
        "SEED = 1234\n",
        "\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SnOknTdzWsCj"
      },
      "source": [
        "TEXT = data.Field(tokenize = 'spacy', include_lengths = True)\n",
        "LABEL = data.LabelField(dtype = torch.float)\n",
        "\n",
        "#from torchtext import datasets\n",
        "\n",
        "#train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4mQM5TIWr-c"
      },
      "source": [
        "#example = next(iter(test_data))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SYrHv1LNWr6b"
      },
      "source": [
        "#field = {'date':('date',TEXT),'text_split':('text_split',TEXT),'change_label':('change_label',LABEL)}\n",
        "#maybe i'll leave out the date first. \n",
        "field = {'text_split':('text_split',TEXT),'change_label':('change_label',LABEL)}\n",
        "#field = {'date':('date',TEXT), 'change_label':('change_label',LABEL)}\n",
        "#field = {'change_label':('change_label',LABEL)}\n",
        "\n",
        "#I need to do something about the 'text_split' field with this tokenizer, the processing might to be done before hand. "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQIc-npDWr2c"
      },
      "source": [
        "train_1, val_1 = data.TabularDataset.splits(path='/content/gdrive/My Drive/Crypto/',train='train_save.csv',validation='val_save.csv',format='csv',fields=field)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bGHigWuSX9K9",
        "outputId": "1917f974-c701-4da6-c61f-d5fe1bd4a8bc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#this step is apparently not needed if my sentence is not gazillion long. \n",
        "import sys\n",
        "import csv\n",
        "\n",
        "csv.field_size_limit(sys.maxsize)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "131072"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8rijb5PdY9oY",
        "outputId": "c28b45f3-9b2e-415c-d525-0642d569bdc1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print (val_1[0].__dict__.keys())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dict_keys(['text_split', 'change_label'])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "as0V9IqVZV2l",
        "outputId": "388a83cd-6e83-4364-e6c8-909ae245eff3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print (val_1[4600].__dict__.values())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dict_values([['card', 'will', 'lead', 'to', 'easier', 'bitcoin', 'adoption', 'bitcoin', 'se', 'dos', 'us', 'as', 'hacker', 'over', 'million', 'worth', 'of', 'stolen', 'eth', 'to', 'unknown', 'wallet', 'btc', 'eth', 'xrp', 'the', 'key', 'to', 'eos', 'adoption', 'is', 'ram', 'on', 'na', 'din', 'me', 'wo', 'one', 'word', 'ho', 'in', 'bitcoin', 'make', 'lasting', 'revival', 'unlikely', 'from', 'want', 'to', 'win', 'free', 'register', 'with', 'only', 'via', 'this', 'is', 'project', 'that', 'your', 'participation', 'this', 'project', 'should', 'undoubtedly', 'interest', 'both', 'and', 'ordinary', 'eth', 'btc', 'ltc', 'bitcoin', 'btc', 'current', 'price', 'hour', 'days', 'why', 'apple', 'card', 'can', 'compete', 'with', 'bitcoin', 'top', 'bitcoin', 'sa', 'from', 'to', 'and', 'beyond', 'our', 'subscription', 'less', 'than', 'cup', 'of', 'coffee', 'here', 'are', 'the', 'monthly', 'day', 'yearly', 'off', 'day', 'is', 'the', 'real', 'money', 'the', 'fed', 'is', 'all', 'about', 'the', 'people', 'fake', 'money', 'like', 'fake', 'news', 'will', 'always', 'be', 'successful', 'in', 'being', 'the', 'user', 'operating', 'system', 'the', 'same', 'way', 'bitcoin', 'will', 'always', 'be', 'successful', 'in', 'being', 'the', 'most', 'secure', 'open', 'borderless', 'and', 'censorship', 'resistant', 'cryptocurrency', 'feel', 'free', 'to', 'refer', 'to', 'my', 'analysis', 'here', 'to', 'see', 'what', 'mean', 'regardless', 'of', 'its', 'old', 'tech', 'bitcoin', 'price', 'is', 'bound', 'to', 'increase', 'lot', 'as', 'you', 'see', 'it', 'great', 'investment', 'no', 'matter', 'what', 'we', 'think', 'of', 'it', 'and', 'despite', 'that', 'xrp', 'is', 'far', 'better', 'da', 'investment', 'moon', 'bitcoin'], '0'])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "820EXEnzsuZq",
        "outputId": "0cb0de5c-ae71-4bdb-8772-046b9bd87ac9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        }
      },
      "source": [
        "df= pd.read_csv('/content/gdrive/My Drive/Crypto/val_save.csv')\n",
        "df[4600:4603]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text_split</th>\n",
              "      <th>change_label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>4600</th>\n",
              "      <td>card will lead to easier bitcoin adoption bitc...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4601</th>\n",
              "      <td>resistant cryptocurrency feel free to refer to...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4602</th>\n",
              "      <td>could easily surpass gold in mike and it to pa...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             text_split  change_label\n",
              "4600  card will lead to easier bitcoin adoption bitc...             0\n",
              "4601  resistant cryptocurrency feel free to refer to...             0\n",
              "4602  could easily surpass gold in mike and it to pa...             0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KN4lfcSUZaRI",
        "outputId": "549cf485-278a-42a7-d099-5cb6b558d476",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        }
      },
      "source": [
        "next(iter(train_1)).change_label\n",
        "#so the text_split part, is a list. of al the tokens i recon, and '[' is even tokenized as well. \n",
        "#now it looks somewhat normal. "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'1'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2bKpu2s7kPH2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l3EJPZUSZgTl",
        "outputId": "5c56c0e8-4e60-4af0-851c-f7353bba9e86",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#now let's try out the build vocabulary part\n",
        "\n",
        "MAX_VOCAB_SIZE = 25_000\n",
        "\n",
        "TEXT.build_vocab(train_1, \n",
        "                 max_size = MAX_VOCAB_SIZE, \n",
        "                 vectors = \"glove.6B.100d\", \n",
        "                 unk_init = torch.Tensor.normal_) # how to initialize unseen words not in glove\n",
        "\n",
        "LABEL.build_vocab(train_1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".vector_cache/glove.6B.zip: 862MB [06:28, 2.22MB/s]                          \n",
            " 99%|█████████▉| 397824/400000 [00:16<00:00, 23257.58it/s]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cXINOdTToVmb",
        "outputId": "b39515dd-33b7-4950-97f7-0617ae8b5140",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        }
      },
      "source": [
        "TEXT.pad_token"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'<pad>'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YC57v0Mk8XvW",
        "outputId": "b06c5791-8870-4ea1-c277-a1990f356924",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#check the vocba size\n",
        "len(TEXT.vocab)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "17909"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "teseLF-jNJGM"
      },
      "source": [
        "#so...now that I've built the vocab, how do I check? obviously this vocab will not be very crypto currency friendly. \n",
        "#so ideally it should also be fine tuned. but let's get on with it for now. "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hV7PZYVIPYPj"
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "\n",
        "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device = 'cuda'\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T7zwGm-nFQmI",
        "outputId": "886e07e2-afc6-440b-80cb-fb27212efc62",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "torch.cuda.is_available()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rRmB3WIaRsSW"
      },
      "source": [
        "train_iterator, val_iterator = data.BucketIterator.splits(\n",
        "    (train_1,val_1), \n",
        "    batch_size = BATCH_SIZE,\n",
        "    #sort_within_batch = True,\n",
        "    sort_key=lambda x: len(x.text_split), # the BucketIterator needs to be told what function it should use to group the data.\n",
        "    sort_within_batch=False,\n",
        "    device = device)\n",
        "\n",
        "#error msg: Example' object has no attribute 'sort_key'\n",
        "#stackoverflow says fields are not passed in the same order as they are in the csv/tsv file\n",
        "#tried to convert labels from true/false to 1/0, didn't solve the error. \n",
        "#check values, nothing is null. how to solve this buuuuug. HALP!!!\n",
        "\n",
        "#...ok, I don't need to split, if, i'm only iterating over train_l data...ok work! "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PFZVlH-1wS0a"
      },
      "source": [
        "# train_iterator = data.BucketIterator.splits(\n",
        "#     train_1, \n",
        "#     batch_size = BATCH_SIZE,\n",
        "#     #sort_within_batch = True,\n",
        "#     sort_key=lambda x: len(x.text_split), # the BucketIterator needs to be told what function it should use to group the data.\n",
        "#     sort_within_batch=False,\n",
        "#     device = device)\n",
        "\n",
        "# train_iterator[64].__dict__"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2NIv5U4Z4Ci9"
      },
      "source": [
        "#seems like the sequnce is kept, which is just what I want, great. \n",
        "\n",
        "# train_iterator[0].dataset.text_split[0:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pVGrJXKh4DFu"
      },
      "source": [
        "# Get into the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lNPRtNEyR3ul"
      },
      "source": [
        "##now, let's get into the modeling part \n",
        "#1. defining the model\n",
        "#2. writting out the training loop \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uatG5C5GWSvY"
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self,vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, \n",
        "                 bidirectional, dropout, pad_idx):\n",
        "      super().__init__()\n",
        "      self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
        "      self.rnn = nn.LSTM(embedding_dim,\n",
        "                         hidden_dim,\n",
        "                         num_layers=n_layers,\n",
        "                         bidirectional=bidirectional,\n",
        "                         dropout=dropout)\n",
        "      self.fc = nn.Linear(hidden_dim*2, output_dim)\n",
        "      self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, text, text_lengths):\n",
        "      embedded = self.dropout(self.embedding(text))\n",
        "      \n",
        "      packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths, enforce_sorted = False) #added the enforce_sorted = false, debug\n",
        "      packed_output, (hidden, cell) = self.rnn(packed_embedded)\n",
        "\n",
        "      output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output)#btw this is not used here i think.\n",
        "\n",
        "      hidden = self.dropout(torch.cat((hidden[-2,:,:],hidden [-1,:,:]),dim =1 ))\n",
        "\n",
        "      return self.fc(hidden)\n",
        "\n",
        "\n",
        "      "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sW4Nx_h6WTuX"
      },
      "source": [
        "INPUT_DIM=len(TEXT.vocab)\n",
        "EMBEDDING_DIM=100\n",
        "HIDDEN_DIM=256\n",
        "OUTPUT_DIM = 1\n",
        "N_LAYERS=2\n",
        "BIDIRECTIONAL=True\n",
        "DROPOUT = 0.5\n",
        "PAD_IDX= TEXT.vocab.stoi[TEXT.pad_token]\n",
        "\n",
        "#i really donno what's the pad_token thing here...or how to use/where to learn vocab.stoi \n",
        "#TEXT.pad_token is just the dictionary key to the padding token, which is <pad> here.\n",
        "#and TEXT.vocab.stoi['<pad>'] gives the padding value, 1. \n",
        "model = RNN(INPUT_DIM,EMBEDDING_DIM,HIDDEN_DIM,OUTPUT_DIM,N_LAYERS,BIDIRECTIONAL,DROPOUT,PAD_IDX)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aran0GdKpETq",
        "outputId": "cd6e7874-af03-41d6-a5ed-4472ec9c5dae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#checking trainable parameters\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 4,101,557 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0WHPHSzWpOTW",
        "outputId": "cf55bcfd-b459-4b10-e5de-17f5f667b81f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "pretrained_embeddings = TEXT.vocab.vectors\n",
        "\n",
        "print(pretrained_embeddings.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([17909, 100])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7wPGK0EJpmfj",
        "outputId": "d261651e-70b9-4885-d2e4-cb743b23c3d9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#copying the pretrained weights into the embedding. \n",
        "model.embedding.weight.data.copy_(pretrained_embeddings)\n",
        "\n",
        "\n",
        "#i'm a little bit confused about this part for sure..why copying weights? then the embeddings is what? \n",
        "#so embedding layer is just like a dictionary with word index and as the key, values are just hte \"weights\"\n",
        "#if i dont'wnat the weigts to be touched, I can specify in the model as\n",
        "# print(\"Do not finetune word embedding layer.\")\n",
        "#             self.emb.weight.requires_grad = False\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.1117, -0.4966,  0.1631,  ...,  1.2647, -0.2753, -0.1325],\n",
              "        [-0.8555, -0.7208,  1.3755,  ...,  0.0825, -1.1314,  0.3997],\n",
              "        [ 0.8403,  0.1327, -0.6158,  ...,  0.7602, -0.2352,  0.3349],\n",
              "        ...,\n",
              "        [ 1.1097,  0.3746, -0.3882,  ..., -0.4966,  0.2572, -0.9995],\n",
              "        [ 0.5207,  0.5319, -0.2065,  ...,  0.3557, -0.5426, -0.3839],\n",
              "        [-0.5879,  0.0165, -0.6412,  ...,  0.1248,  0.0362, -0.3741]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 153
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "POwW4z-kpptF",
        "outputId": "6593c2eb-31dd-46e8-ddca-487e552d8765",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#this step will set the <unk> and <pad> to be 0, and make them irrelevant for the model\n",
        "#is this good though? the pretrained model is not on crypto words, which means the model WILL ignore crypto words\n",
        "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
        "\n",
        "model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
        "model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n",
        "\n",
        "print(model.embedding.weight.data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [ 0.8403,  0.1327, -0.6158,  ...,  0.7602, -0.2352,  0.3349],\n",
            "        ...,\n",
            "        [ 1.1097,  0.3746, -0.3882,  ..., -0.4966,  0.2572, -0.9995],\n",
            "        [ 0.5207,  0.5319, -0.2065,  ...,  0.3557, -0.5426, -0.3839],\n",
            "        [-0.5879,  0.0165, -0.6412,  ...,  0.1248,  0.0362, -0.3741]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F5gOwUwHhNTW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxWZgTiuhQnS"
      },
      "source": [
        "# Now Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ROIJ_wZahSzf"
      },
      "source": [
        "#choose optimizer \n",
        "import torch.optim as optim\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(),weight_decay=0.001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wYgk02YEixY8"
      },
      "source": [
        "#moving to GPU. \n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "model= model.to(device)\n",
        "criterion = criterion.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SR0FFbHXbcdF",
        "outputId": "3edaa226-91c1-4d5c-aa06-7c2e3f79c6fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        }
      },
      "source": [
        "device"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'cuda'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 183
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y0v_m0UVjKo2"
      },
      "source": [
        "#defining accuracy metrics\n",
        "def binary_accuracy(preds, y):\n",
        "    \"\"\"\n",
        "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
        "    \"\"\"\n",
        "\n",
        "    #round predictions to the closest integer\n",
        "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
        "    correct = (rounded_preds == y).float() #convert into float for division \n",
        "    acc = correct.sum() / len(correct)\n",
        "    return acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ibwmgjtblNSw"
      },
      "source": [
        "#note, here we seperate the text and text.length as they are tuple\n",
        "\n",
        "def train(model,iterator,optimizer, criterion):\n",
        "  epoch_loss = 0 \n",
        "  epoch_acc = 0\n",
        "\n",
        "  model.train() #so that droput function works \n",
        "  for batch in iterator: \n",
        "    optimizer.zero_grad()#why? ok, to not accumulate un-intended gradient i guess.\n",
        "\n",
        "    text, text_lengths = batch.text_split\n",
        "    #predictions = model(text, text_lengths).squeeze(1) #why the fk do i need to move the length to cpu. \n",
        "    predictions = model(text, text_lengths.cpu()).squeeze(1) #what's the squeeze, what does squeeze do? pack np.array to be lower dimension?\n",
        "    loss = criterion(predictions, batch.change_label)\n",
        "\n",
        "    acc = binary_accuracy(predictions, batch.change_label)\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    epoch_loss += loss.item()\n",
        "    epoch_acc += acc.item()\n",
        "\n",
        "  return epoch_loss / len(iterator) , epoch_acc / len(iterator)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sFljptalqM38"
      },
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "\n",
        "  epoch_loss = 0 \n",
        "  epoch_acc = 0\n",
        "\n",
        "  model.eval() #so that droput function DONT ACTIVATE\n",
        "\n",
        "  with torch.no_grad():\n",
        "  #is this what we do for evaluation always? \n",
        "  #answer: \"with torch.no_grad()\" temporarily set all the requires_grad flag to false.\n",
        "    for batch in iterator: \n",
        "      text, text_lengths = batch.text_split\n",
        "      #predictions = model(text, text_lengths).squeeze(1) #what's the squeeze, what does squeeze do? pack np.array to be lower dimension?\n",
        "      predictions = model(text, text_lengths.cpu()).squeeze(1)\n",
        "      loss = criterion(predictions, batch.change_label)\n",
        "\n",
        "      acc = binary_accuracy(predictions, batch.change_label)\n",
        "\n",
        "      epoch_loss += loss.item()\n",
        "      epoch_acc += acc.item()\n",
        "\n",
        "  return epoch_loss / len(iterator) , epoch_acc / len(iterator)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q5KmMLzLq9PH"
      },
      "source": [
        "#define a timing function \n",
        "import time\n",
        "\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iGZ34P7uWI2K"
      },
      "source": [
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Y_xyJhlsFcU",
        "outputId": "8ce8e189-70a6-4496-d022-b201b1deb02f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#now try to train yea\n",
        "N_EPOCHS = 5\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
        "    valid_loss, valid_acc = evaluate(model, val_iterator, criterion)\n",
        "    \n",
        "    end_time = time.time()\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    # if valid_loss < best_valid_loss:\n",
        "    #     best_valid_loss = valid_loss\n",
        "    #     torch.save(model.state_dict(), 'tut2-model.pt')\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\t Train Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Epoch Time: 0m 28s\n",
            "\t Train Loss: 0.688 | Train Acc: 54.93%\n",
            "\t Val. Loss: 0.680 |  Val. Acc: 58.46%\n",
            "Epoch: 02 | Epoch Time: 0m 29s\n",
            "\t Train Loss: 0.688 | Train Acc: 54.93%\n",
            "\t Val. Loss: 0.682 |  Val. Acc: 58.46%\n",
            "Epoch: 03 | Epoch Time: 0m 28s\n",
            "\t Train Loss: 0.688 | Train Acc: 54.92%\n",
            "\t Val. Loss: 0.681 |  Val. Acc: 58.46%\n",
            "Epoch: 04 | Epoch Time: 0m 28s\n",
            "\t Train Loss: 0.688 | Train Acc: 54.90%\n",
            "\t Val. Loss: 0.682 |  Val. Acc: 58.46%\n",
            "Epoch: 05 | Epoch Time: 0m 29s\n",
            "\t Train Loss: 0.688 | Train Acc: 54.91%\n",
            "\t Val. Loss: 0.682 |  Val. Acc: 58.46%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MKWoYOeZNC-Q"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v-tfvrXGu9_U"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}