{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Price Base Model New Input",
      "provenance": [],
      "collapsed_sections": [
        "NpHkfoUnkocT",
        "6EuYIwJAHyT6",
        "SFfMPY--bvrO",
        "R7lIxy82XsK0",
        "mruUFVdyIxfH",
        "bf_54Mrvw1aO",
        "XqSIcZPr9qCY"
      ],
      "toc_visible": true,
      "authorship_tag": "ABX9TyM98/JDNueGYwYGTp4i6Q0p",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YanzhaoZ/PreBit/blob/main/Price_Base_Model_New_Input.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NpHkfoUnkocT"
      },
      "source": [
        "#set-ups "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AKJf58FLcogN",
        "outputId": "0bc42097-3ecc-40c4-8b78-cf7542a8374d"
      },
      "source": [
        " !pip install yfinance\n",
        "import yfinance as yf\n",
        "import pandas as pd\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting yfinance\n",
            "  Downloading https://files.pythonhosted.org/packages/7a/e8/b9d7104d3a4bf39924799067592d9e59119fcfc900a425a12e80a3123ec8/yfinance-0.1.55.tar.gz\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.6/dist-packages (from yfinance) (1.1.5)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.6/dist-packages (from yfinance) (1.19.4)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.6/dist-packages (from yfinance) (2.23.0)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from yfinance) (0.0.9)\n",
            "Collecting lxml>=4.5.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bd/78/56a7c88a57d0d14945472535d0df9fb4bbad7d34ede658ec7961635c790e/lxml-4.6.2-cp36-cp36m-manylinux1_x86_64.whl (5.5MB)\n",
            "\u001b[K     |████████████████████████████████| 5.5MB 12.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.24->yfinance) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.24->yfinance) (2.8.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20->yfinance) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20->yfinance) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20->yfinance) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20->yfinance) (1.24.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.7.3->pandas>=0.24->yfinance) (1.15.0)\n",
            "Building wheels for collected packages: yfinance\n",
            "  Building wheel for yfinance (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for yfinance: filename=yfinance-0.1.55-py2.py3-none-any.whl size=22616 sha256=7474e8f19e417c9d188813cb02c3f73b1733c4e5b42d8db9990fb1ad1a03accf\n",
            "  Stored in directory: /root/.cache/pip/wheels/04/98/cc/2702a4242d60bdc14f48b4557c427ded1fe92aedf257d4565c\n",
            "Successfully built yfinance\n",
            "Installing collected packages: lxml, yfinance\n",
            "  Found existing installation: lxml 4.2.6\n",
            "    Uninstalling lxml-4.2.6:\n",
            "      Successfully uninstalled lxml-4.2.6\n",
            "Successfully installed lxml-4.6.2 yfinance-0.1.55\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h0hPGdzadWND"
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, Dataset, WeightedRandomSampler\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from sklearn.preprocessing import MinMaxScaler\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BshD6EjfdHGy"
      },
      "source": [
        "# Loading Price Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IGzvjjDYdLoi",
        "outputId": "4f8990ca-ec93-48c6-bb42-dfa70006515a"
      },
      "source": [
        "start_date ='2015-01-01'\n",
        "end_date = '2019-12-31'\n",
        "price = yf.download(\"BTC-USD\", start=start_date, end=end_date)\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oiqImSEpgGt5"
      },
      "source": [
        "def get_technical_indicators(price):\n",
        "    # Create 7 and 21 days Moving Average\n",
        "    dataset = price.copy()\n",
        "\n",
        "    dataset['ma7'] = dataset['Close'].rolling(window=7).mean()\n",
        "    dataset['ma21'] = dataset['Close'].rolling(window=21).mean()\n",
        "    \n",
        "    # Create MACD\n",
        "    #dataset['26ema'] = pd.ewma(dataset['Close'], span=26)\n",
        "    dataset['26ema'] = dataset['Close'].ewm(span=26).mean()\n",
        "    #dataset['12ema'] = pd.ewma(dataset['Close'], span=12)\n",
        "    dataset['12ema'] = dataset['Close'].ewm(span=12).mean()\n",
        "    dataset['MACD'] = (dataset['12ema']-dataset['26ema'])\n",
        "\n",
        "    # Create Bollinger Bands\n",
        "    #dataset['20sd'] = pd.stats.moments.rolling_std(dataset['Close'],20)\n",
        "    dataset['20sd'] = dataset[\"Close\"].rolling(window=20).std()\n",
        "    dataset['upper_band'] = dataset['ma21'] + (dataset['20sd']*2)\n",
        "    dataset['lower_band'] = dataset['ma21'] - (dataset['20sd']*2)\n",
        "    \n",
        "    # Create Exponential moving average\n",
        "    dataset['ema'] = dataset['Close'].ewm(com=0.5).mean()\n",
        "    \n",
        "    # Create Momentum\n",
        "    #dataset['momentum'] = dataset['Close']-1\n",
        "\n",
        "    # Create high-low spred\n",
        "    dataset['spread'] = dataset['High'] - dataset['Low']\n",
        "    \n",
        "    return dataset"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-keoW2D4TEb"
      },
      "source": [
        "The Gold etf SPDR Gold future, has a lot of NAN values. \r\n",
        "That might cause some issues, but let's see how it goes first."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Szog27YrNQSG"
      },
      "source": [
        " # to be finished \n",
        "# def get_related_asset (price, start_date, end_date):\n",
        "#     other_price = yf.download(\"ETH-USD\", start=start_date, end=end_date)\n",
        "\n",
        "#     price['eth']=other_price[\"Close\"]\n",
        "#     #there are nan values because the price datarange is not the same. so I should probalby fill the value by non-0, coz i need to do division later...\n",
        "#     #let's fill it with the next valid value \n",
        "#     price.eth.fillna(method='bfill', inplace= True, axis=0)\n",
        "\n",
        "#     return price\n",
        "\n",
        " # to be finished \n",
        "def get_related_asset (price, start_date, end_date):\n",
        "    other_price = yf.download(\"ETH-USD GLD\", start=start_date, end=end_date)\n",
        "\n",
        "    price['eth']=other_price.Close['ETH-USD']\n",
        "    price['gold'] = other_price.Close['GLD']\n",
        "    #there are nan values because the price datarange is not the same. so I should probalby fill the value by non-0, coz i need to do division later...\n",
        "    #let's fill it with the next valid value \n",
        "    price.eth.fillna(method='bfill', inplace= True, axis=0)\n",
        "    price.gold.fillna(method='bfill', inplace= True, axis=0) \n",
        "\n",
        "    return price\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "291fHPUribeU"
      },
      "source": [
        "def get_label (price,threshold):\n",
        "\n",
        "\n",
        "    price['change']=price.shift(-1).High/price.Close -1 \n",
        "    price['change_label']=price['change'].apply (lambda x: x> threshold)\n",
        "\n",
        "    #convert True/False to 1/0\n",
        "    class2idx = {True: 1, False:0}\n",
        "    price['change_label'].replace(class2idx, inplace=True)\n",
        "\n",
        "    return price"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PrQfP2Crtl8X"
      },
      "source": [
        "def normalise_close(price):\n",
        "    \n",
        "    df = price.copy()\n",
        "    for key in df.keys():\n",
        "        if not key in ['change','change_label','Volume','MACD','20sd','spread','eth','gold']:\n",
        "            df[key]=df[key]/price['Close'].shift(1) - 1\n",
        "        \n",
        "    df['Volume']=df['Volume']/price['Volume'].shift(1)-1\n",
        "    df['eth']  = df['eth']/price['eth'].shift(1)-1\n",
        "    df['gold'] = df['gold']/price['gold'].shift(1)-1\n",
        "    df['MACD'] = df['MACD']/price['Close'].shift(1)\n",
        "    df['20sd'] = df['20sd']/price['Close'].shift(1)\n",
        "    df['spread'] = df['spread']/price['Close'].shift(1)\n",
        "\n",
        "\n",
        "\n",
        "    return df\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HB1cLbyxCsb_"
      },
      "source": [
        "def get_ma_feature (price,threshold):\n",
        "    price['ma_feature']=price['ma7'].apply(lambda x: x > threshold)\n",
        "    \n",
        "    class2idx = {True: 1, False:0}\n",
        "    price['ma_feature'].replace(class2idx, inplace=True)\n",
        "    return price"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wr-qb6DFd-Dw"
      },
      "source": [
        "#Here I want to define a function to return a new dataframe, that's INDEPENDANT, and produce relative price. \n",
        "def process_price (original_df,threshold, start_date,end_date):\n",
        "    #get the indicators\n",
        "    df = get_technical_indicators(original_df)\n",
        "\n",
        "    #get related asset\n",
        "    df = get_related_asset(df,start_date,end_date)\n",
        "\n",
        "    #get label\n",
        "    df = get_label(df, threshold)\n",
        "\n",
        "    #normalise the data\n",
        "    df_normalised_close = normalise_close(df)\n",
        "\n",
        "    #get ma label\n",
        "    df_normalised_close = get_ma_feature(df_normalised_close, threshold)\n",
        "\n",
        "    #return both normalized, and un-normalized data (w/o ma label). \n",
        "    return df, df_normalised_close\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nS3krSQDeA93",
        "outputId": "b35aa4bf-582f-43a0-e92e-61f9d06d24d7"
      },
      "source": [
        "test_df, test_df_norm = process_price(price,0.05,start_date,end_date)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[*********************100%***********************]  2 of 2 completed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "id": "z-lIuKV9eA4H",
        "outputId": "d226efc3-4d9d-4ad8-ea41-3bffd11bf1a3"
      },
      "source": [
        "test_df.head()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Close</th>\n",
              "      <th>Adj Close</th>\n",
              "      <th>Volume</th>\n",
              "      <th>ma7</th>\n",
              "      <th>ma21</th>\n",
              "      <th>26ema</th>\n",
              "      <th>12ema</th>\n",
              "      <th>MACD</th>\n",
              "      <th>20sd</th>\n",
              "      <th>upper_band</th>\n",
              "      <th>lower_band</th>\n",
              "      <th>ema</th>\n",
              "      <th>spread</th>\n",
              "      <th>eth</th>\n",
              "      <th>gold</th>\n",
              "      <th>change</th>\n",
              "      <th>change_label</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Date</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2015-01-01</th>\n",
              "      <td>320.434998</td>\n",
              "      <td>320.434998</td>\n",
              "      <td>314.002991</td>\n",
              "      <td>314.248993</td>\n",
              "      <td>314.248993</td>\n",
              "      <td>8036550</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>314.248993</td>\n",
              "      <td>314.248993</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>314.248993</td>\n",
              "      <td>6.432007</td>\n",
              "      <td>2.77212</td>\n",
              "      <td>114.080002</td>\n",
              "      <td>0.005060</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2015-01-02</th>\n",
              "      <td>314.079010</td>\n",
              "      <td>315.838989</td>\n",
              "      <td>313.565002</td>\n",
              "      <td>315.032013</td>\n",
              "      <td>315.032013</td>\n",
              "      <td>7860650</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>314.655561</td>\n",
              "      <td>314.673129</td>\n",
              "      <td>0.017568</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>314.836258</td>\n",
              "      <td>2.273987</td>\n",
              "      <td>2.77212</td>\n",
              "      <td>114.080002</td>\n",
              "      <td>0.000375</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2015-01-03</th>\n",
              "      <td>314.846008</td>\n",
              "      <td>315.149994</td>\n",
              "      <td>281.082001</td>\n",
              "      <td>281.082001</td>\n",
              "      <td>281.082001</td>\n",
              "      <td>33054400</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>302.592907</td>\n",
              "      <td>301.562504</td>\n",
              "      <td>-1.030403</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>291.467926</td>\n",
              "      <td>34.067993</td>\n",
              "      <td>2.77212</td>\n",
              "      <td>115.800003</td>\n",
              "      <td>0.021873</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2015-01-04</th>\n",
              "      <td>281.145996</td>\n",
              "      <td>287.230011</td>\n",
              "      <td>257.612000</td>\n",
              "      <td>264.195007</td>\n",
              "      <td>264.195007</td>\n",
              "      <td>55629100</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>291.858532</td>\n",
              "      <td>289.767045</td>\n",
              "      <td>-2.091487</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>273.058706</td>\n",
              "      <td>29.618011</td>\n",
              "      <td>2.77212</td>\n",
              "      <td>115.800003</td>\n",
              "      <td>0.053544</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2015-01-05</th>\n",
              "      <td>265.084015</td>\n",
              "      <td>278.341003</td>\n",
              "      <td>265.084015</td>\n",
              "      <td>274.473999</td>\n",
              "      <td>274.473999</td>\n",
              "      <td>43962800</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>287.826987</td>\n",
              "      <td>285.611979</td>\n",
              "      <td>-2.215008</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>274.006134</td>\n",
              "      <td>13.256989</td>\n",
              "      <td>2.77212</td>\n",
              "      <td>115.800003</td>\n",
              "      <td>0.047651</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                  Open        High  ...    change  change_label\n",
              "Date                                ...                        \n",
              "2015-01-01  320.434998  320.434998  ...  0.005060             0\n",
              "2015-01-02  314.079010  315.838989  ...  0.000375             0\n",
              "2015-01-03  314.846008  315.149994  ...  0.021873             0\n",
              "2015-01-04  281.145996  287.230011  ...  0.053544             1\n",
              "2015-01-05  265.084015  278.341003  ...  0.047651             0\n",
              "\n",
              "[5 rows x 20 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "id": "e4KJZaWZeA1r",
        "outputId": "3a6b99d2-55ab-4d44-cb08-d03dc81f63e6"
      },
      "source": [
        "test_df_norm.head()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Close</th>\n",
              "      <th>Adj Close</th>\n",
              "      <th>Volume</th>\n",
              "      <th>ma7</th>\n",
              "      <th>ma21</th>\n",
              "      <th>26ema</th>\n",
              "      <th>12ema</th>\n",
              "      <th>MACD</th>\n",
              "      <th>20sd</th>\n",
              "      <th>upper_band</th>\n",
              "      <th>lower_band</th>\n",
              "      <th>ema</th>\n",
              "      <th>spread</th>\n",
              "      <th>eth</th>\n",
              "      <th>gold</th>\n",
              "      <th>change</th>\n",
              "      <th>change_label</th>\n",
              "      <th>ma_feature</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Date</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2015-01-01</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.005060</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2015-01-02</th>\n",
              "      <td>-0.000541</td>\n",
              "      <td>0.005060</td>\n",
              "      <td>-0.002177</td>\n",
              "      <td>0.002492</td>\n",
              "      <td>0.002492</td>\n",
              "      <td>-0.021888</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.001294</td>\n",
              "      <td>0.001350</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.001869</td>\n",
              "      <td>0.007236</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000375</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2015-01-03</th>\n",
              "      <td>-0.000590</td>\n",
              "      <td>0.000375</td>\n",
              "      <td>-0.107767</td>\n",
              "      <td>-0.107767</td>\n",
              "      <td>-0.107767</td>\n",
              "      <td>3.205047</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.039485</td>\n",
              "      <td>-0.042756</td>\n",
              "      <td>-0.003271</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.074799</td>\n",
              "      <td>0.108141</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.015077</td>\n",
              "      <td>0.021873</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2015-01-04</th>\n",
              "      <td>0.000228</td>\n",
              "      <td>0.021873</td>\n",
              "      <td>-0.083499</td>\n",
              "      <td>-0.060079</td>\n",
              "      <td>-0.060079</td>\n",
              "      <td>0.682956</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.038339</td>\n",
              "      <td>0.030899</td>\n",
              "      <td>-0.007441</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.028544</td>\n",
              "      <td>0.105371</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.053544</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2015-01-05</th>\n",
              "      <td>0.003365</td>\n",
              "      <td>0.053544</td>\n",
              "      <td>0.003365</td>\n",
              "      <td>0.038907</td>\n",
              "      <td>0.038907</td>\n",
              "      <td>-0.209716</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.089449</td>\n",
              "      <td>0.081065</td>\n",
              "      <td>-0.008384</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.037136</td>\n",
              "      <td>0.050179</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.047651</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                Open      High       Low  ...    change  change_label  ma_feature\n",
              "Date                                      ...                                    \n",
              "2015-01-01       NaN       NaN       NaN  ...  0.005060             0           0\n",
              "2015-01-02 -0.000541  0.005060 -0.002177  ...  0.000375             0           0\n",
              "2015-01-03 -0.000590  0.000375 -0.107767  ...  0.021873             0           0\n",
              "2015-01-04  0.000228  0.021873 -0.083499  ...  0.053544             1           0\n",
              "2015-01-05  0.003365  0.053544  0.003365  ...  0.047651             0           0\n",
              "\n",
              "[5 rows x 21 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6EuYIwJAHyT6"
      },
      "source": [
        "# Some analysis with MA only"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7X5-Qm6rHx_b"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ePkc-bMv5VZ",
        "outputId": "efd5152d-6e8c-47a7-9cc5-f7322122b7ef"
      },
      "source": [
        "(test_df_norm['change_label']==test_df_norm['ma_feature']).value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True     1537\n",
              "False     289\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 164
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HCinzG02Dyba",
        "outputId": "2204c0f1-0a1c-4092-f68e-77cfd93fa485"
      },
      "source": [
        "test_df_norm['change_label'].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    1608\n",
              "1     218\n",
              "Name: change_label, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 165
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hOgB2oowD_Uz",
        "outputId": "a5739857-2cb7-4112-b072-c6c624f73f62"
      },
      "source": [
        "#why not get a confusion matrix with these two labels and check it out. \n",
        "accuracy = (test_df_norm['change_label']==test_df_norm['ma_feature']).sum() / len(test_df_norm['change_label']==test_df_norm['ma_feature'])\n",
        "print ('accuracy:', accuracy)\n",
        "print (confusion_matrix(test_df_norm['change_label'].values, test_df_norm['ma_feature'].values))\n",
        "print (classification_report(test_df_norm['change_label'].values, test_df_norm['ma_feature'].values))\n",
        "\n",
        "#The true label, accuracy is only 0.27, and recall rate is quite low to be honest. "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy: 0.8417305585980285\n",
            "[[1495  113]\n",
            " [ 176   42]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.93      0.91      1608\n",
            "           1       0.27      0.19      0.23       218\n",
            "\n",
            "    accuracy                           0.84      1826\n",
            "   macro avg       0.58      0.56      0.57      1826\n",
            "weighted avg       0.82      0.84      0.83      1826\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mt6Jsxx8F5I3",
        "outputId": "15d0d89a-87bb-4aed-c61b-8106ccb3398c"
      },
      "source": [
        "#for 21day ma features\n",
        "#why not get a confusion matrix with these two labels and check it out. \n",
        "accuracy = (test_df_norm['change_label']==test_df_norm['ma_feature']).sum() / len(test_df_norm['change_label']==test_df_norm['ma_feature'])\n",
        "print ('accuracy:', accuracy)\n",
        "print (confusion_matrix(test_df_norm['change_label'].values, test_df_norm['ma_feature'].values))\n",
        "print (classification_report(test_df_norm['change_label'].values, test_df_norm['ma_feature'].values))\n",
        "\n",
        "#The true label, accuracy is only 0.27, and recall rate is quite low to be honest. "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy: 0.7431544359255202\n",
            "[[1301  307]\n",
            " [ 162   56]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.81      0.85      1608\n",
            "           1       0.15      0.26      0.19       218\n",
            "\n",
            "    accuracy                           0.74      1826\n",
            "   macro avg       0.52      0.53      0.52      1826\n",
            "weighted avg       0.80      0.74      0.77      1826\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "67tCONaUIdX7",
        "outputId": "3a60c80a-bff8-4771-e2b5-2f8f4aec74db"
      },
      "source": [
        "#for ema features\n",
        "#why not get a confusion matrix with these two labels and check it out. \n",
        "accuracy = (test_df_norm['change_label']==test_df_norm['ma_feature']).sum() / len(test_df_norm['change_label']==test_df_norm['ma_feature'])\n",
        "print ('accuracy:', accuracy)\n",
        "print (confusion_matrix(test_df_norm['change_label'].values, test_df_norm['ma_feature'].values))\n",
        "print (classification_report(test_df_norm['change_label'].values, test_df_norm['ma_feature'].values))\n",
        "\n",
        "#The true label, accuracy is only 0.27, and recall rate is quite low to be honest. "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy: 0.864184008762322\n",
            "[[1561   47]\n",
            " [ 201   17]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.97      0.93      1608\n",
            "           1       0.27      0.08      0.12       218\n",
            "\n",
            "    accuracy                           0.86      1826\n",
            "   macro avg       0.58      0.52      0.52      1826\n",
            "weighted avg       0.81      0.86      0.83      1826\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NH2o4ZSOWDUY"
      },
      "source": [
        "#so bascially 7day ma works best among these 3. got it. \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WM2W-oEPWNs0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SFfMPY--bvrO"
      },
      "source": [
        "# Format Custom Dataset input for Torch Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TkcbqR134S4I"
      },
      "source": [
        "Creating two dataset classes for future use, 1 is for classification tast, the other for Regression task"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "me2mllPfb6bP"
      },
      "source": [
        "class ClassifierDataset(Dataset):\n",
        "    def __init__(self,df):\n",
        "        self.df = df\n",
        "        self.X, self.Y = self.clean_df()\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.Y)\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.Y[idx]\n",
        "\n",
        "\n",
        "    def clean_df(self):\n",
        "        #drop NaN values, and also load the values into torch.tensor. \n",
        "        df = self.df.dropna()\n",
        "        y_values = df.change_label.values\n",
        "        df = df.drop (['change','change_label'],axis=1)\n",
        "        #try put the label as part of the feature see if the model cna leran that \n",
        "        #df = df.drop (['change'],axis=1)\n",
        "        x_values = df.values\n",
        "\n",
        "        return torch.from_numpy(x_values).float(), torch.from_numpy(y_values).long()\n",
        "\n",
        "\n",
        "      \n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w7l-Gu-vkq83"
      },
      "source": [
        "class RegressionDataset(Dataset):\n",
        "    def __init__(self,df):\n",
        "        self.df = df\n",
        "        self.X, self.Y = self.clean_df()\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.Y)\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.Y[idx]\n",
        "\n",
        "\n",
        "    def clean_df(self):\n",
        "        #drop NaN values, and also load the values into torch.tensor.\n",
        "        df = self.df.dropna()\n",
        "        y_values = df.change.values\n",
        "        df = df.drop (['change','change_label'],axis=1)\n",
        "        x_values = df.values\n",
        "\n",
        "        return torch.from_numpy(x_values).float(), torch.from_numpy(y_values).float()\n",
        "\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IfHatgydCwHs"
      },
      "source": [
        "# define a function for train_test split, and initiate dataset? \n",
        "def split_train_test (df, threshold):\n",
        "    length = len(df)\n",
        "    split_number  = round(threshold*length)\n",
        "    return df[0:split_number], df[split_number:]"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DVI-SUEoD_k2"
      },
      "source": [
        "#for classification dataset construction\n",
        "test_df_train,test_df_eval = split_train_test(test_df_norm, 0.7)\n",
        "train_dataset = ClassifierDataset(test_df_train)\n",
        "eval_dataset = ClassifierDataset (test_df_eval)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OTBt70ZjGOiw"
      },
      "source": [
        "# for Regression dataset construction\n",
        "\n",
        "# test_df_train,test_df_eval = split_train_test(test_df_norm, 0.7)\n",
        "# train_dataset = RegressionDataset(test_df_train)\n",
        "# eval_dataset = RegressionDataset (test_df_eval)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5HfWHePSo1Pa",
        "outputId": "56502d7a-ffeb-4899-840e-fac28c332756"
      },
      "source": [
        "eval_dataset[0]"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([-0.0009,  0.0467, -0.0125,  0.0358,  0.0358, -0.0817, -0.0207,  0.0035,\n",
              "          0.0373, -0.0022, -0.0395,  0.0413,  0.0861, -0.0791,  0.0226,  0.0592,\n",
              "          0.0472,  0.0000,  0.0000]), tensor(0))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMo638lbIbdp"
      },
      "source": [
        "# Format DataLoader, different sampling options"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h0kk1mR_tJI0"
      },
      "source": [
        "#Now let's try to flow the data into dataloader \n",
        "train_loader = DataLoader(train_dataset, batch_size = 64, )\n",
        "eval_loader = DataLoader(eval_dataset, batch_size = 64, )\n",
        "# add in shuffle/sampler options later"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lX7oSm_rOE9R"
      },
      "source": [
        "The above is the normal dataloader. Below I will try to use weighted samplers, which will include oversample and undersamples, and in this case, it will also disrupt the ordering/sequence of the input, ummmmmmmmmmm, How will that affect the model performance, let's seeeeeeeee"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ZvQSmAVOEXy"
      },
      "source": [
        "#obtain the label list. \r\n",
        "target_list = []\r\n",
        "for _,t in train_dataset:\r\n",
        "  target_list.append(t)\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yN4VLeQuZfHh",
        "outputId": "d184fd17-7ce5-42ad-80b9-0a3017dad123"
      },
      "source": [
        "#obtain the class weights\r\n",
        "leng = len(target_list)\r\n",
        "total_true = sum(target_list)\r\n",
        "class_count = [leng-total_true, total_true]\r\n",
        "class_weight = 1./torch.tensor(class_count, dtype=torch.float)\r\n",
        "class_weight"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.0009, 0.0062])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rCvHSf-2apzB"
      },
      "source": [
        "#a weight for each sample\r\n",
        "class_weights_all = class_weight[target_list]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-H4P0ryYe-3B"
      },
      "source": [
        "weighted_sampler = WeightedRandomSampler(\r\n",
        "    weights = class_weights_all,\r\n",
        "    num_samples = leng,\r\n",
        "    replacement = True\r\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J4ku89M3fiah"
      },
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size = 64, sampler=weighted_sampler )\r\n",
        "eval_loader = DataLoader(eval_dataset, batch_size = 64, )\r\n",
        "# add in shuffle/sampler options later"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G_-C0_dCfsSJ",
        "outputId": "764038af-5266-431d-a887-98ab3fc7e162"
      },
      "source": [
        "for i, batch in enumerate(train_loader):\r\n",
        "    _,t = (m for m in batch)\r\n",
        "    print (sum(t)/len(t))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(0.4531)\n",
            "tensor(0.4688)\n",
            "tensor(0.5000)\n",
            "tensor(0.5312)\n",
            "tensor(0.4375)\n",
            "tensor(0.5469)\n",
            "tensor(0.5469)\n",
            "tensor(0.5156)\n",
            "tensor(0.4375)\n",
            "tensor(0.4531)\n",
            "tensor(0.4844)\n",
            "tensor(0.4375)\n",
            "tensor(0.4375)\n",
            "tensor(0.5625)\n",
            "tensor(0.5000)\n",
            "tensor(0.4062)\n",
            "tensor(0.6406)\n",
            "tensor(0.6250)\n",
            "tensor(0.5000)\n",
            "tensor(0.5714)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chR_wC8ytJHm"
      },
      "source": [
        "#next(iter(train_loader))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GFK252mLtJGT"
      },
      "source": [
        "# for i, batch in enumerate(train_loader):\n",
        "#     print (i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aLdhmP5MtJA8"
      },
      "source": [
        "#weighed random sampler test. \n",
        "\n",
        "# import torch\n",
        "# from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler, TensorDataset\n",
        "# train_dataset = torch.tensor([1, 1 , 1, 1, 1, 1, 1, 1, 1, 0])\n",
        "\n",
        "\n",
        "# class_weights_all = [0.1, 0.1, 0.1, 0.1,0.1, 0.1, 0.1, 0.1,0.1, 0.9]\n",
        "\n",
        "# weighted_sampler = WeightedRandomSampler(\n",
        "#     weights=class_weights_all,\n",
        "#     num_samples=10,\n",
        "#     replacement=True #if True, sampler will draw repeating inputs, False will have 0 repeats. \n",
        "# )\n",
        "\n",
        "# BATCH_SIZE = 5\n",
        "# dataset = TensorDataset(train_dataset)\n",
        "# train_loader = DataLoader(dataset,\n",
        "#                           batch_size=BATCH_SIZE,\n",
        "#                           sampler=weighted_sampler\n",
        "# )\n",
        "# for batch in train_loader:\n",
        "#     print(batch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7lIxy82XsK0"
      },
      "source": [
        "#Format Dataset and DataLoader for LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5_t3QEgzXvh3"
      },
      "source": [
        "class LSTMDataset(Dataset):\r\n",
        "    def __init__(self,df,window_size):\r\n",
        "        self.df = df\r\n",
        "        self.window_size = window_size\r\n",
        "        self.X, self.Y = self.clean_df()\r\n",
        "\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        return len(self.Y)\r\n",
        "\r\n",
        "\r\n",
        "    def __getitem__(self, idx):\r\n",
        "        return self.X[idx], self.Y[idx]\r\n",
        "\r\n",
        "\r\n",
        "    def clean_df(self):\r\n",
        "        #drop NaN values, and also load the values into torch.tensor. \r\n",
        "        df = self.df.dropna()\r\n",
        "        y_values = df.change_label.values[self.window_size:]\r\n",
        "        df = df.drop (['change','change_label'],axis=1)\r\n",
        "        #try put the label as part of the feature see if the model cna leran that \r\n",
        "        #df = df.drop (['change'],axis=1)\r\n",
        "        x_values = df.values\r\n",
        "        output = self.process(x_values)\r\n",
        "\r\n",
        "\r\n",
        "        return torch.tensor(output,dtype=torch.float), torch.from_numpy(y_values).long()\r\n",
        "\r\n",
        "    def process(self,data):\r\n",
        "        output = []\r\n",
        "        for i in range(self.window_size-1, len(data)):\r\n",
        "            raw_data = data[i-self.window_size+1:i+1]\r\n",
        "            output.append(raw_data)\r\n",
        "        return output\r\n",
        "\r\n"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D7SAU11Qe11f"
      },
      "source": [
        "# define a function for train_test split, and initiate dataset? \r\n",
        "def split_train_test (df, threshold):\r\n",
        "    length = len(df)\r\n",
        "    split_number  = round(threshold*length)\r\n",
        "    return df[0:split_number], df[split_number:]"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JirYuopfdwYd"
      },
      "source": [
        "#for classification dataset construction\r\n",
        "test_df_train,test_df_eval = split_train_test(test_df_norm, 0.7)\r\n",
        "WINDOW_SIZE = 10 \r\n",
        "train_dataset = LSTMDataset(test_df_train, window_size=WINDOW_SIZE)\r\n",
        "eval_dataset = LSTMDataset (test_df_eval, window_size= WINDOW_SIZE)"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K9nGvys-eNX9",
        "outputId": "d7aa95c6-2765-4c87-fbc2-7bf3a6e2df98"
      },
      "source": [
        "train_dataset[0]"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[ 2.9815e-04,  7.7955e-02, -4.8741e-04,  7.3738e-02,  7.3738e-02,\n",
              "           2.4421e-01,  9.5185e-04,  1.9347e-01,  1.2295e-01,  6.5804e-02,\n",
              "          -5.7147e-02,  1.8713e-01,  5.6772e-01, -1.8079e-01,  4.9743e-02,\n",
              "           7.8442e-02,  0.0000e+00,  2.4160e-04,  0.0000e+00],\n",
              "         [ 1.8731e-03,  4.4611e-02, -2.0406e-03,  2.8687e-02,  2.8687e-02,\n",
              "           1.2097e-01, -5.2953e-02,  9.4539e-02,  4.4277e-02, -1.6947e-03,\n",
              "          -4.5971e-02,  1.6074e-01,  4.1603e-01, -2.2695e-01,  1.1676e-02,\n",
              "           4.6651e-02,  0.0000e+00,  8.0496e-03,  0.0000e+00],\n",
              "         [ 4.7553e-04,  6.1652e-03, -3.5175e-02, -2.2579e-03, -2.2579e-03,\n",
              "          -2.6600e-01, -6.4195e-02,  4.7255e-02,  1.3600e-02, -2.5246e-02,\n",
              "          -3.8846e-02,  1.5229e-01,  3.5183e-01, -2.5732e-01, -7.0176e-03,\n",
              "           4.1340e-02,  0.0000e+00, -7.9853e-03,  0.0000e+00],\n",
              "         [-7.6864e-04,  6.5833e-02, -1.2268e-02,  6.4274e-02,  6.4274e-02,\n",
              "           6.5308e-03, -3.2272e-02,  4.2829e-02,  2.0149e-02, -9.3590e-03,\n",
              "          -2.9508e-02,  1.5119e-01,  3.4520e-01, -2.5954e-01,  4.1259e-02,\n",
              "           7.8101e-02,  0.0000e+00, -9.9815e-03,  0.0000e+00],\n",
              "         [-1.9972e-03,  2.9159e-02, -1.5965e-02,  2.3688e-02,  2.3688e-02,\n",
              "           3.5510e-01, -6.5712e-02, -2.2162e-02, -3.5809e-02, -5.4675e-02,\n",
              "          -1.8865e-02,  1.3917e-01,  2.5617e-01, -3.0050e-01,  8.5837e-03,\n",
              "           4.5125e-02,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
              "         [ 1.4228e-03,  2.1940e-01,  1.4228e-03,  7.7862e-02,  7.7862e-02,\n",
              "           2.1800e+00, -5.4330e-02, -4.4977e-02, -4.6473e-02, -5.2481e-02,\n",
              "          -6.0081e-03,  1.3290e-01,  2.2082e-01, -3.1078e-01,  4.6990e-02,\n",
              "           2.1798e-01,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
              "         [-1.1189e-03,  7.3390e-03, -8.3445e-02, -3.6559e-02, -3.6559e-02,\n",
              "          -5.8426e-01, -9.5395e-02, -1.1792e-01, -1.0868e-01, -1.0780e-01,\n",
              "           8.7805e-04,  1.1612e-01,  1.1432e-01, -3.5017e-01, -3.3920e-02,\n",
              "           9.0784e-02,  0.0000e+00,  1.1464e-02,  0.0000e+00],\n",
              "         [-4.7061e-04,  1.1614e-02, -1.3826e-01, -1.1219e-01, -1.1219e-01,\n",
              "          -1.0541e-03, -5.7263e-02, -9.5369e-02, -7.7987e-02, -7.9887e-02,\n",
              "          -1.8997e-03,  1.1371e-01,  1.3206e-01, -3.2280e-01, -7.3882e-02,\n",
              "           1.4988e-01,  0.0000e+00, -7.8778e-03,  0.0000e+00],\n",
              "         [-2.4239e-03,  2.0482e-02, -5.6444e-02, -1.7185e-03, -1.7185e-03,\n",
              "          -2.7369e-01,  6.1936e-02,  8.8042e-03,  3.5189e-02,  3.0479e-02,\n",
              "          -4.7092e-03,  1.1584e-01,  2.4048e-01, -2.2287e-01,  1.3238e-02,\n",
              "           7.6925e-02,  0.0000e+00, -2.1552e-02,  1.0000e+00],\n",
              "         [-3.1733e-03,  3.9989e-02, -3.2863e-02, -3.0354e-02, -3.0354e-02,\n",
              "          -1.7410e-01,  5.9816e-02, -2.5069e-03,  3.1433e-02,  2.2557e-02,\n",
              "          -8.8765e-03,  1.0831e-01,  2.1411e-01, -2.1913e-01, -1.5242e-02,\n",
              "           7.2852e-02,  0.0000e+00,  2.2276e-02,  1.0000e+00]]), tensor(1))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r5QoruL2evOV"
      },
      "source": [
        "#a vanilla dataloader\r\n",
        "\r\n",
        "train_loader = DataLoader(train_dataset, batch_size = 64, )\r\n",
        "eval_loader = DataLoader(eval_dataset, batch_size = 64, )\r\n"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1GBZUjaKg78J"
      },
      "source": [
        "#weighted_dataloader\r\n",
        "\r\n",
        "#obtain the label list. \r\n",
        "target_list = []\r\n",
        "for _,t in train_dataset:\r\n",
        "  target_list.append(t)\r\n",
        "\r\n",
        "#obtain the class weights\r\n",
        "leng = len(target_list)\r\n",
        "total_true = sum(target_list)\r\n",
        "class_count = [leng-total_true, total_true]\r\n",
        "class_weight = 1./torch.tensor(class_count, dtype=torch.float)\r\n",
        "\r\n",
        "\r\n",
        "#a weight for each sample\r\n",
        "class_weights_all = class_weight[target_list]\r\n",
        "\r\n",
        "weighted_sampler = WeightedRandomSampler(\r\n",
        "    weights = class_weights_all,\r\n",
        "    num_samples = leng,\r\n",
        "    replacement = True\r\n",
        ")"
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2o6_8iXAhUB1"
      },
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size = 64, sampler=weighted_sampler )\r\n",
        "eval_loader = DataLoader(eval_dataset, batch_size = 64, )\r\n",
        "# add in shuffle/sampler options later"
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5xgqDc28hZ4D",
        "outputId": "ca9458b4-d344-4823-9482-12b00c7ee408"
      },
      "source": [
        "next(iter(train_loader))"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[tensor([[[-5.0697e-05,  5.1311e-03, -2.5842e-04,  ...,  6.3146e-03,\n",
              "            7.4544e-04,  0.0000e+00],\n",
              "          [ 4.8668e-04,  4.0088e-02,  3.4721e-05,  ..., -8.5336e-03,\n",
              "           -2.2346e-03,  0.0000e+00],\n",
              "          [ 4.9744e-04,  5.8346e-03, -5.1849e-03,  ..., -2.9408e-03,\n",
              "            0.0000e+00,  0.0000e+00],\n",
              "          ...,\n",
              "          [-4.5472e-04,  3.0959e-03, -6.0307e-03,  ..., -3.6053e-02,\n",
              "            4.7104e-03,  0.0000e+00],\n",
              "          [ 9.2511e-04,  4.4589e-02,  5.8145e-04,  ..., -5.7780e-02,\n",
              "            2.9610e-03,  0.0000e+00],\n",
              "          [-5.0530e-04, -5.0530e-04, -2.5199e-02,  ...,  7.0678e-02,\n",
              "            0.0000e+00,  0.0000e+00]],\n",
              " \n",
              "         [[ 7.5962e-06,  1.0283e-02, -1.9245e-02,  ...,  1.2855e-01,\n",
              "            3.1564e-03,  0.0000e+00],\n",
              "          [ 1.0330e-03,  8.1429e-03, -9.6471e-04,  ..., -2.5404e-02,\n",
              "           -9.1082e-03,  0.0000e+00],\n",
              "          [ 6.0491e-05,  1.9761e-02, -5.1975e-03,  ...,  1.5567e-01,\n",
              "            0.0000e+00,  0.0000e+00],\n",
              "          ...,\n",
              "          [ 2.0810e-03,  5.2261e-02, -4.7865e-03,  ..., -2.6633e-02,\n",
              "            1.8837e-03,  0.0000e+00],\n",
              "          [ 8.7441e-04,  1.5012e-02, -8.3255e-03,  ...,  3.6176e-02,\n",
              "           -2.2221e-03,  0.0000e+00],\n",
              "          [ 4.2432e-04,  1.1350e-02, -1.2060e-02,  ..., -3.8913e-02,\n",
              "            0.0000e+00,  0.0000e+00]],\n",
              " \n",
              "         [[ 8.9137e-04,  1.7317e-02, -4.2144e-02,  ..., -1.5380e-04,\n",
              "            1.2939e-02,  0.0000e+00],\n",
              "          [ 2.5148e-03,  1.3127e-02, -2.4091e-02,  ..., -2.4425e-02,\n",
              "            5.2504e-03,  0.0000e+00],\n",
              "          [-6.3809e-03,  1.6024e-03, -2.5267e-02,  ..., -4.0879e-03,\n",
              "            0.0000e+00,  0.0000e+00],\n",
              "          ...,\n",
              "          [ 8.0243e-04,  1.5485e-02, -6.7233e-02,  ...,  2.2481e-02,\n",
              "            1.1686e-02,  1.0000e+00],\n",
              "          [ 2.8444e-04,  4.6053e-02, -3.9184e-03,  ...,  4.5915e-03,\n",
              "            0.0000e+00,  1.0000e+00],\n",
              "          [ 4.2346e-03,  1.2536e-02, -6.4051e-02,  ..., -4.2501e-02,\n",
              "            0.0000e+00,  1.0000e+00]],\n",
              " \n",
              "         ...,\n",
              " \n",
              "         [[-7.5782e-04,  7.3287e-03, -5.1042e-03,  ...,  0.0000e+00,\n",
              "           -5.3172e-03,  0.0000e+00],\n",
              "          [ 2.3565e-04,  3.4096e-03, -2.5472e-03,  ...,  0.0000e+00,\n",
              "            2.9401e-03,  0.0000e+00],\n",
              "          [-1.0199e-04,  2.5485e-03, -1.3151e-02,  ...,  0.0000e+00,\n",
              "            0.0000e+00,  0.0000e+00],\n",
              "          ...,\n",
              "          [ 0.0000e+00,  5.8858e-03, -1.7065e-03,  ...,  0.0000e+00,\n",
              "           -2.6487e-04,  0.0000e+00],\n",
              "          [-2.6957e-04,  1.1610e-02, -3.3567e-03,  ...,  0.0000e+00,\n",
              "            4.4158e-03,  0.0000e+00],\n",
              "          [ 1.7215e-04,  1.0568e-02, -1.7125e-03,  ...,  0.0000e+00,\n",
              "            0.0000e+00,  0.0000e+00]],\n",
              " \n",
              "         [[ 2.9615e-03,  2.2010e-02, -5.1474e-02,  ..., -4.9487e-03,\n",
              "            0.0000e+00,  0.0000e+00],\n",
              "          [ 3.6495e-03,  1.5018e-02, -9.8335e-03,  ...,  3.9438e-02,\n",
              "            2.3656e-03,  0.0000e+00],\n",
              "          [ 1.2744e-03,  1.9541e-02, -2.5121e-03,  ...,  3.9342e-02,\n",
              "            7.7879e-03,  0.0000e+00],\n",
              "          ...,\n",
              "          [ 1.0157e-03,  5.0474e-03, -4.8430e-02,  ..., -3.8659e-02,\n",
              "            0.0000e+00,  0.0000e+00],\n",
              "          [ 1.6120e-03,  2.8204e-02, -2.1958e-02,  ..., -1.6146e-02,\n",
              "            9.4024e-04,  0.0000e+00],\n",
              "          [ 5.3581e-03,  3.7422e-02, -2.0349e-03,  ...,  4.3538e-02,\n",
              "            7.8277e-04,  0.0000e+00]],\n",
              " \n",
              "         [[-2.4722e-03,  2.6952e-02, -2.4722e-03,  ...,  0.0000e+00,\n",
              "            0.0000e+00,  0.0000e+00],\n",
              "          [ 6.9312e-06,  6.0979e-03, -2.1401e-02,  ...,  0.0000e+00,\n",
              "           -5.4147e-03,  0.0000e+00],\n",
              "          [-1.5342e-03, -5.9196e-04, -1.2481e-01,  ...,  0.0000e+00,\n",
              "            1.9599e-02,  0.0000e+00],\n",
              "          ...,\n",
              "          [-2.4258e-04,  3.4845e-02, -2.3194e-02,  ...,  0.0000e+00,\n",
              "            0.0000e+00,  0.0000e+00],\n",
              "          [-6.1106e-04,  9.8598e-04, -8.4670e-02,  ...,  0.0000e+00,\n",
              "            2.4499e-03,  0.0000e+00],\n",
              "          [ 7.6427e-03,  1.4638e-02, -3.6971e-02,  ...,  0.0000e+00,\n",
              "            1.3966e-03,  1.0000e+00]]]),\n",
              " tensor([1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,\n",
              "         0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,\n",
              "         1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0])]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mruUFVdyIxfH"
      },
      "source": [
        "\n",
        "\n",
        "# Torch Model 1 - FeedForward Classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2zH_3fttI6v"
      },
      "source": [
        "#let's first try a simple FC NN\n",
        "class FFNN (nn.Module):\n",
        "    def __init__(self, input_size, num_classes,num_hidden, hidden_dim ):\n",
        "        super().__init__()\n",
        "\n",
        "        assert num_hidden > 0 \n",
        "\n",
        "        self.fc1 = nn.Linear(input_size,hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim,hidden_dim)\n",
        "\n",
        "        self.hidden_layers = nn.ModuleList([])\n",
        "        self.hidden_layers.append (self.fc1)\n",
        "        for i in range (num_hidden -1 ):\n",
        "            self.hidden_layers.append(self.fc2)\n",
        "        \n",
        "        self.final_layer = nn.Linear (hidden_dim,num_classes)\n",
        "        self.dropout = nn.Dropout()\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self,x):\n",
        "        for hidden_layer in self.hidden_layers:\n",
        "            x = hidden_layer(x)\n",
        "            x = self.dropout(x)\n",
        "            x = self.relu(x)\n",
        "        \n",
        "        out = self.final_layer(x)\n",
        "        out_dist = F.log_softmax(out, dim= -1) #why is it -1, ok, coz it's batch x input x class. \n",
        "        \n",
        "        return out_dist\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pbT1kH2jFB6E"
      },
      "source": [
        "#try initiating the model\n",
        "INPUT_SIZE = 19\n",
        "NUM_CLASSES = 2\n",
        "NUM_HIDDEN = 2\n",
        "HIDDEN_DIM = 512\n",
        "\n",
        "model = FFNN(INPUT_SIZE, NUM_CLASSES, NUM_HIDDEN, HIDDEN_DIM)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zgc97Ziqkn1G",
        "outputId": "b5f16ab8-ac9a-41a3-fadf-b818372fd1b0"
      },
      "source": [
        "model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FFNN(\n",
              "  (fc1): Linear(in_features=19, out_features=512, bias=True)\n",
              "  (fc2): Linear(in_features=512, out_features=512, bias=True)\n",
              "  (hidden_layers): ModuleList(\n",
              "    (0): Linear(in_features=19, out_features=512, bias=True)\n",
              "    (1): Linear(in_features=512, out_features=512, bias=True)\n",
              "  )\n",
              "  (final_layer): Linear(in_features=512, out_features=2, bias=True)\n",
              "  (dropout): Dropout(p=0.5, inplace=False)\n",
              "  (relu): ReLU()\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 284
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3nW0_Vjqk4Ec",
        "outputId": "9146a36e-d0c9-48fd-a6fc-df5c322ce5e0"
      },
      "source": [
        "#print model parameters\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 273,922 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gy_YDstOu6kG"
      },
      "source": [
        "#define loss fuction and optimizer\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "loss_fn = nn.NLLLoss()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhKFYZjEvrJ9"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "loss_fn = loss_fn.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6ejBYTW2rVg"
      },
      "source": [
        "#define a function to calcualte prediction accuracy\n",
        "def accuracy(pred,label):\n",
        "    _,pred_label = torch.max(pred,dim=1)\n",
        "    correct = (pred_label==label).float()\n",
        "    accuracy = correct.sum()/len(correct)\n",
        "    return accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lGnVYr78lcs0"
      },
      "source": [
        "#can i define a function to calculate the f1 score/confusion matrix? \r\n",
        "def get_cfm (pred, label):\r\n",
        "    _,pred_label = torch.max(pred,dim=1)\r\n",
        "    m = classification_report(pred_label.cpu(), label.cpu())\r\n",
        "    return m\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TV01Q5NlvukI"
      },
      "source": [
        "#now define the training and evaluation loop\n",
        "def train(model, dataloader, optimizer, loss_fn):\n",
        "    model.train()\n",
        "\n",
        "    epoch_loss= 0\n",
        "    epoch_acc = 0\n",
        "\n",
        "    for step,batch in enumerate(dataloader):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        data, label = (t for t in batch)\n",
        "        data = data.to(device)\n",
        "        label = label.to(device)\n",
        "\n",
        "        pred = model(data)\n",
        "        loss = loss_fn(pred,label)\n",
        "        acc  = accuracy(pred,label)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc\n",
        "\n",
        "    return epoch_loss/len(dataloader), epoch_acc/len(dataloader)\n",
        "        \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a07XNBqh5HdM"
      },
      "source": [
        "def evaluate(model,dataloader,loss_fn):\n",
        "    model.eval()\n",
        "\n",
        "    epoch_loss = 0 \n",
        "    epoch_acc = 0 \n",
        "\n",
        "    pred_list =[]\n",
        "    label_list =[]\n",
        "    for step,batch in enumerate(dataloader):\n",
        "        data,label = (t for t in batch)\n",
        "        data = data.to(device)\n",
        "        label = label.to(device)\n",
        "\n",
        "        pred = model(data)\n",
        "        loss = loss_fn(pred,label)\n",
        "        acc = accuracy(pred,label)\n",
        "        \n",
        "        #append the prediction result and show it per epoch later. \n",
        "        if step ==0:     \n",
        "            pred_list=pred.cpu().detach().numpy()\n",
        "            label_list = label.cpu().detach().numpy()\n",
        "        if step != 0:\n",
        "            pred_list = np.concatenate([pred_list,pred.cpu().detach().numpy()],axis =0)\n",
        "            label_list = np.concatenate([label_list,label.cpu().detach().numpy()],axis =0)\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc\n",
        "\n",
        "    report = get_cfm(torch.tensor(pred_list),torch.tensor(label_list))\n",
        "    print (report)\n",
        "    return epoch_loss/len(dataloader), epoch_acc/len(dataloader)\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_EZeClpayNzv",
        "outputId": "2e20fe82-7dfc-48c7-d45c-54395db7bdcd"
      },
      "source": [
        "N_EPOCHS = 5\n",
        "\n",
        "for i in range(N_EPOCHS):\n",
        "    train_loss,train_acc = train(model,train_loader, optimizer, loss_fn)\n",
        "    \n",
        "    eval_loss, eval_acc = evaluate(model,eval_loader, loss_fn)\n",
        "    \n",
        "    print (f' Epoch Number {i}') \n",
        "    print (f' Train. Loss: {train_loss:.3f} Train. Acc: {train_acc*100:.2f}%')\n",
        "    print (f' Eval loss , {eval_loss:.3f}, eval acc, {eval_acc*100:.2f} %')\n",
        "    # print(f'\\t Train. Loss: {train_loss:.3f} |  Train. Acc: {train_acc*100:.2f}%')\n",
        "    # print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.93      0.90       466\n",
            "           1       0.33      0.21      0.26        81\n",
            "\n",
            "    accuracy                           0.82       547\n",
            "   macro avg       0.60      0.57      0.58       547\n",
            "weighted avg       0.79      0.82      0.80       547\n",
            "\n",
            " Epoch Number 0\n",
            " Train. Loss: 0.673 Train. Acc: 54.78%\n",
            " Eval loss , 0.601, eval acc, 82.41 %\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.93      0.88       445\n",
            "           1       0.35      0.18      0.24       102\n",
            "\n",
            "    accuracy                           0.79       547\n",
            "   macro avg       0.59      0.55      0.56       547\n",
            "weighted avg       0.74      0.79      0.76       547\n",
            "\n",
            " Epoch Number 1\n",
            " Train. Loss: 0.610 Train. Acc: 68.18%\n",
            " Eval loss , 0.541, eval acc, 79.11 %\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.92      0.88       449\n",
            "           1       0.33      0.17      0.23        98\n",
            "\n",
            "    accuracy                           0.79       547\n",
            "   macro avg       0.59      0.55      0.55       547\n",
            "weighted avg       0.75      0.79      0.76       547\n",
            "\n",
            " Epoch Number 2\n",
            " Train. Loss: 0.587 Train. Acc: 69.86%\n",
            " Eval loss , 0.484, eval acc, 79.60 %\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.77      0.93      0.85       410\n",
            "           1       0.47      0.18      0.26       137\n",
            "\n",
            "    accuracy                           0.74       547\n",
            "   macro avg       0.62      0.55      0.55       547\n",
            "weighted avg       0.70      0.74      0.70       547\n",
            "\n",
            " Epoch Number 3\n",
            " Train. Loss: 0.591 Train. Acc: 70.13%\n",
            " Eval loss , 0.547, eval acc, 75.26 %\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.92      0.88       449\n",
            "           1       0.33      0.17      0.23        98\n",
            "\n",
            "    accuracy                           0.79       547\n",
            "   macro avg       0.59      0.55      0.55       547\n",
            "weighted avg       0.75      0.79      0.76       547\n",
            "\n",
            " Epoch Number 4\n",
            " Train. Loss: 0.575 Train. Acc: 71.00%\n",
            " Eval loss , 0.481, eval acc, 79.60 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mHUUqSS434YB"
      },
      "source": [
        "# #obtain the label list. \r\n",
        "# target_list = []\r\n",
        "# for _,t in eval_dataset:\r\n",
        "#   target_list.append(t)\r\n",
        "\r\n",
        "# #obtain the class weights\r\n",
        "# leng = len(target_list)\r\n",
        "# total_true = sum(target_list)\r\n",
        "# class_count = [leng-total_true, total_true]\r\n",
        "# class_count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KRZib4vB9pq7"
      },
      "source": [
        "# for step,batch in enumerate(eval_loader):\r\n",
        "\r\n",
        "#     data,label = (t for t in batch)\r\n",
        "#     data = data.to(device)\r\n",
        "#     label = label.to(device)\r\n",
        "\r\n",
        "#     pred = model(data)\r\n",
        "#     loss = loss_fn(pred,label)\r\n",
        "#     acc = accuracy(pred,label)\r\n",
        "#     report = get_cfm(pred,label)\r\n",
        "    \r\n",
        "#     epoch_loss += loss.item()\r\n",
        "#     epoch_acc += acc     \r\n",
        "\r\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jy1Db8zvm_fU"
      },
      "source": [
        "    # pred_list =[]\r\n",
        "    # label_list =[]\r\n",
        "    # for step,batch in enumerate(eval_loader):\r\n",
        "    #     data,label = (t for t in batch)\r\n",
        "    #     data = data.to(device)\r\n",
        "    #     label = label.to(device)\r\n",
        "\r\n",
        "    #     pred = model(data)\r\n",
        "    #     loss = loss_fn(pred,label)\r\n",
        "    #     acc = accuracy(pred,label)\r\n",
        "    #     if step ==0:\r\n",
        "    #         pred_list=pred.cpu().detach().numpy()\r\n",
        "    #         label_list = label.cpu().detach().numpy()\r\n",
        "    #     if step != 0:\r\n",
        "    #         pred_list = np.concatenate([pred_list,pred.cpu().detach().numpy()],axis =0)\r\n",
        "    #         label_list = np.concatenate([label_list,label.cpu().detach().numpy()],axis =0)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "    # report = get_cfm(torch.tensor(pred_list),torch.tensor(label_list))\r\n",
        "    # print (report)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6RRyt4SystDX",
        "outputId": "a018c6ae-29a4-4958-d39c-72c08bea8741"
      },
      "source": [
        "np.empty([1,2])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.76096972, 0.50680272]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 296
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bf_54Mrvw1aO"
      },
      "source": [
        "# Torch Model 2 - Feedforward Classification with F1 loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8hcgMTYw7h6"
      },
      "source": [
        "#let's first try a simple FC NN\r\n",
        "class FFNN_F1 (nn.Module):\r\n",
        "    def __init__(self, input_size, num_classes,num_hidden, hidden_dim ):\r\n",
        "        super().__init__()\r\n",
        "\r\n",
        "        assert num_hidden > 0 \r\n",
        "\r\n",
        "        self.fc1 = nn.Linear(input_size,hidden_dim)\r\n",
        "        self.fc2 = nn.Linear(hidden_dim,hidden_dim)\r\n",
        "\r\n",
        "        self.hidden_layers = nn.ModuleList([])\r\n",
        "        self.hidden_layers.append (self.fc1)\r\n",
        "        for i in range (num_hidden -1 ):\r\n",
        "            self.hidden_layers.append(self.fc2)\r\n",
        "        \r\n",
        "        self.final_layer = nn.Linear (hidden_dim,num_classes)\r\n",
        "        self.dropout = nn.Dropout()\r\n",
        "        self.relu = nn.ReLU()\r\n",
        "\r\n",
        "    def forward(self,x):\r\n",
        "        for hidden_layer in self.hidden_layers:\r\n",
        "            x = hidden_layer(x)\r\n",
        "            x = self.dropout(x)\r\n",
        "            x = self.relu(x)\r\n",
        "        \r\n",
        "        out = self.final_layer(x)\r\n",
        "               \r\n",
        "        return out\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5v_3EXVoYrtc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JNYld1FHw7fq",
        "outputId": "e7f2f390-5bf2-4ece-df2f-bf8a53da78f0"
      },
      "source": [
        "#try initiating the model\r\n",
        "INPUT_SIZE = 19\r\n",
        "NUM_CLASSES = 2\r\n",
        "NUM_HIDDEN = 2\r\n",
        "HIDDEN_DIM = 512\r\n",
        "\r\n",
        "model = FFNN_F1(INPUT_SIZE, NUM_CLASSES, NUM_HIDDEN, HIDDEN_DIM)\r\n",
        "\r\n",
        "model\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FFNN_F1(\n",
              "  (fc1): Linear(in_features=19, out_features=512, bias=True)\n",
              "  (fc2): Linear(in_features=512, out_features=512, bias=True)\n",
              "  (hidden_layers): ModuleList(\n",
              "    (0): Linear(in_features=19, out_features=512, bias=True)\n",
              "    (1): Linear(in_features=512, out_features=512, bias=True)\n",
              "  )\n",
              "  (final_layer): Linear(in_features=512, out_features=2, bias=True)\n",
              "  (dropout): Dropout(p=0.5, inplace=False)\n",
              "  (relu): ReLU()\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jKh99uxBY6ie",
        "outputId": "bc4d55f7-9c5d-4f0b-b15b-defcfd488aa4"
      },
      "source": [
        "#print model parameters\r\n",
        "def count_parameters(model):\r\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\r\n",
        "\r\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 273,922 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xb7-RwCBw7dp",
        "outputId": "b2d9ad1d-e510-492e-d5d7-d56bcbc5862f"
      },
      "source": [
        "#define optimizer, loss function\r\n",
        "class F1_Loss(nn.Module):\r\n",
        "    '''Calculate F1 score. Can work with gpu tensors\r\n",
        "    \r\n",
        "    The original implmentation is written by Michal Haltuf on Kaggle.\r\n",
        "    \r\n",
        "    Returns\r\n",
        "    -------\r\n",
        "    torch.Tensor\r\n",
        "        `ndim` == 1. epsilon <= val <= 1\r\n",
        "    \r\n",
        "    Reference\r\n",
        "    ---------\r\n",
        "    - https://www.kaggle.com/rejpalcz/best-loss-function-for-f1-score-metric\r\n",
        "    - https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score\r\n",
        "    - https://discuss.pytorch.org/t/calculating-precision-recall-and-f1-score-in-case-of-multi-label-classification/28265/6\r\n",
        "    - http://www.ryanzhang.info/python/writing-your-own-loss-function-module-for-pytorch/\r\n",
        "    '''\r\n",
        "    def __init__(self, epsilon=1e-7):\r\n",
        "        super().__init__()\r\n",
        "        self.epsilon = epsilon\r\n",
        "        \r\n",
        "    def forward(self, y_pred, y_true,):\r\n",
        "        assert y_pred.ndim == 2\r\n",
        "        assert y_true.ndim == 1\r\n",
        "        y_true = F.one_hot(y_true, 2).to(torch.float32)\r\n",
        "        y_pred = F.softmax(y_pred, dim=1)\r\n",
        "        \r\n",
        "        tp = (y_true * y_pred).sum(dim=0).to(torch.float32)\r\n",
        "        tn = ((1 - y_true) * (1 - y_pred)).sum(dim=0).to(torch.float32)\r\n",
        "        fp = ((1 - y_true) * y_pred).sum(dim=0).to(torch.float32)\r\n",
        "        fn = (y_true * (1 - y_pred)).sum(dim=0).to(torch.float32)\r\n",
        "\r\n",
        "        precision = tp / (tp + fp + self.epsilon)\r\n",
        "        recall = tp / (tp + fn + self.epsilon)\r\n",
        "\r\n",
        "        f1 = 2* (precision*recall) / (precision + recall + self.epsilon)\r\n",
        "        f1 = f1.clamp(min=self.epsilon, max=1-self.epsilon)\r\n",
        "        return 1 - f1.mean()\r\n",
        "\r\n",
        "\r\n",
        "params = model.parameters()\r\n",
        "optimizer = optim.Adam(params)\r\n",
        "loss_fn =F1_Loss()\r\n",
        "\r\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\n",
        "model = model.to(device)\r\n",
        "loss_fn = loss_fn.to(device)\r\n",
        "\r\n",
        "print (device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8hgaRdJEw7ak"
      },
      "source": [
        "def accuracy(preds, y):\r\n",
        "    \"\"\"\r\n",
        "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    #round predictions to the closest integer\r\n",
        "    prediciton = F.softmax(preds, dim =1)\r\n",
        "    _, pred = torch.max(prediciton, 1)\r\n",
        "    correct = (pred == y).float() #convert into float for division \r\n",
        "    acc = correct.sum() / len(correct)\r\n",
        "    return acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0M9YnRqa1_l"
      },
      "source": [
        "#can i define a function to calculate the f1 score/confusion matrix? \r\n",
        "def get_cfm (preds, label):\r\n",
        "    pred = F.softmax(preds, dim =1)\r\n",
        "    _,pred_label = torch.max(pred,dim=1)\r\n",
        "    m = classification_report(pred_label.cpu(), label.cpu())\r\n",
        "    return m"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3kn6Ua6KZZYF"
      },
      "source": [
        "#now define the training and evaluation loop\r\n",
        "def train(model, dataloader, optimizer, loss_fn):\r\n",
        "    model.train()\r\n",
        "\r\n",
        "    epoch_loss= 0\r\n",
        "    epoch_acc = 0\r\n",
        "\r\n",
        "    for step,batch in enumerate(dataloader):\r\n",
        "        optimizer.zero_grad()\r\n",
        "\r\n",
        "        data, label = (t for t in batch)\r\n",
        "        data = data.to(device)\r\n",
        "        label = label.to(device)\r\n",
        "\r\n",
        "        pred = model(data)\r\n",
        "        loss = loss_fn(pred,label)\r\n",
        "        acc  = accuracy(pred,label)\r\n",
        "\r\n",
        "        loss.backward()\r\n",
        "        optimizer.step()\r\n",
        "\r\n",
        "        epoch_loss += loss.item()\r\n",
        "        epoch_acc += acc\r\n",
        "\r\n",
        "    return epoch_loss/len(dataloader), epoch_acc/len(dataloader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bUWmYf6mZZVI"
      },
      "source": [
        "def evaluate(model,dataloader,loss_fn):\r\n",
        "    model.eval()\r\n",
        "\r\n",
        "    epoch_loss = 0 \r\n",
        "    epoch_acc = 0 \r\n",
        "\r\n",
        "    pred_list =[]\r\n",
        "    label_list =[]\r\n",
        "    for step,batch in enumerate(dataloader):\r\n",
        "        data,label = (t for t in batch)\r\n",
        "        data = data.to(device)\r\n",
        "        label = label.to(device)\r\n",
        "\r\n",
        "        pred = model(data)\r\n",
        "        loss = loss_fn(pred,label)\r\n",
        "        acc = accuracy(pred,label)\r\n",
        "        \r\n",
        "        #append the prediction result and show it per epoch later. \r\n",
        "        if step ==0:     \r\n",
        "            pred_list=pred.cpu().detach().numpy()\r\n",
        "            label_list = label.cpu().detach().numpy()\r\n",
        "        if step != 0:\r\n",
        "            pred_list = np.concatenate([pred_list,pred.cpu().detach().numpy()],axis =0)\r\n",
        "            label_list = np.concatenate([label_list,label.cpu().detach().numpy()],axis =0)\r\n",
        "\r\n",
        "        epoch_loss += loss.item()\r\n",
        "        epoch_acc += acc\r\n",
        "\r\n",
        "    report = get_cfm(torch.tensor(pred_list),torch.tensor(label_list))\r\n",
        "    print (report)\r\n",
        "    return epoch_loss/len(dataloader), epoch_acc/len(dataloader)\r\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PvIYSXJCZZSe",
        "outputId": "fe276acd-01e6-41f9-9c45-9ce877d28456"
      },
      "source": [
        "N_EPOCHS = 10\r\n",
        "\r\n",
        "for i in range(N_EPOCHS):\r\n",
        "    train_loss,train_acc = train(model,train_loader, optimizer, loss_fn)\r\n",
        "    \r\n",
        "    eval_loss, eval_acc = evaluate(model,eval_loader, loss_fn)\r\n",
        "    \r\n",
        "    print (f' Epoch Number {i}') \r\n",
        "    print (f' Train. Loss: {train_loss:.3f} Train. Acc: {train_acc*100:.2f}%')\r\n",
        "    print (f' Eval loss , {eval_loss:.3f}, eval acc, {eval_acc*100:.2f} %')\r\n",
        "    # print(f'\\t Train. Loss: {train_loss:.3f} |  Train. Acc: {train_acc*100:.2f}%')\r\n",
        "    # print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.92      0.90       474\n",
            "           1       0.27      0.19      0.23        73\n",
            "\n",
            "    accuracy                           0.82       547\n",
            "   macro avg       0.58      0.56      0.56       547\n",
            "weighted avg       0.80      0.82      0.81       547\n",
            "\n",
            " Epoch Number 0\n",
            " Train. Loss: 0.481 Train. Acc: 57.39%\n",
            " Eval loss , 0.571, eval acc, 82.76 %\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.93      0.88       444\n",
            "           1       0.39      0.19      0.26       103\n",
            "\n",
            "    accuracy                           0.79       547\n",
            "   macro avg       0.61      0.56      0.57       547\n",
            "weighted avg       0.75      0.79      0.76       547\n",
            "\n",
            " Epoch Number 1\n",
            " Train. Loss: 0.427 Train. Acc: 62.00%\n",
            " Eval loss , 0.521, eval acc, 79.63 %\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.73      0.94      0.82       383\n",
            "           1       0.55      0.17      0.26       164\n",
            "\n",
            "    accuracy                           0.71       547\n",
            "   macro avg       0.64      0.56      0.54       547\n",
            "weighted avg       0.67      0.71      0.65       547\n",
            "\n",
            " Epoch Number 2\n",
            " Train. Loss: 0.353 Train. Acc: 68.11%\n",
            " Eval loss , 0.534, eval acc, 71.96 %\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.93      0.86       424\n",
            "           1       0.43      0.18      0.25       123\n",
            "\n",
            "    accuracy                           0.76       547\n",
            "   macro avg       0.61      0.56      0.56       547\n",
            "weighted avg       0.71      0.76      0.72       547\n",
            "\n",
            " Epoch Number 3\n",
            " Train. Loss: 0.331 Train. Acc: 69.19%\n",
            " Eval loss , 0.505, eval acc, 77.00 %\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.93      0.86       426\n",
            "           1       0.39      0.17      0.23       121\n",
            "\n",
            "    accuracy                           0.76       547\n",
            "   macro avg       0.59      0.55      0.54       547\n",
            "weighted avg       0.71      0.76      0.72       547\n",
            "\n",
            " Epoch Number 4\n",
            " Train. Loss: 0.304 Train. Acc: 71.08%\n",
            " Eval loss , 0.502, eval acc, 76.51 %\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.71      0.94      0.81       375\n",
            "           1       0.57      0.17      0.26       172\n",
            "\n",
            "    accuracy                           0.70       547\n",
            "   macro avg       0.64      0.55      0.54       547\n",
            "weighted avg       0.67      0.70      0.64       547\n",
            "\n",
            " Epoch Number 5\n",
            " Train. Loss: 0.324 Train. Acc: 67.98%\n",
            " Eval loss , 0.520, eval acc, 70.78 %\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.93      0.85       419\n",
            "           1       0.41      0.16      0.23       128\n",
            "\n",
            "    accuracy                           0.75       547\n",
            "   macro avg       0.60      0.55      0.54       547\n",
            "weighted avg       0.70      0.75      0.71       547\n",
            "\n",
            " Epoch Number 6\n",
            " Train. Loss: 0.320 Train. Acc: 69.38%\n",
            " Eval loss , 0.505, eval acc, 75.78 %\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.93      0.85       417\n",
            "           1       0.41      0.16      0.23       130\n",
            "\n",
            "    accuracy                           0.75       547\n",
            "   macro avg       0.60      0.54      0.54       547\n",
            "weighted avg       0.69      0.75      0.70       547\n",
            "\n",
            " Epoch Number 7\n",
            " Train. Loss: 0.319 Train. Acc: 69.08%\n",
            " Eval loss , 0.507, eval acc, 75.44 %\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.93      0.85       417\n",
            "           1       0.43      0.17      0.24       130\n",
            "\n",
            "    accuracy                           0.75       547\n",
            "   macro avg       0.61      0.55      0.55       547\n",
            "weighted avg       0.70      0.75      0.71       547\n",
            "\n",
            " Epoch Number 8\n",
            " Train. Loss: 0.300 Train. Acc: 70.80%\n",
            " Eval loss , 0.508, eval acc, 75.78 %\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.75      0.94      0.84       397\n",
            "           1       0.53      0.18      0.27       150\n",
            "\n",
            "    accuracy                           0.73       547\n",
            "   macro avg       0.64      0.56      0.55       547\n",
            "weighted avg       0.69      0.73      0.68       547\n",
            "\n",
            " Epoch Number 9\n",
            " Train. Loss: 0.302 Train. Acc: 70.76%\n",
            " Eval loss , 0.513, eval acc, 73.90 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XqSIcZPr9qCY"
      },
      "source": [
        "# Torch Model 3 - Feedforward Regresssion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U02qHiN79wNe"
      },
      "source": [
        "#let's first try a simple FC NN\n",
        "class FFNN_regression (nn.Module):\n",
        "    def __init__(self, input_size,num_hidden, hidden_dim ):\n",
        "        super().__init__()\n",
        "\n",
        "        assert num_hidden > 0 \n",
        "\n",
        "        self.fc1 = nn.Linear(input_size,hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim,hidden_dim)\n",
        "\n",
        "        self.hidden_layers = nn.ModuleList([])\n",
        "        self.hidden_layers.append (self.fc1)\n",
        "        for i in range (num_hidden -1 ):\n",
        "            self.hidden_layers.append(self.fc2)\n",
        "        \n",
        "        self.final_layer = nn.Linear (hidden_dim,1)\n",
        "        self.dropout = nn.Dropout()\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self,x):\n",
        "        for hidden_layer in self.hidden_layers:\n",
        "            x = hidden_layer(x)\n",
        "            x = self.dropout(x)\n",
        "            x = self.relu(x)\n",
        "        \n",
        "        out = self.final_layer(x)\n",
        "        \n",
        "        return out\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oOkl8d-M-EtB"
      },
      "source": [
        "#try initiating the model\n",
        "INPUT_SIZE = 18\n",
        "NUM_HIDDEN = 4\n",
        "HIDDEN_DIM = 1024\n",
        "\n",
        "model = FFNN_regression(INPUT_SIZE, NUM_HIDDEN, HIDDEN_DIM)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gE-ZITBl-EqZ"
      },
      "source": [
        "#define loss fuction and optimizer\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "loss_fn = nn.L1Loss()\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "loss_fn = loss_fn.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qQhSlaHL-Eit"
      },
      "source": [
        "#now define the training and evaluation loop\n",
        "def train(model, dataloader, optimizer, loss_fn):\n",
        "    model.train()\n",
        "\n",
        "    epoch_loss= 0\n",
        "\n",
        "    for step,batch in enumerate(dataloader):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        data, label = (t for t in batch)\n",
        "        data = data.to(device)\n",
        "        label = label.to(device)\n",
        "\n",
        "        pred = model(data).squeeze(1)\n",
        "        loss = loss_fn(pred,label)\n",
        "\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss/len(dataloader)\n",
        "        \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1LTWnox_rJK"
      },
      "source": [
        "def evaluate(model,dataloader,loss_fn):\n",
        "    model.eval()\n",
        "\n",
        "    epoch_loss = 0 \n",
        "\n",
        "    for step,batch in enumerate(dataloader):\n",
        "        data,label = (t for t in batch)\n",
        "        data = data.to(device)\n",
        "        label = label.to(device)\n",
        "\n",
        "        pred = model(data).squeeze(1)\n",
        "        loss = loss_fn(pred,label)\n",
        "\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss/len(dataloader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JGq3iPf3-lO1",
        "outputId": "55b6a105-3143-4c36-e4dc-7c85e1a66556"
      },
      "source": [
        "N_EPOCHS = 20 \n",
        "\n",
        "for i in range(N_EPOCHS):\n",
        "    train_loss = train(model,train_loader, optimizer, loss_fn)\n",
        "    eval_loss = evaluate(model,eval_loader, loss_fn)\n",
        "    \n",
        "    print (f' Epoch Number {i}') \n",
        "    print (f' Train. Loss: {train_loss:.5f} ')\n",
        "    print (f' Eval loss , {eval_loss:.5f} ')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Epoch Number 0\n",
            " Train. Loss: 0.01819 \n",
            " Eval loss , 0.01586 \n",
            " Epoch Number 1\n",
            " Train. Loss: 0.01819 \n",
            " Eval loss , 0.01614 \n",
            " Epoch Number 2\n",
            " Train. Loss: 0.01786 \n",
            " Eval loss , 0.01623 \n",
            " Epoch Number 3\n",
            " Train. Loss: 0.01790 \n",
            " Eval loss , 0.01642 \n",
            " Epoch Number 4\n",
            " Train. Loss: 0.01800 \n",
            " Eval loss , 0.01593 \n",
            " Epoch Number 5\n",
            " Train. Loss: 0.01772 \n",
            " Eval loss , 0.01546 \n",
            " Epoch Number 6\n",
            " Train. Loss: 0.01777 \n",
            " Eval loss , 0.01570 \n",
            " Epoch Number 7\n",
            " Train. Loss: 0.01737 \n",
            " Eval loss , 0.01564 \n",
            " Epoch Number 8\n",
            " Train. Loss: 0.01764 \n",
            " Eval loss , 0.01640 \n",
            " Epoch Number 9\n",
            " Train. Loss: 0.01744 \n",
            " Eval loss , 0.01590 \n",
            " Epoch Number 10\n",
            " Train. Loss: 0.01722 \n",
            " Eval loss , 0.01566 \n",
            " Epoch Number 11\n",
            " Train. Loss: 0.01740 \n",
            " Eval loss , 0.01632 \n",
            " Epoch Number 12\n",
            " Train. Loss: 0.01794 \n",
            " Eval loss , 0.01632 \n",
            " Epoch Number 13\n",
            " Train. Loss: 0.01759 \n",
            " Eval loss , 0.01537 \n",
            " Epoch Number 14\n",
            " Train. Loss: 0.01768 \n",
            " Eval loss , 0.01596 \n",
            " Epoch Number 15\n",
            " Train. Loss: 0.01705 \n",
            " Eval loss , 0.01582 \n",
            " Epoch Number 16\n",
            " Train. Loss: 0.01724 \n",
            " Eval loss , 0.01615 \n",
            " Epoch Number 17\n",
            " Train. Loss: 0.01743 \n",
            " Eval loss , 0.01585 \n",
            " Epoch Number 18\n",
            " Train. Loss: 0.01743 \n",
            " Eval loss , 0.01560 \n",
            " Epoch Number 19\n",
            " Train. Loss: 0.01714 \n",
            " Eval loss , 0.01593 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PyEdBEQh_ayw"
      },
      "source": [
        "The loss seems way too small??? am I calculating it correctly???\n",
        "I'm going to use L1loss instead of L2 loss and see how it goes. \n",
        "It seems that the class imbalance is too dominant, I'll have to work around it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EVf57rdX_GQh",
        "outputId": "3241d689-b33d-4b95-c43c-af6153234831"
      },
      "source": [
        "for i,batch in enumerate(train_loader):\n",
        "    data,label = (t for t in batch)\n",
        "    print (i)\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GboHeo62BWBW",
        "outputId": "df0a8af7-4279-4a38-db5f-9a85f08658b7"
      },
      "source": [
        "data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 2.9815e-04,  7.7955e-02, -4.8741e-04,  ...,  7.8442e-02,\n",
              "          0.0000e+00,  0.0000e+00],\n",
              "        [ 1.8731e-03,  4.4611e-02, -2.0406e-03,  ...,  4.6651e-02,\n",
              "          0.0000e+00,  0.0000e+00],\n",
              "        [ 4.7553e-04,  6.1652e-03, -3.5175e-02,  ...,  4.1340e-02,\n",
              "          0.0000e+00,  0.0000e+00],\n",
              "        ...,\n",
              "        [-2.4258e-04,  3.4845e-02, -2.3194e-02,  ...,  5.8039e-02,\n",
              "          0.0000e+00,  0.0000e+00],\n",
              "        [-6.1106e-04,  9.8598e-04, -8.4670e-02,  ...,  8.5656e-02,\n",
              "          0.0000e+00,  0.0000e+00],\n",
              "        [ 7.6427e-03,  1.4638e-02, -3.6971e-02,  ...,  5.1609e-02,\n",
              "          0.0000e+00,  1.0000e+00]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 290
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6v5viZ5fBxE5",
        "outputId": "4a23d3c0-223a-4650-e376-a19c80c97d28"
      },
      "source": [
        "label"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0.0446,  0.0062,  0.0658,  0.0292,  0.2194,  0.0073,  0.0116,  0.0205,\n",
              "         0.0400,  0.0313,  0.0649,  0.0670,  0.0324,  0.0123,  0.0553,  0.0617,\n",
              "         0.0361,  0.0074,  0.0025,  0.0077,  0.0162,  0.0138,  0.0834,  0.1036,\n",
              "         0.0322,  0.0200,  0.0510,  0.0026,  0.0269,  0.0284,  0.0473,  0.0076,\n",
              "         0.0175,  0.0042,  0.0025,  0.0010,  0.0856,  0.0034,  0.0291,  0.0619,\n",
              "         0.0367,  0.0090,  0.0314,  0.0052,  0.0188,  0.0058,  0.0669,  0.0360,\n",
              "         0.0193,  0.0024,  0.0005,  0.0035,  0.0165,  0.0270,  0.0061, -0.0006,\n",
              "         0.0310,  0.0150,  0.0017,  0.0374,  0.0348,  0.0010,  0.0146,  0.0331])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 291
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n3swifsyBysK",
        "outputId": "e3be6c4d-f594-4733-aad6-04a049b6ab0c"
      },
      "source": [
        "model(data.to(device)).squeeze()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.0306, 0.0253, 0.0240, 0.0256, 0.0248, 0.0387, 0.0230, 0.0273, 0.0294,\n",
              "        0.0302, 0.0312, 0.0329, 0.0207, 0.0208, 0.0152, 0.0203, 0.0126, 0.0092,\n",
              "        0.0113, 0.0096, 0.0081, 0.0083, 0.0082, 0.0177, 0.0202, 0.0215, 0.0082,\n",
              "        0.0101, 0.0153, 0.0084, 0.0109, 0.0119, 0.0129, 0.0082, 0.0082, 0.0078,\n",
              "        0.0074, 0.0177, 0.0110, 0.0160, 0.0200, 0.0211, 0.0205, 0.0203, 0.0196,\n",
              "        0.0192, 0.0197, 0.0226, 0.0210, 0.0200, 0.0206, 0.0205, 0.0193, 0.0161,\n",
              "        0.0170, 0.0176, 0.0211, 0.0222, 0.0179, 0.0064, 0.0064, 0.0088, 0.0134,\n",
              "        0.0227], device='cuda:0', grad_fn=<SqueezeBackward0>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 292
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zp13tzHLB1mx",
        "outputId": "a27cd2bf-3d43-4de8-d2d3-a36645bc10a9"
      },
      "source": [
        "loss_fn(model(data.to(device)).squeeze(),label.to(device))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.0214, device='cuda:0', grad_fn=<L1LossBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 293
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6_e3F0DCGp2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_E-oMKnKN-L9"
      },
      "source": [
        "#Torch Model 4 - LSTM Classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X5LDTZiSN9jL"
      },
      "source": [
        ""
      ],
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uhvmCMcpOi48"
      },
      "source": [
        "The previous 3 models used the almost the same dataloader because the similar input format, for LSTM I will need to make some adjustments. And perhaps try out 2 scenarios with and w/o the weighted sampler. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4EPDbh22O5Xn"
      },
      "source": [
        "class LSTMModel(nn.Module):\r\n",
        "    def __init__(self, input_size, num_classes,  num_hidden, hidden_dim):\r\n",
        "        super().__init__()\r\n",
        "        #num_layers = num_layers\r\n",
        "        # self.input_size = input_size\r\n",
        "        # self.hidden_size = hidden_size\r\n",
        "        self.lstm = nn.LSTM(input_size, hidden_dim, num_hidden, batch_first=True, bidirectional=False, dropout=0.5)\r\n",
        "        self.ff = nn.Linear(hidden_dim, num_classes)\r\n",
        "        self.dropout = nn.Dropout()\r\n",
        "\r\n",
        "        #need to change the loss function. \r\n",
        "    def forward(self, input):\r\n",
        "        output, (hn, cn) = self.lstm(input)\r\n",
        "        hn = self.dropout(hn[-1,:,:])\r\n",
        "        return self.ff(hn)"
      ],
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Y0n47b1Qj2I"
      },
      "source": [
        "INPUT_SIZE = 19\r\n",
        "NUM_CLASSES = 2\r\n",
        "NUM_HIDDEN = 3\r\n",
        "HIDDEN_DIM = 1024\r\n",
        "\r\n",
        "model = LSTMModel(INPUT_SIZE, NUM_CLASSES, NUM_HIDDEN, HIDDEN_DIM)"
      ],
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "93C2J3V1Ud8c",
        "outputId": "c1942b0e-8fed-410e-a2b5-70e65b19afa7"
      },
      "source": [
        "model"
      ],
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LSTMModel(\n",
              "  (lstm): LSTM(19, 1024, num_layers=3, batch_first=True, dropout=0.5)\n",
              "  (ff): Linear(in_features=1024, out_features=2, bias=True)\n",
              "  (dropout): Dropout(p=0.5, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 152
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SFfo_4kuUpcS",
        "outputId": "9f3e2da0-9c7c-442a-ae7b-8f27a66161cc"
      },
      "source": [
        "#print model parameters\r\n",
        "def count_parameters(model):\r\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\r\n",
        "\r\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')\r\n"
      ],
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 21,075,970 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p0hdVHycWRIi"
      },
      "source": [
        "#define loss fuction and optimizer\r\n",
        "optimizer = optim.Adam(model.parameters())\r\n",
        "loss_fn = nn.NLLLoss()"
      ],
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fnCJxYz-WX_z"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\n",
        "model = model.to(device)\r\n",
        "loss_fn = loss_fn.to(device)"
      ],
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5T6Pr2z9UvD5"
      },
      "source": [
        "def accuracy(preds, y):\r\n",
        "    \"\"\"\r\n",
        "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    #round predictions to the closest integer\r\n",
        "    prediciton = F.softmax(preds, dim =1)\r\n",
        "    _, pred = torch.max(prediciton, 1)\r\n",
        "    correct = (pred == y).float() #convert into float for division \r\n",
        "    acc = correct.sum() / len(correct)\r\n",
        "    return acc"
      ],
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8mJpUVIWdSI"
      },
      "source": [
        "#can i define a function to calculate the f1 score/confusion matrix? \r\n",
        "def get_cfm (preds, label):\r\n",
        "    pred = F.softmax(preds, dim =1)\r\n",
        "    _,pred_label = torch.max(pred,dim=1)\r\n",
        "    m = classification_report(pred_label.cpu(), label.cpu())\r\n",
        "    return m"
      ],
      "execution_count": 157,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TpJK5wkYWhN8"
      },
      "source": [
        "#now define the training and evaluation loop\r\n",
        "def train(model, dataloader, optimizer, loss_fn):\r\n",
        "    model.train()\r\n",
        "\r\n",
        "    epoch_loss= 0\r\n",
        "    epoch_acc = 0\r\n",
        "\r\n",
        "    for step,batch in enumerate(dataloader):\r\n",
        "        optimizer.zero_grad()\r\n",
        "\r\n",
        "        data, label = (t for t in batch)\r\n",
        "        data = data.to(device)\r\n",
        "        label = label.to(device)\r\n",
        "\r\n",
        "        pred = model(data)\r\n",
        "        loss = loss_fn(F.log_softmax(pred,dim=1),label)\r\n",
        "        acc  = accuracy(pred,label)\r\n",
        "\r\n",
        "        #append the prediction result and show it per epoch later. \r\n",
        "        if step ==0:     \r\n",
        "            pred_list=pred.cpu().detach().numpy()\r\n",
        "            label_list = label.cpu().detach().numpy()\r\n",
        "        if step != 0:\r\n",
        "            pred_list = np.concatenate([pred_list,pred.cpu().detach().numpy()],axis =0)\r\n",
        "            label_list = np.concatenate([label_list,label.cpu().detach().numpy()],axis =0)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "        loss.backward()\r\n",
        "        optimizer.step()\r\n",
        "\r\n",
        "        epoch_loss += loss.item()\r\n",
        "        epoch_acc += acc\r\n",
        "    report = get_cfm(torch.tensor(pred_list),torch.tensor(label_list))\r\n",
        "    print (report)\r\n",
        "    return epoch_loss/len(dataloader), epoch_acc/len(dataloader)\r\n",
        "\r\n"
      ],
      "execution_count": 158,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ls3DvxraWkgw"
      },
      "source": [
        "def evaluate(model,dataloader,loss_fn):\r\n",
        "    model.eval()\r\n",
        "\r\n",
        "    epoch_loss = 0 \r\n",
        "    epoch_acc = 0 \r\n",
        "\r\n",
        "    pred_list =[]\r\n",
        "    label_list =[]\r\n",
        "    for step,batch in enumerate(dataloader):\r\n",
        "        data,label = (t for t in batch)\r\n",
        "        data = data.to(device)\r\n",
        "        label = label.to(device)\r\n",
        "\r\n",
        "        pred = model(data)\r\n",
        "        loss = loss_fn(F.log_softmax(pred,dim=1),label)\r\n",
        "        acc = accuracy(pred,label)\r\n",
        "        \r\n",
        "    #     #append the prediction result and show it per epoch later. \r\n",
        "    #     if step ==0:     \r\n",
        "    #         pred_list=pred.cpu().detach().numpy()\r\n",
        "    #         label_list = label.cpu().detach().numpy()\r\n",
        "    #     if step != 0:\r\n",
        "    #         pred_list = np.concatenate([pred_list,pred.cpu().detach().numpy()],axis =0)\r\n",
        "    #         label_list = np.concatenate([label_list,label.cpu().detach().numpy()],axis =0)\r\n",
        "\r\n",
        "        epoch_loss += loss.item()\r\n",
        "        epoch_acc += acc\r\n",
        "\r\n",
        "    # report = get_cfm(torch.tensor(pred_list),torch.tensor(label_list))\r\n",
        "    # print (report)\r\n",
        "    return epoch_loss/len(dataloader), epoch_acc/len(dataloader)"
      ],
      "execution_count": 159,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H6fc8cJzWnjN",
        "outputId": "0bf1c515-d418-463e-abbf-53cdb4dd8039"
      },
      "source": [
        "N_EPOCHS = 15\r\n",
        "\r\n",
        "for i in range(N_EPOCHS):\r\n",
        "    train_loss,train_acc = train(model,train_loader, optimizer, loss_fn)\r\n",
        "    \r\n",
        "    eval_loss, eval_acc = evaluate(model,eval_loader, loss_fn)\r\n",
        "    \r\n",
        "    print (f' Epoch Number {i}') \r\n",
        "    print (f' Train. Loss: {train_loss:.3f} Train. Acc: {train_acc*100:.2f}%')\r\n",
        "    print (f' Eval loss , {eval_loss:.3f}, eval acc, {eval_acc*100:.2f} %')\r\n",
        "    # print(f'\\t Train. Loss: {train_loss:.3f} |  Train. Acc: {train_acc*100:.2f}%')\r\n",
        "    # print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
      ],
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.72      0.54      0.62       843\n",
            "           1       0.37      0.55      0.44       405\n",
            "\n",
            "    accuracy                           0.55      1248\n",
            "   macro avg       0.54      0.55      0.53      1248\n",
            "weighted avg       0.60      0.55      0.56      1248\n",
            "\n",
            " Epoch Number 0\n",
            " Train. Loss: 0.685 Train. Acc: 54.69%\n",
            " Eval loss , 0.597, eval acc, 86.71 %\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.58      0.69      0.63       524\n",
            "           1       0.74      0.63      0.68       724\n",
            "\n",
            "    accuracy                           0.66      1248\n",
            "   macro avg       0.66      0.66      0.65      1248\n",
            "weighted avg       0.67      0.66      0.66      1248\n",
            "\n",
            " Epoch Number 1\n",
            " Train. Loss: 0.651 Train. Acc: 65.62%\n",
            " Eval loss , 0.483, eval acc, 80.46 %\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      0.69      0.68       591\n",
            "           1       0.71      0.69      0.70       657\n",
            "\n",
            "    accuracy                           0.69      1248\n",
            "   macro avg       0.69      0.69      0.69      1248\n",
            "weighted avg       0.69      0.69      0.69      1248\n",
            "\n",
            " Epoch Number 2\n",
            " Train. Loss: 0.614 Train. Acc: 69.06%\n",
            " Eval loss , 0.845, eval acc, 39.19 %\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.57      0.74      0.64       470\n",
            "           1       0.81      0.66      0.73       778\n",
            "\n",
            "    accuracy                           0.69      1248\n",
            "   macro avg       0.69      0.70      0.69      1248\n",
            "weighted avg       0.72      0.69      0.70      1248\n",
            "\n",
            " Epoch Number 3\n",
            " Train. Loss: 0.598 Train. Acc: 69.06%\n",
            " Eval loss , 0.623, eval acc, 61.53 %\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.59      0.72      0.65       516\n",
            "           1       0.76      0.64      0.70       732\n",
            "\n",
            "    accuracy                           0.67      1248\n",
            "   macro avg       0.67      0.68      0.67      1248\n",
            "weighted avg       0.69      0.67      0.68      1248\n",
            "\n",
            " Epoch Number 4\n",
            " Train. Loss: 0.614 Train. Acc: 67.50%\n",
            " Eval loss , 0.513, eval acc, 72.65 %\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.64      0.68      0.66       544\n",
            "           1       0.74      0.70      0.72       704\n",
            "\n",
            "    accuracy                           0.69      1248\n",
            "   macro avg       0.69      0.69      0.69      1248\n",
            "weighted avg       0.69      0.69      0.69      1248\n",
            "\n",
            " Epoch Number 5\n",
            " Train. Loss: 0.605 Train. Acc: 68.67%\n",
            " Eval loss , 0.571, eval acc, 71.95 %\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.58      0.72      0.64       488\n",
            "           1       0.79      0.67      0.73       760\n",
            "\n",
            "    accuracy                           0.69      1248\n",
            "   macro avg       0.69      0.69      0.68      1248\n",
            "weighted avg       0.71      0.69      0.69      1248\n",
            "\n",
            " Epoch Number 6\n",
            " Train. Loss: 0.608 Train. Acc: 68.98%\n",
            " Eval loss , 0.692, eval acc, 60.35 %\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.60      0.70      0.65       551\n",
            "           1       0.73      0.63      0.68       697\n",
            "\n",
            "    accuracy                           0.66      1248\n",
            "   macro avg       0.67      0.67      0.66      1248\n",
            "weighted avg       0.67      0.66      0.66      1248\n",
            "\n",
            " Epoch Number 7\n",
            " Train. Loss: 0.618 Train. Acc: 66.17%\n",
            " Eval loss , 0.548, eval acc, 71.95 %\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.64      0.68      0.66       594\n",
            "           1       0.69      0.66      0.67       654\n",
            "\n",
            "    accuracy                           0.67      1248\n",
            "   macro avg       0.67      0.67      0.67      1248\n",
            "weighted avg       0.67      0.67      0.67      1248\n",
            "\n",
            " Epoch Number 8\n",
            " Train. Loss: 0.614 Train. Acc: 66.64%\n",
            " Eval loss , 0.502, eval acc, 81.33 %\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.66      0.72      0.69       574\n",
            "           1       0.75      0.69      0.71       674\n",
            "\n",
            "    accuracy                           0.70      1248\n",
            "   macro avg       0.70      0.71      0.70      1248\n",
            "weighted avg       0.71      0.70      0.70      1248\n",
            "\n",
            " Epoch Number 9\n",
            " Train. Loss: 0.602 Train. Acc: 70.00%\n",
            " Eval loss , 0.647, eval acc, 61.71 %\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.63      0.74      0.68       531\n",
            "           1       0.78      0.67      0.72       717\n",
            "\n",
            "    accuracy                           0.70      1248\n",
            "   macro avg       0.70      0.71      0.70      1248\n",
            "weighted avg       0.72      0.70      0.71      1248\n",
            "\n",
            " Epoch Number 10\n",
            " Train. Loss: 0.568 Train. Acc: 70.31%\n",
            " Eval loss , 0.591, eval acc, 67.78 %\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.66      0.74      0.70       558\n",
            "           1       0.77      0.69      0.73       690\n",
            "\n",
            "    accuracy                           0.71      1248\n",
            "   macro avg       0.71      0.72      0.71      1248\n",
            "weighted avg       0.72      0.71      0.71      1248\n",
            "\n",
            " Epoch Number 11\n",
            " Train. Loss: 0.582 Train. Acc: 71.17%\n",
            " Eval loss , 0.489, eval acc, 77.33 %\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.62      0.68      0.65       538\n",
            "           1       0.74      0.68      0.71       710\n",
            "\n",
            "    accuracy                           0.68      1248\n",
            "   macro avg       0.68      0.68      0.68      1248\n",
            "weighted avg       0.68      0.68      0.68      1248\n",
            "\n",
            " Epoch Number 12\n",
            " Train. Loss: 0.589 Train. Acc: 67.97%\n",
            " Eval loss , 0.683, eval acc, 62.58 %\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.66      0.74      0.69       553\n",
            "           1       0.77      0.69      0.73       695\n",
            "\n",
            "    accuracy                           0.71      1248\n",
            "   macro avg       0.71      0.71      0.71      1248\n",
            "weighted avg       0.72      0.71      0.71      1248\n",
            "\n",
            " Epoch Number 13\n",
            " Train. Loss: 0.582 Train. Acc: 71.25%\n",
            " Eval loss , 0.586, eval acc, 68.13 %\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.62      0.74      0.67       521\n",
            "           1       0.78      0.67      0.72       727\n",
            "\n",
            "    accuracy                           0.70      1248\n",
            "   macro avg       0.70      0.70      0.70      1248\n",
            "weighted avg       0.71      0.70      0.70      1248\n",
            "\n",
            " Epoch Number 14\n",
            " Train. Loss: 0.588 Train. Acc: 69.69%\n",
            " Eval loss , 0.522, eval acc, 75.77 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1rtD7Guh-g5"
      },
      "source": [
        "why is the loss negative here, did I mess up with the NLLloss? I passed the softmax into the NLLloss correctly ya? \r\n",
        "Ah, because i used softmax, it should be log_softmax instead. \r\n",
        "\r\n",
        " what about the actual result here. maybe try a few more epochs? \r\n",
        "\r\n",
        " The results is not exactly stable ?\r\n",
        " It's actually a common theme, throughout all the 4 models though .\r\n",
        "\r\n",
        " Hard to say, the recall of true label is not very good in evaluation set, although the accuracy is going up. The recall rate in trainning set seems decent though. ummm. \r\n",
        "\r\n",
        " What if I want to focus on the recall rate, maybe the focal loss could come into play here ???? maybe it can address the class imbalance better than over/undersampling? "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oEFnPR3BiFYw"
      },
      "source": [
        ""
      ],
      "execution_count": 160,
      "outputs": []
    }
  ]
}