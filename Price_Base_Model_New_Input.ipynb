{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Price Base Model New Input",
      "provenance": [],
      "collapsed_sections": [
        "NpHkfoUnkocT",
        "BshD6EjfdHGy",
        "6EuYIwJAHyT6",
        "SFfMPY--bvrO",
        "FMo638lbIbdp",
        "R7lIxy82XsK0",
        "mruUFVdyIxfH",
        "ojTvT9eqxR5b",
        "bf_54Mrvw1aO",
        "XqSIcZPr9qCY",
        "_E-oMKnKN-L9",
        "6pN-Is6Lv5OE"
      ],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOY65yb+7wymH+h9geAUeg0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YanzhaoZ/PreBit/blob/main/Price_Base_Model_New_Input.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NpHkfoUnkocT"
      },
      "source": [
        "#set-ups "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AKJf58FLcogN",
        "outputId": "2e73b851-f879-4163-cf7b-6df9be0ad2d5"
      },
      "source": [
        " !pip install yfinance\n",
        "import yfinance as yf\n",
        "import pandas as pd\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting yfinance\n",
            "  Downloading https://files.pythonhosted.org/packages/7a/e8/b9d7104d3a4bf39924799067592d9e59119fcfc900a425a12e80a3123ec8/yfinance-0.1.55.tar.gz\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.6/dist-packages (from yfinance) (1.1.5)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.6/dist-packages (from yfinance) (1.19.4)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.6/dist-packages (from yfinance) (2.23.0)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from yfinance) (0.0.9)\n",
            "Collecting lxml>=4.5.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bd/78/56a7c88a57d0d14945472535d0df9fb4bbad7d34ede658ec7961635c790e/lxml-4.6.2-cp36-cp36m-manylinux1_x86_64.whl (5.5MB)\n",
            "\u001b[K     |████████████████████████████████| 5.5MB 8.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.24->yfinance) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.24->yfinance) (2018.9)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20->yfinance) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20->yfinance) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20->yfinance) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20->yfinance) (1.24.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.7.3->pandas>=0.24->yfinance) (1.15.0)\n",
            "Building wheels for collected packages: yfinance\n",
            "  Building wheel for yfinance (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for yfinance: filename=yfinance-0.1.55-py2.py3-none-any.whl size=22616 sha256=752adf18a318d2ef61fe8e802fb21b6a001cd2ffa48df6c4a16f8421db4bd225\n",
            "  Stored in directory: /root/.cache/pip/wheels/04/98/cc/2702a4242d60bdc14f48b4557c427ded1fe92aedf257d4565c\n",
            "Successfully built yfinance\n",
            "Installing collected packages: lxml, yfinance\n",
            "  Found existing installation: lxml 4.2.6\n",
            "    Uninstalling lxml-4.2.6:\n",
            "      Successfully uninstalled lxml-4.2.6\n",
            "Successfully installed lxml-4.6.2 yfinance-0.1.55\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h0hPGdzadWND"
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, Dataset, WeightedRandomSampler\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from sklearn.preprocessing import MinMaxScaler\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BshD6EjfdHGy"
      },
      "source": [
        "# Loading Price Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IGzvjjDYdLoi",
        "outputId": "93fcc56e-8d5b-4a26-fd86-152c57348a20"
      },
      "source": [
        "start_date ='2015-01-01'\n",
        "end_date = '2019-12-31'\n",
        "price = yf.download(\"BTC-USD\", start=start_date, end=end_date)\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oiqImSEpgGt5"
      },
      "source": [
        "def get_technical_indicators(price):\n",
        "    # Create 7 and 21 days Moving Average\n",
        "    dataset = price.copy()\n",
        "\n",
        "    dataset['ma7'] = dataset['Close'].rolling(window=7).mean()\n",
        "    dataset['ma21'] = dataset['Close'].rolling(window=21).mean()\n",
        "    \n",
        "    # Create MACD\n",
        "    #dataset['26ema'] = pd.ewma(dataset['Close'], span=26)\n",
        "    dataset['26ema'] = dataset['Close'].ewm(span=26).mean()\n",
        "    #dataset['12ema'] = pd.ewma(dataset['Close'], span=12)\n",
        "    dataset['12ema'] = dataset['Close'].ewm(span=12).mean()\n",
        "    dataset['MACD'] = (dataset['12ema']-dataset['26ema'])\n",
        "\n",
        "    # Create Bollinger Bands\n",
        "    #dataset['20sd'] = pd.stats.moments.rolling_std(dataset['Close'],20)\n",
        "    dataset['20sd'] = dataset[\"Close\"].rolling(window=20).std()\n",
        "    dataset['upper_band'] = dataset['ma21'] + (dataset['20sd']*2)\n",
        "    dataset['lower_band'] = dataset['ma21'] - (dataset['20sd']*2)\n",
        "    \n",
        "    # Create Exponential moving average\n",
        "    dataset['ema'] = dataset['Close'].ewm(com=0.5).mean()\n",
        "    \n",
        "    # Create Momentum\n",
        "    #dataset['momentum'] = dataset['Close']-1\n",
        "\n",
        "    # Create high-low spred\n",
        "    dataset['spread'] = dataset['High'] - dataset['Low']\n",
        "    \n",
        "    return dataset"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-keoW2D4TEb"
      },
      "source": [
        "The Gold etf SPDR Gold future, has a lot of NAN values. \r\n",
        "That might cause some issues, but let's see how it goes first."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Szog27YrNQSG"
      },
      "source": [
        " # to be finished \n",
        "# def get_related_asset (price, start_date, end_date):\n",
        "#     other_price = yf.download(\"ETH-USD\", start=start_date, end=end_date)\n",
        "\n",
        "#     price['eth']=other_price[\"Close\"]\n",
        "#     #there are nan values because the price datarange is not the same. so I should probalby fill the value by non-0, coz i need to do division later...\n",
        "#     #let's fill it with the next valid value \n",
        "#     price.eth.fillna(method='bfill', inplace= True, axis=0)\n",
        "\n",
        "#     return price\n",
        "\n",
        " # to be finished \n",
        "def get_related_asset (price, start_date, end_date):\n",
        "    other_price = yf.download(\"ETH-USD GLD\", start=start_date, end=end_date)\n",
        "\n",
        "    price['eth']=other_price.Close['ETH-USD']\n",
        "    price['gold'] = other_price.Close['GLD']\n",
        "    #there are nan values because the price datarange is not the same. so I should probalby fill the value by non-0, coz i need to do division later...\n",
        "    #let's fill it with the next valid value \n",
        "    price.eth.fillna(method='bfill', inplace= True, axis=0)\n",
        "    price.gold.fillna(method='bfill', inplace= True, axis=0) \n",
        "\n",
        "    return price\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "291fHPUribeU"
      },
      "source": [
        "def get_label (price,threshold):\n",
        "\n",
        "\n",
        "    price['change']=price.shift(-1).High/price.Close -1 \n",
        "    price['change_label']=price['change'].apply (lambda x: x> threshold)\n",
        "\n",
        "    #convert True/False to 1/0\n",
        "    class2idx = {True: 1, False:0}\n",
        "    price['change_label'].replace(class2idx, inplace=True)\n",
        "\n",
        "    return price"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PrQfP2Crtl8X"
      },
      "source": [
        "def normalise_close(price):\n",
        "    \n",
        "    df = price.copy()\n",
        "    for key in df.keys():\n",
        "        if not key in ['change','change_label','Volume','MACD','20sd','spread','eth','gold']:\n",
        "            df[key]=df[key]/price['Close'].shift(1) - 1\n",
        "        \n",
        "    df['Volume']=df['Volume']/price['Volume'].shift(1)-1\n",
        "    df['eth']  = df['eth']/price['eth'].shift(1)-1\n",
        "    df['gold'] = df['gold']/price['gold'].shift(1)-1\n",
        "    df['MACD'] = df['MACD']/price['Close'].shift(1)\n",
        "    df['20sd'] = df['20sd']/price['Close'].shift(1)\n",
        "    df['spread'] = df['spread']/price['Close'].shift(1)\n",
        "\n",
        "\n",
        "\n",
        "    return df\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HB1cLbyxCsb_"
      },
      "source": [
        "def get_ma_feature (price,threshold):\n",
        "    price['ma_feature']=price['ma7'].apply(lambda x: x > threshold)\n",
        "    \n",
        "    class2idx = {True: 1, False:0}\n",
        "    price['ma_feature'].replace(class2idx, inplace=True)\n",
        "    return price"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wr-qb6DFd-Dw"
      },
      "source": [
        "#Here I want to define a function to return a new dataframe, that's INDEPENDANT, and produce relative price. \n",
        "def process_price (original_df,threshold, start_date,end_date):\n",
        "    #get the indicators\n",
        "    df = get_technical_indicators(original_df)\n",
        "\n",
        "    #get related asset\n",
        "    df = get_related_asset(df,start_date,end_date)\n",
        "\n",
        "    #get label\n",
        "    df = get_label(df, threshold)\n",
        "\n",
        "    #normalise the data\n",
        "    df_normalised_close = normalise_close(df)\n",
        "\n",
        "    #get ma label\n",
        "    df_normalised_close = get_ma_feature(df_normalised_close, threshold)\n",
        "\n",
        "    #return both normalized, and un-normalized data (w/o ma label). \n",
        "    return df, df_normalised_close\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nS3krSQDeA93",
        "outputId": "939e4cf6-d840-46de-96c9-1f3a80205f6d"
      },
      "source": [
        "test_df, test_df_norm = process_price(price,0.05,start_date,end_date)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[*********************100%***********************]  2 of 2 completed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "id": "z-lIuKV9eA4H",
        "outputId": "e47865a6-798d-4c90-981e-fd0e835579d3"
      },
      "source": [
        "test_df.head()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Close</th>\n",
              "      <th>Adj Close</th>\n",
              "      <th>Volume</th>\n",
              "      <th>ma7</th>\n",
              "      <th>ma21</th>\n",
              "      <th>26ema</th>\n",
              "      <th>12ema</th>\n",
              "      <th>MACD</th>\n",
              "      <th>20sd</th>\n",
              "      <th>upper_band</th>\n",
              "      <th>lower_band</th>\n",
              "      <th>ema</th>\n",
              "      <th>spread</th>\n",
              "      <th>eth</th>\n",
              "      <th>gold</th>\n",
              "      <th>change</th>\n",
              "      <th>change_label</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Date</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2015-01-01</th>\n",
              "      <td>320.434998</td>\n",
              "      <td>320.434998</td>\n",
              "      <td>314.002991</td>\n",
              "      <td>314.248993</td>\n",
              "      <td>314.248993</td>\n",
              "      <td>8036550</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>314.248993</td>\n",
              "      <td>314.248993</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>314.248993</td>\n",
              "      <td>6.432007</td>\n",
              "      <td>2.77212</td>\n",
              "      <td>114.080002</td>\n",
              "      <td>0.005060</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2015-01-02</th>\n",
              "      <td>314.079010</td>\n",
              "      <td>315.838989</td>\n",
              "      <td>313.565002</td>\n",
              "      <td>315.032013</td>\n",
              "      <td>315.032013</td>\n",
              "      <td>7860650</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>314.655561</td>\n",
              "      <td>314.673129</td>\n",
              "      <td>0.017568</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>314.836258</td>\n",
              "      <td>2.273987</td>\n",
              "      <td>2.77212</td>\n",
              "      <td>114.080002</td>\n",
              "      <td>0.000375</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2015-01-03</th>\n",
              "      <td>314.846008</td>\n",
              "      <td>315.149994</td>\n",
              "      <td>281.082001</td>\n",
              "      <td>281.082001</td>\n",
              "      <td>281.082001</td>\n",
              "      <td>33054400</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>302.592907</td>\n",
              "      <td>301.562504</td>\n",
              "      <td>-1.030403</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>291.467926</td>\n",
              "      <td>34.067993</td>\n",
              "      <td>2.77212</td>\n",
              "      <td>115.800003</td>\n",
              "      <td>0.021873</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2015-01-04</th>\n",
              "      <td>281.145996</td>\n",
              "      <td>287.230011</td>\n",
              "      <td>257.612000</td>\n",
              "      <td>264.195007</td>\n",
              "      <td>264.195007</td>\n",
              "      <td>55629100</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>291.858532</td>\n",
              "      <td>289.767045</td>\n",
              "      <td>-2.091487</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>273.058706</td>\n",
              "      <td>29.618011</td>\n",
              "      <td>2.77212</td>\n",
              "      <td>115.800003</td>\n",
              "      <td>0.053544</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2015-01-05</th>\n",
              "      <td>265.084015</td>\n",
              "      <td>278.341003</td>\n",
              "      <td>265.084015</td>\n",
              "      <td>274.473999</td>\n",
              "      <td>274.473999</td>\n",
              "      <td>43962800</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>287.826987</td>\n",
              "      <td>285.611979</td>\n",
              "      <td>-2.215008</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>274.006134</td>\n",
              "      <td>13.256989</td>\n",
              "      <td>2.77212</td>\n",
              "      <td>115.800003</td>\n",
              "      <td>0.047651</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                  Open        High  ...    change  change_label\n",
              "Date                                ...                        \n",
              "2015-01-01  320.434998  320.434998  ...  0.005060             0\n",
              "2015-01-02  314.079010  315.838989  ...  0.000375             0\n",
              "2015-01-03  314.846008  315.149994  ...  0.021873             0\n",
              "2015-01-04  281.145996  287.230011  ...  0.053544             1\n",
              "2015-01-05  265.084015  278.341003  ...  0.047651             0\n",
              "\n",
              "[5 rows x 20 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "id": "e4KJZaWZeA1r",
        "outputId": "bb664e71-9606-4b7d-9da0-c9f689400a3a"
      },
      "source": [
        "test_df_norm.head()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Close</th>\n",
              "      <th>Adj Close</th>\n",
              "      <th>Volume</th>\n",
              "      <th>ma7</th>\n",
              "      <th>ma21</th>\n",
              "      <th>26ema</th>\n",
              "      <th>12ema</th>\n",
              "      <th>MACD</th>\n",
              "      <th>20sd</th>\n",
              "      <th>upper_band</th>\n",
              "      <th>lower_band</th>\n",
              "      <th>ema</th>\n",
              "      <th>spread</th>\n",
              "      <th>eth</th>\n",
              "      <th>gold</th>\n",
              "      <th>change</th>\n",
              "      <th>change_label</th>\n",
              "      <th>ma_feature</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Date</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2015-01-01</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.005060</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2015-01-02</th>\n",
              "      <td>-0.000541</td>\n",
              "      <td>0.005060</td>\n",
              "      <td>-0.002177</td>\n",
              "      <td>0.002492</td>\n",
              "      <td>0.002492</td>\n",
              "      <td>-0.021888</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.001294</td>\n",
              "      <td>0.001350</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.001869</td>\n",
              "      <td>0.007236</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000375</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2015-01-03</th>\n",
              "      <td>-0.000590</td>\n",
              "      <td>0.000375</td>\n",
              "      <td>-0.107767</td>\n",
              "      <td>-0.107767</td>\n",
              "      <td>-0.107767</td>\n",
              "      <td>3.205047</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.039485</td>\n",
              "      <td>-0.042756</td>\n",
              "      <td>-0.003271</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.074799</td>\n",
              "      <td>0.108141</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.015077</td>\n",
              "      <td>0.021873</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2015-01-04</th>\n",
              "      <td>0.000228</td>\n",
              "      <td>0.021873</td>\n",
              "      <td>-0.083499</td>\n",
              "      <td>-0.060079</td>\n",
              "      <td>-0.060079</td>\n",
              "      <td>0.682956</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.038339</td>\n",
              "      <td>0.030899</td>\n",
              "      <td>-0.007441</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.028544</td>\n",
              "      <td>0.105371</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.053544</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2015-01-05</th>\n",
              "      <td>0.003365</td>\n",
              "      <td>0.053544</td>\n",
              "      <td>0.003365</td>\n",
              "      <td>0.038907</td>\n",
              "      <td>0.038907</td>\n",
              "      <td>-0.209716</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.089449</td>\n",
              "      <td>0.081065</td>\n",
              "      <td>-0.008384</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.037136</td>\n",
              "      <td>0.050179</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.047651</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                Open      High       Low  ...    change  change_label  ma_feature\n",
              "Date                                      ...                                    \n",
              "2015-01-01       NaN       NaN       NaN  ...  0.005060             0           0\n",
              "2015-01-02 -0.000541  0.005060 -0.002177  ...  0.000375             0           0\n",
              "2015-01-03 -0.000590  0.000375 -0.107767  ...  0.021873             0           0\n",
              "2015-01-04  0.000228  0.021873 -0.083499  ...  0.053544             1           0\n",
              "2015-01-05  0.003365  0.053544  0.003365  ...  0.047651             0           0\n",
              "\n",
              "[5 rows x 21 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jSRfavFZyt_8",
        "outputId": "bf68183c-4622-477f-de0a-1c68eb0b2718"
      },
      "source": [
        "test_df_norm.change_label.value_counts()"
      ],
      "execution_count": 254,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    1608\n",
              "1     218\n",
              "Name: change_label, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 254
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6EuYIwJAHyT6"
      },
      "source": [
        "# Some analysis with MA only"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7X5-Qm6rHx_b"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ePkc-bMv5VZ",
        "outputId": "efd5152d-6e8c-47a7-9cc5-f7322122b7ef"
      },
      "source": [
        "(test_df_norm['change_label']==test_df_norm['ma_feature']).value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True     1537\n",
              "False     289\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 164
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HCinzG02Dyba",
        "outputId": "2204c0f1-0a1c-4092-f68e-77cfd93fa485"
      },
      "source": [
        "test_df_norm['change_label'].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    1608\n",
              "1     218\n",
              "Name: change_label, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 165
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hOgB2oowD_Uz",
        "outputId": "a5739857-2cb7-4112-b072-c6c624f73f62"
      },
      "source": [
        "#why not get a confusion matrix with these two labels and check it out. \n",
        "accuracy = (test_df_norm['change_label']==test_df_norm['ma_feature']).sum() / len(test_df_norm['change_label']==test_df_norm['ma_feature'])\n",
        "print ('accuracy:', accuracy)\n",
        "print (confusion_matrix(test_df_norm['change_label'].values, test_df_norm['ma_feature'].values))\n",
        "print (classification_report(test_df_norm['change_label'].values, test_df_norm['ma_feature'].values))\n",
        "\n",
        "#The true label, accuracy is only 0.27, and recall rate is quite low to be honest. "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy: 0.8417305585980285\n",
            "[[1495  113]\n",
            " [ 176   42]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.93      0.91      1608\n",
            "           1       0.27      0.19      0.23       218\n",
            "\n",
            "    accuracy                           0.84      1826\n",
            "   macro avg       0.58      0.56      0.57      1826\n",
            "weighted avg       0.82      0.84      0.83      1826\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mt6Jsxx8F5I3",
        "outputId": "15d0d89a-87bb-4aed-c61b-8106ccb3398c"
      },
      "source": [
        "#for 21day ma features\n",
        "#why not get a confusion matrix with these two labels and check it out. \n",
        "accuracy = (test_df_norm['change_label']==test_df_norm['ma_feature']).sum() / len(test_df_norm['change_label']==test_df_norm['ma_feature'])\n",
        "print ('accuracy:', accuracy)\n",
        "print (confusion_matrix(test_df_norm['change_label'].values, test_df_norm['ma_feature'].values))\n",
        "print (classification_report(test_df_norm['change_label'].values, test_df_norm['ma_feature'].values))\n",
        "\n",
        "#The true label, accuracy is only 0.27, and recall rate is quite low to be honest. "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy: 0.7431544359255202\n",
            "[[1301  307]\n",
            " [ 162   56]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.81      0.85      1608\n",
            "           1       0.15      0.26      0.19       218\n",
            "\n",
            "    accuracy                           0.74      1826\n",
            "   macro avg       0.52      0.53      0.52      1826\n",
            "weighted avg       0.80      0.74      0.77      1826\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "67tCONaUIdX7",
        "outputId": "3a60c80a-bff8-4771-e2b5-2f8f4aec74db"
      },
      "source": [
        "#for ema features\n",
        "#why not get a confusion matrix with these two labels and check it out. \n",
        "accuracy = (test_df_norm['change_label']==test_df_norm['ma_feature']).sum() / len(test_df_norm['change_label']==test_df_norm['ma_feature'])\n",
        "print ('accuracy:', accuracy)\n",
        "print (confusion_matrix(test_df_norm['change_label'].values, test_df_norm['ma_feature'].values))\n",
        "print (classification_report(test_df_norm['change_label'].values, test_df_norm['ma_feature'].values))\n",
        "\n",
        "#The true label, accuracy is only 0.27, and recall rate is quite low to be honest. "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy: 0.864184008762322\n",
            "[[1561   47]\n",
            " [ 201   17]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.97      0.93      1608\n",
            "           1       0.27      0.08      0.12       218\n",
            "\n",
            "    accuracy                           0.86      1826\n",
            "   macro avg       0.58      0.52      0.52      1826\n",
            "weighted avg       0.81      0.86      0.83      1826\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NH2o4ZSOWDUY"
      },
      "source": [
        "#so bascially 7day ma works best among these 3. got it. \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WM2W-oEPWNs0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SFfMPY--bvrO"
      },
      "source": [
        "# Format Custom Dataset input for Torch Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TkcbqR134S4I"
      },
      "source": [
        "Creating two dataset classes for future use, 1 is for classification tast, the other for Regression task"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "me2mllPfb6bP"
      },
      "source": [
        "class ClassifierDataset(Dataset):\n",
        "    def __init__(self,df):\n",
        "        self.df = df\n",
        "        self.X, self.Y = self.clean_df()\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.Y)\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.Y[idx]\n",
        "\n",
        "\n",
        "    def clean_df(self):\n",
        "        #drop NaN values, and also load the values into torch.tensor. \n",
        "        df = self.df.dropna()\n",
        "        y_values = df.change_label.values\n",
        "        df = df.drop (['change','change_label'],axis=1)\n",
        "        #try put the label as part of the feature see if the model cna leran that \n",
        "        #df = df.drop (['change'],axis=1)\n",
        "        x_values = df.values\n",
        "\n",
        "        return torch.from_numpy(x_values).float(), torch.from_numpy(y_values).long()\n",
        "\n",
        "\n",
        "      \n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w7l-Gu-vkq83"
      },
      "source": [
        "class RegressionDataset(Dataset):\n",
        "    def __init__(self,df):\n",
        "        self.df = df\n",
        "        self.X, self.Y = self.clean_df()\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.Y)\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.Y[idx]\n",
        "\n",
        "\n",
        "    def clean_df(self):\n",
        "        #drop NaN values, and also load the values into torch.tensor.\n",
        "        df = self.df.dropna()\n",
        "        y_values = df.change.values\n",
        "        df = df.drop (['change','change_label'],axis=1)\n",
        "        x_values = df.values\n",
        "\n",
        "        return torch.from_numpy(x_values).float(), torch.from_numpy(y_values).float()\n",
        "\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IfHatgydCwHs"
      },
      "source": [
        "# define a function for train_test split, and initiate dataset? \n",
        "def split_train_test (df, threshold):\n",
        "    length = len(df)\n",
        "    split_number  = round(threshold*length)\n",
        "    return df[0:split_number], df[split_number:]"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DVI-SUEoD_k2"
      },
      "source": [
        "#for classification dataset construction\n",
        "test_df_train,test_df_eval = split_train_test(test_df_norm, 0.7)\n",
        "train_dataset = ClassifierDataset(test_df_train)\n",
        "eval_dataset = ClassifierDataset (test_df_eval)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OTBt70ZjGOiw"
      },
      "source": [
        "# for Regression dataset construction\n",
        "\n",
        "# test_df_train,test_df_eval = split_train_test(test_df_norm, 0.7)\n",
        "# train_dataset = RegressionDataset(test_df_train)\n",
        "# eval_dataset = RegressionDataset (test_df_eval)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5HfWHePSo1Pa",
        "outputId": "d157b846-b791-4180-8981-fd341eda1d0d"
      },
      "source": [
        "eval_dataset[0]"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([-0.0009,  0.0467, -0.0125,  0.0358,  0.0358, -0.0817, -0.0207,  0.0035,\n",
              "          0.0373, -0.0022, -0.0395,  0.0413,  0.0861, -0.0791,  0.0226,  0.0592,\n",
              "          0.0472,  0.0000,  0.0000]), tensor(0))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMo638lbIbdp"
      },
      "source": [
        "# Format DataLoader, different sampling options"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h0kk1mR_tJI0"
      },
      "source": [
        "#Now let's try to flow the data into dataloader \n",
        "train_loader = DataLoader(train_dataset, batch_size = 64, )\n",
        "eval_loader = DataLoader(eval_dataset, batch_size = 64, )\n",
        "# add in shuffle/sampler options later"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lX7oSm_rOE9R"
      },
      "source": [
        "The above is the normal dataloader. Below I will try to use weighted samplers, which will include oversample and undersamples, and in this case, it will also disrupt the ordering/sequence of the input, ummmmmmmmmmm, How will that affect the model performance, let's seeeeeeeee"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ZvQSmAVOEXy"
      },
      "source": [
        "#obtain the label list. \r\n",
        "target_list = []\r\n",
        "for _,t in train_dataset:\r\n",
        "  target_list.append(t)\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yN4VLeQuZfHh",
        "outputId": "7bae1de5-d089-494a-b2c2-9f83b3568826"
      },
      "source": [
        "#obtain the class weights\r\n",
        "leng = len(target_list)\r\n",
        "total_true = sum(target_list)\r\n",
        "class_count = [leng-total_true, total_true]\r\n",
        "class_weight = 1./torch.tensor(class_count, dtype=torch.float)\r\n",
        "class_weight"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.0009, 0.0062])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rCvHSf-2apzB"
      },
      "source": [
        "#a weight for each sample\r\n",
        "class_weights_all = class_weight[target_list]"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-H4P0ryYe-3B"
      },
      "source": [
        "weighted_sampler = WeightedRandomSampler(\r\n",
        "    weights = class_weights_all,\r\n",
        "    num_samples = leng,\r\n",
        "    replacement = True\r\n",
        ")"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J4ku89M3fiah"
      },
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size = 64, sampler=weighted_sampler )\r\n",
        "eval_loader = DataLoader(eval_dataset, batch_size = 64, )\r\n",
        "# add in shuffle/sampler options later"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G_-C0_dCfsSJ",
        "outputId": "c61c4e9d-a35d-4972-ac50-c855bc207019"
      },
      "source": [
        "for i, batch in enumerate(train_loader):\r\n",
        "    _,t = (m for m in batch)\r\n",
        "    print (sum(t)/len(t))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(0.5312)\n",
            "tensor(0.5625)\n",
            "tensor(0.4688)\n",
            "tensor(0.5938)\n",
            "tensor(0.4844)\n",
            "tensor(0.3906)\n",
            "tensor(0.4844)\n",
            "tensor(0.4844)\n",
            "tensor(0.4062)\n",
            "tensor(0.5312)\n",
            "tensor(0.4375)\n",
            "tensor(0.5781)\n",
            "tensor(0.5000)\n",
            "tensor(0.3906)\n",
            "tensor(0.5000)\n",
            "tensor(0.5625)\n",
            "tensor(0.5000)\n",
            "tensor(0.7031)\n",
            "tensor(0.3594)\n",
            "tensor(0.5238)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chR_wC8ytJHm"
      },
      "source": [
        "#next(iter(train_loader))"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GFK252mLtJGT"
      },
      "source": [
        "# for i, batch in enumerate(train_loader):\n",
        "#     print (i)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aLdhmP5MtJA8"
      },
      "source": [
        "#weighed random sampler test. \n",
        "\n",
        "# import torch\n",
        "# from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler, TensorDataset\n",
        "# train_dataset = torch.tensor([1, 1 , 1, 1, 1, 1, 1, 1, 1, 0])\n",
        "\n",
        "\n",
        "# class_weights_all = [0.1, 0.1, 0.1, 0.1,0.1, 0.1, 0.1, 0.1,0.1, 0.9]\n",
        "\n",
        "# weighted_sampler = WeightedRandomSampler(\n",
        "#     weights=class_weights_all,\n",
        "#     num_samples=10,\n",
        "#     replacement=True #if True, sampler will draw repeating inputs, False will have 0 repeats. \n",
        "# )\n",
        "\n",
        "# BATCH_SIZE = 5\n",
        "# dataset = TensorDataset(train_dataset)\n",
        "# train_loader = DataLoader(dataset,\n",
        "#                           batch_size=BATCH_SIZE,\n",
        "#                           sampler=weighted_sampler\n",
        "# )\n",
        "# for batch in train_loader:\n",
        "#     print(batch)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7lIxy82XsK0"
      },
      "source": [
        "#Format Dataset and DataLoader for LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5_t3QEgzXvh3"
      },
      "source": [
        "class LSTMDataset(Dataset):\r\n",
        "    def __init__(self,df,window_size):\r\n",
        "        self.df = df\r\n",
        "        self.window_size = window_size\r\n",
        "        self.X, self.Y = self.clean_df()\r\n",
        "\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        return len(self.Y)\r\n",
        "\r\n",
        "\r\n",
        "    def __getitem__(self, idx):\r\n",
        "        return self.X[idx], self.Y[idx]\r\n",
        "\r\n",
        "\r\n",
        "    def clean_df(self):\r\n",
        "        #drop NaN values, and also load the values into torch.tensor. \r\n",
        "        df = self.df.dropna()\r\n",
        "        y_values = df.change_label.values[self.window_size:]\r\n",
        "        df = df.drop (['change','change_label'],axis=1)\r\n",
        "        #try put the label as part of the feature see if the model cna leran that \r\n",
        "        #df = df.drop (['change'],axis=1)\r\n",
        "        x_values = df.values\r\n",
        "        output = self.process(x_values)\r\n",
        "\r\n",
        "\r\n",
        "        return torch.tensor(output,dtype=torch.float), torch.from_numpy(y_values).long()\r\n",
        "\r\n",
        "    def process(self,data):\r\n",
        "        output = []\r\n",
        "        for i in range(self.window_size-1, len(data)):\r\n",
        "            raw_data = data[i-self.window_size+1:i+1]\r\n",
        "            output.append(raw_data)\r\n",
        "        return output\r\n",
        "\r\n"
      ],
      "execution_count": 315,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D7SAU11Qe11f"
      },
      "source": [
        "# define a function for train_test split, and initiate dataset? \r\n",
        "def split_train_test (df, threshold):\r\n",
        "    length = len(df)\r\n",
        "    split_number  = round(threshold*length)\r\n",
        "    return df[0:split_number], df[split_number:]"
      ],
      "execution_count": 316,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JirYuopfdwYd"
      },
      "source": [
        "#for classification dataset construction\r\n",
        "test_df_train,test_df_eval = split_train_test(test_df_norm, 0.7)\r\n",
        "WINDOW_SIZE = 10 \r\n",
        "train_dataset = LSTMDataset(test_df_train, window_size=WINDOW_SIZE)\r\n",
        "eval_dataset = LSTMDataset (test_df_eval, window_size= WINDOW_SIZE)"
      ],
      "execution_count": 317,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K9nGvys-eNX9",
        "outputId": "d3de324c-f447-4222-e241-50786c5caf52"
      },
      "source": [
        "train_dataset[0]"
      ],
      "execution_count": 318,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[ 2.9815e-04,  7.7955e-02, -4.8741e-04,  7.3738e-02,  7.3738e-02,\n",
              "           2.4421e-01,  9.5185e-04,  1.9347e-01,  1.2295e-01,  6.5804e-02,\n",
              "          -5.7147e-02,  1.8713e-01,  5.6772e-01, -1.8079e-01,  4.9743e-02,\n",
              "           7.8442e-02,  0.0000e+00,  2.4160e-04,  0.0000e+00],\n",
              "         [ 1.8731e-03,  4.4611e-02, -2.0406e-03,  2.8687e-02,  2.8687e-02,\n",
              "           1.2097e-01, -5.2953e-02,  9.4539e-02,  4.4277e-02, -1.6947e-03,\n",
              "          -4.5971e-02,  1.6074e-01,  4.1603e-01, -2.2695e-01,  1.1676e-02,\n",
              "           4.6651e-02,  0.0000e+00,  8.0496e-03,  0.0000e+00],\n",
              "         [ 4.7553e-04,  6.1652e-03, -3.5175e-02, -2.2579e-03, -2.2579e-03,\n",
              "          -2.6600e-01, -6.4195e-02,  4.7255e-02,  1.3600e-02, -2.5246e-02,\n",
              "          -3.8846e-02,  1.5229e-01,  3.5183e-01, -2.5732e-01, -7.0176e-03,\n",
              "           4.1340e-02,  0.0000e+00, -7.9853e-03,  0.0000e+00],\n",
              "         [-7.6864e-04,  6.5833e-02, -1.2268e-02,  6.4274e-02,  6.4274e-02,\n",
              "           6.5308e-03, -3.2272e-02,  4.2829e-02,  2.0149e-02, -9.3590e-03,\n",
              "          -2.9508e-02,  1.5119e-01,  3.4520e-01, -2.5954e-01,  4.1259e-02,\n",
              "           7.8101e-02,  0.0000e+00, -9.9815e-03,  0.0000e+00],\n",
              "         [-1.9972e-03,  2.9159e-02, -1.5965e-02,  2.3688e-02,  2.3688e-02,\n",
              "           3.5510e-01, -6.5712e-02, -2.2162e-02, -3.5809e-02, -5.4675e-02,\n",
              "          -1.8865e-02,  1.3917e-01,  2.5617e-01, -3.0050e-01,  8.5837e-03,\n",
              "           4.5125e-02,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
              "         [ 1.4228e-03,  2.1940e-01,  1.4228e-03,  7.7862e-02,  7.7862e-02,\n",
              "           2.1800e+00, -5.4330e-02, -4.4977e-02, -4.6473e-02, -5.2481e-02,\n",
              "          -6.0081e-03,  1.3290e-01,  2.2082e-01, -3.1078e-01,  4.6990e-02,\n",
              "           2.1798e-01,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
              "         [-1.1189e-03,  7.3390e-03, -8.3445e-02, -3.6559e-02, -3.6559e-02,\n",
              "          -5.8426e-01, -9.5395e-02, -1.1792e-01, -1.0868e-01, -1.0780e-01,\n",
              "           8.7805e-04,  1.1612e-01,  1.1432e-01, -3.5017e-01, -3.3920e-02,\n",
              "           9.0784e-02,  0.0000e+00,  1.1464e-02,  0.0000e+00],\n",
              "         [-4.7061e-04,  1.1614e-02, -1.3826e-01, -1.1219e-01, -1.1219e-01,\n",
              "          -1.0541e-03, -5.7263e-02, -9.5369e-02, -7.7987e-02, -7.9887e-02,\n",
              "          -1.8997e-03,  1.1371e-01,  1.3206e-01, -3.2280e-01, -7.3882e-02,\n",
              "           1.4988e-01,  0.0000e+00, -7.8778e-03,  0.0000e+00],\n",
              "         [-2.4239e-03,  2.0482e-02, -5.6444e-02, -1.7185e-03, -1.7185e-03,\n",
              "          -2.7369e-01,  6.1936e-02,  8.8042e-03,  3.5189e-02,  3.0479e-02,\n",
              "          -4.7092e-03,  1.1584e-01,  2.4048e-01, -2.2287e-01,  1.3238e-02,\n",
              "           7.6925e-02,  0.0000e+00, -2.1552e-02,  1.0000e+00],\n",
              "         [-3.1733e-03,  3.9989e-02, -3.2863e-02, -3.0354e-02, -3.0354e-02,\n",
              "          -1.7410e-01,  5.9816e-02, -2.5069e-03,  3.1433e-02,  2.2557e-02,\n",
              "          -8.8765e-03,  1.0831e-01,  2.1411e-01, -2.1913e-01, -1.5242e-02,\n",
              "           7.2852e-02,  0.0000e+00,  2.2276e-02,  1.0000e+00]]), tensor(1))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 318
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r5QoruL2evOV"
      },
      "source": [
        "#a vanilla dataloader\r\n",
        "\r\n",
        "train_loader = DataLoader(train_dataset, batch_size = 64, )\r\n",
        "eval_loader = DataLoader(eval_dataset, batch_size = 64, )\r\n"
      ],
      "execution_count": 319,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1GBZUjaKg78J"
      },
      "source": [
        "#weighted_dataloader\r\n",
        "\r\n",
        "#obtain the label list. \r\n",
        "target_list = []\r\n",
        "for _,t in train_dataset:\r\n",
        "  target_list.append(t)\r\n",
        "\r\n",
        "#obtain the class weights\r\n",
        "leng = len(target_list)\r\n",
        "total_true = sum(target_list)\r\n",
        "class_count = [leng-total_true, total_true]\r\n",
        "class_weight = 1./torch.tensor(class_count, dtype=torch.float)\r\n",
        "\r\n",
        "\r\n",
        "#a weight for each sample\r\n",
        "class_weights_all = class_weight[target_list]\r\n",
        "\r\n",
        "weighted_sampler = WeightedRandomSampler(\r\n",
        "    weights = class_weights_all,\r\n",
        "    num_samples = leng,\r\n",
        "    replacement = True\r\n",
        ")"
      ],
      "execution_count": 320,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2o6_8iXAhUB1"
      },
      "source": [
        "#train_loader = DataLoader(train_dataset, batch_size = 64, sampler=weighted_sampler )\r\n",
        "train_loader = DataLoader(train_dataset, batch_size = 64, )\r\n",
        "eval_loader = DataLoader(eval_dataset, batch_size = 64, )\r\n",
        "# add in shuffle/sampler options later"
      ],
      "execution_count": 321,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5xgqDc28hZ4D",
        "outputId": "0dd1060e-05cf-45f7-aaec-66debfd01c9d"
      },
      "source": [
        "next(iter(train_loader))"
      ],
      "execution_count": 322,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[tensor([[[ 2.9815e-04,  7.7955e-02, -4.8741e-04,  ...,  0.0000e+00,\n",
              "            2.4160e-04,  0.0000e+00],\n",
              "          [ 1.8731e-03,  4.4611e-02, -2.0406e-03,  ...,  0.0000e+00,\n",
              "            8.0496e-03,  0.0000e+00],\n",
              "          [ 4.7553e-04,  6.1652e-03, -3.5175e-02,  ...,  0.0000e+00,\n",
              "           -7.9853e-03,  0.0000e+00],\n",
              "          ...,\n",
              "          [-4.7061e-04,  1.1614e-02, -1.3826e-01,  ...,  0.0000e+00,\n",
              "           -7.8778e-03,  0.0000e+00],\n",
              "          [-2.4239e-03,  2.0482e-02, -5.6444e-02,  ...,  0.0000e+00,\n",
              "           -2.1552e-02,  1.0000e+00],\n",
              "          [-3.1733e-03,  3.9989e-02, -3.2863e-02,  ...,  0.0000e+00,\n",
              "            2.2276e-02,  1.0000e+00]],\n",
              " \n",
              "         [[ 1.8731e-03,  4.4611e-02, -2.0406e-03,  ...,  0.0000e+00,\n",
              "            8.0496e-03,  0.0000e+00],\n",
              "          [ 4.7553e-04,  6.1652e-03, -3.5175e-02,  ...,  0.0000e+00,\n",
              "           -7.9853e-03,  0.0000e+00],\n",
              "          [-7.6864e-04,  6.5833e-02, -1.2268e-02,  ...,  0.0000e+00,\n",
              "           -9.9815e-03,  0.0000e+00],\n",
              "          ...,\n",
              "          [-2.4239e-03,  2.0482e-02, -5.6444e-02,  ...,  0.0000e+00,\n",
              "           -2.1552e-02,  1.0000e+00],\n",
              "          [-3.1733e-03,  3.9989e-02, -3.2863e-02,  ...,  0.0000e+00,\n",
              "            2.2276e-02,  1.0000e+00],\n",
              "          [ 7.0625e-05,  3.1264e-02, -4.4677e-02,  ...,  0.0000e+00,\n",
              "           -8.3434e-03,  1.0000e+00]],\n",
              " \n",
              "         [[ 4.7553e-04,  6.1652e-03, -3.5175e-02,  ...,  0.0000e+00,\n",
              "           -7.9853e-03,  0.0000e+00],\n",
              "          [-7.6864e-04,  6.5833e-02, -1.2268e-02,  ...,  0.0000e+00,\n",
              "           -9.9815e-03,  0.0000e+00],\n",
              "          [-1.9972e-03,  2.9159e-02, -1.5965e-02,  ...,  0.0000e+00,\n",
              "            0.0000e+00,  0.0000e+00],\n",
              "          ...,\n",
              "          [-3.1733e-03,  3.9989e-02, -3.2863e-02,  ...,  0.0000e+00,\n",
              "            2.2276e-02,  1.0000e+00],\n",
              "          [ 7.0625e-05,  3.1264e-02, -4.4677e-02,  ...,  0.0000e+00,\n",
              "           -8.3434e-03,  1.0000e+00],\n",
              "          [-2.7453e-03,  6.4884e-02, -2.5057e-02,  ...,  0.0000e+00,\n",
              "            0.0000e+00,  1.0000e+00]],\n",
              " \n",
              "         ...,\n",
              " \n",
              "         [[-2.4258e-04,  3.4845e-02, -2.3194e-02,  ...,  0.0000e+00,\n",
              "            0.0000e+00,  0.0000e+00],\n",
              "          [-6.1106e-04,  9.8598e-04, -8.4670e-02,  ...,  0.0000e+00,\n",
              "            2.4499e-03,  0.0000e+00],\n",
              "          [ 7.6427e-03,  1.4638e-02, -3.6971e-02,  ...,  0.0000e+00,\n",
              "            1.3966e-03,  1.0000e+00],\n",
              "          ...,\n",
              "          [ 6.8394e-04,  2.6900e-02, -1.4416e-02,  ...,  0.0000e+00,\n",
              "            0.0000e+00,  0.0000e+00],\n",
              "          [-2.9090e-04,  4.8641e-03, -1.9339e-02,  ...,  0.0000e+00,\n",
              "           -7.9118e-04,  0.0000e+00],\n",
              "          [-4.0611e-06,  1.3582e-02, -1.2546e-02,  ...,  0.0000e+00,\n",
              "            1.7068e-02,  0.0000e+00]],\n",
              " \n",
              "         [[-6.1106e-04,  9.8598e-04, -8.4670e-02,  ...,  0.0000e+00,\n",
              "            2.4499e-03,  0.0000e+00],\n",
              "          [ 7.6427e-03,  1.4638e-02, -3.6971e-02,  ...,  0.0000e+00,\n",
              "            1.3966e-03,  1.0000e+00],\n",
              "          [ 3.2086e-04,  3.3132e-02, -5.2479e-03,  ...,  0.0000e+00,\n",
              "            6.5371e-03,  0.0000e+00],\n",
              "          ...,\n",
              "          [-2.9090e-04,  4.8641e-03, -1.9339e-02,  ...,  0.0000e+00,\n",
              "           -7.9118e-04,  0.0000e+00],\n",
              "          [-4.0611e-06,  1.3582e-02, -1.2546e-02,  ...,  0.0000e+00,\n",
              "            1.7068e-02,  0.0000e+00],\n",
              "          [-7.4007e-04,  2.9073e-02, -7.5059e-03,  ...,  0.0000e+00,\n",
              "           -2.7682e-03,  0.0000e+00]],\n",
              " \n",
              "         [[ 7.6427e-03,  1.4638e-02, -3.6971e-02,  ...,  0.0000e+00,\n",
              "            1.3966e-03,  1.0000e+00],\n",
              "          [ 3.2086e-04,  3.3132e-02, -5.2479e-03,  ...,  0.0000e+00,\n",
              "            6.5371e-03,  0.0000e+00],\n",
              "          [ 1.3679e-04,  3.3312e-02, -1.3354e-02,  ...,  0.0000e+00,\n",
              "           -3.6370e-03,  0.0000e+00],\n",
              "          ...,\n",
              "          [-4.0611e-06,  1.3582e-02, -1.2546e-02,  ...,  0.0000e+00,\n",
              "            1.7068e-02,  0.0000e+00],\n",
              "          [-7.4007e-04,  2.9073e-02, -7.5059e-03,  ...,  0.0000e+00,\n",
              "           -2.7682e-03,  0.0000e+00],\n",
              "          [ 2.7272e-04,  1.2008e-02, -4.4505e-03,  ...,  0.0000e+00,\n",
              "            1.2231e-02,  0.0000e+00]]]),\n",
              " tensor([1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 322
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mruUFVdyIxfH"
      },
      "source": [
        "\n",
        "\n",
        "# Torch Model 1 - FeedForward Classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2zH_3fttI6v"
      },
      "source": [
        "#let's first try a simple FC NN\n",
        "class FFNN (nn.Module):\n",
        "    def __init__(self, input_size, num_classes,num_hidden, hidden_dim ):\n",
        "        super().__init__()\n",
        "\n",
        "        assert num_hidden > 0 \n",
        "\n",
        "        self.fc1 = nn.Linear(input_size,hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim,hidden_dim)\n",
        "\n",
        "        self.hidden_layers = nn.ModuleList([])\n",
        "        self.hidden_layers.append (self.fc1)\n",
        "        for i in range (num_hidden -1 ):\n",
        "            self.hidden_layers.append(self.fc2)\n",
        "        \n",
        "        self.final_layer = nn.Linear (hidden_dim,num_classes)\n",
        "        self.dropout = nn.Dropout()\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self,x):\n",
        "        for hidden_layer in self.hidden_layers:\n",
        "            x = hidden_layer(x)\n",
        "            x = self.dropout(x)\n",
        "            x = self.relu(x)\n",
        "        \n",
        "        out = self.final_layer(x)\n",
        "        out_dist = F.log_softmax(out, dim= -1) #why is it -1, ok, coz it's batch x input x class. \n",
        "\n",
        "        return out_dist\n",
        "\n"
      ],
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pbT1kH2jFB6E"
      },
      "source": [
        "#try initiating the model\n",
        "INPUT_SIZE = 19\n",
        "NUM_CLASSES = 2\n",
        "NUM_HIDDEN = 2\n",
        "HIDDEN_DIM = 512\n",
        "\n",
        "model = FFNN(INPUT_SIZE, NUM_CLASSES, NUM_HIDDEN, HIDDEN_DIM)\n"
      ],
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zgc97Ziqkn1G",
        "outputId": "a486e1f9-b527-437f-82ff-ce8597aa498d"
      },
      "source": [
        "model"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FFNN(\n",
              "  (fc1): Linear(in_features=19, out_features=512, bias=True)\n",
              "  (fc2): Linear(in_features=512, out_features=512, bias=True)\n",
              "  (hidden_layers): ModuleList(\n",
              "    (0): Linear(in_features=19, out_features=512, bias=True)\n",
              "    (1): Linear(in_features=512, out_features=512, bias=True)\n",
              "  )\n",
              "  (final_layer): Linear(in_features=512, out_features=2, bias=True)\n",
              "  (dropout): Dropout(p=0.5, inplace=False)\n",
              "  (relu): ReLU()\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3nW0_Vjqk4Ec",
        "outputId": "95624251-e5d0-428a-f5f9-d51d447c74b3"
      },
      "source": [
        "#print model parameters\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')\n"
      ],
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 273,922 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gy_YDstOu6kG"
      },
      "source": [
        "#define loss fuction and optimizer\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "loss_fn = nn.NLLLoss()\n"
      ],
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhKFYZjEvrJ9"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "loss_fn = loss_fn.to(device)"
      ],
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6ejBYTW2rVg"
      },
      "source": [
        "#define a function to calcualte prediction accuracy\n",
        "def accuracy(pred,label):\n",
        "    _,pred_label = torch.max(pred,dim=1)\n",
        "    correct = (pred_label==label).float()\n",
        "    accuracy = correct.sum()/len(correct)\n",
        "    return accuracy"
      ],
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lGnVYr78lcs0"
      },
      "source": [
        "#can i define a function to calculate the f1 score/confusion matrix? \r\n",
        "def get_cfm (pred, label):\r\n",
        "    _,pred_label = torch.max(pred,dim=1)\r\n",
        "    m = classification_report(pred_label.cpu(), label.cpu())\r\n",
        "    return m\r\n"
      ],
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TV01Q5NlvukI"
      },
      "source": [
        "#now define the training and evaluation loop\n",
        "def train(model, dataloader, optimizer, loss_fn):\n",
        "    model.train()\n",
        "\n",
        "    epoch_loss= 0\n",
        "    epoch_acc = 0\n",
        "\n",
        "    for step,batch in enumerate(dataloader):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        data, label = (t for t in batch)\n",
        "        data = data.to(device)\n",
        "        label = label.to(device)\n",
        "\n",
        "        pred = model(data)\n",
        "        loss = loss_fn(pred,label)\n",
        "        acc  = accuracy(pred,label)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc\n",
        "\n",
        "    return epoch_loss/len(dataloader), epoch_acc/len(dataloader)\n",
        "        \n",
        "\n"
      ],
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a07XNBqh5HdM"
      },
      "source": [
        "def evaluate(model,dataloader,loss_fn):\n",
        "    model.eval()\n",
        "\n",
        "    epoch_loss = 0 \n",
        "    epoch_acc = 0 \n",
        "\n",
        "    pred_list =[]\n",
        "    label_list =[]\n",
        "    for step,batch in enumerate(dataloader):\n",
        "        data,label = (t for t in batch)\n",
        "        data = data.to(device)\n",
        "        label = label.to(device)\n",
        "\n",
        "        pred = model(data)\n",
        "        loss = loss_fn(pred,label)\n",
        "        acc = accuracy(pred,label)\n",
        "        \n",
        "        #append the prediction result and show it per epoch later. \n",
        "        if step ==0:     \n",
        "            pred_list=pred.cpu().detach().numpy()\n",
        "            label_list = label.cpu().detach().numpy()\n",
        "        if step != 0:\n",
        "            pred_list = np.concatenate([pred_list,pred.cpu().detach().numpy()],axis =0)\n",
        "            label_list = np.concatenate([label_list,label.cpu().detach().numpy()],axis =0)\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc\n",
        "\n",
        "    report = get_cfm(torch.tensor(pred_list),torch.tensor(label_list))\n",
        "    print (report)\n",
        "    return epoch_loss/len(dataloader), epoch_acc/len(dataloader)\n",
        "        "
      ],
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_EZeClpayNzv",
        "outputId": "2fced397-80ea-41f2-a9de-b0be86c932b8"
      },
      "source": [
        "N_EPOCHS = 5\n",
        "\n",
        "for i in range(N_EPOCHS):\n",
        "    train_loss,train_acc = train(model,train_loader, optimizer, loss_fn)\n",
        "    \n",
        "    eval_loss, eval_acc = evaluate(model,eval_loader, loss_fn)\n",
        "    \n",
        "    print (f' Epoch Number {i}') \n",
        "    print (f' Train. Loss: {train_loss:.3f} Train. Acc: {train_acc*100:.2f}%')\n",
        "    print (f' Eval loss , {eval_loss:.3f}, eval acc, {eval_acc*100:.2f} %')\n",
        "    # print(f'\\t Train. Loss: {train_loss:.3f} |  Train. Acc: {train_acc*100:.2f}%')\n",
        "    # print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
      ],
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.92      0.91       480\n",
            "           1       0.25      0.19      0.22        67\n",
            "\n",
            "    accuracy                           0.83       547\n",
            "   macro avg       0.57      0.56      0.56       547\n",
            "weighted avg       0.81      0.83      0.82       547\n",
            "\n",
            " Epoch Number 0\n",
            " Train. Loss: 0.678 Train. Acc: 55.99%\n",
            " Eval loss , 0.589, eval acc, 83.45 %\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.92      0.91       478\n",
            "           1       0.29      0.22      0.25        69\n",
            "\n",
            "    accuracy                           0.84       547\n",
            "   macro avg       0.59      0.57      0.58       547\n",
            "weighted avg       0.82      0.84      0.82       547\n",
            "\n",
            " Epoch Number 1\n",
            " Train. Loss: 0.618 Train. Acc: 67.00%\n",
            " Eval loss , 0.471, eval acc, 83.80 %\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.92      0.88       448\n",
            "           1       0.33      0.17      0.23        99\n",
            "\n",
            "    accuracy                           0.79       547\n",
            "   macro avg       0.58      0.55      0.55       547\n",
            "weighted avg       0.74      0.79      0.76       547\n",
            "\n",
            " Epoch Number 2\n",
            " Train. Loss: 0.608 Train. Acc: 67.09%\n",
            " Eval loss , 0.478, eval acc, 79.29 %\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.93      0.87       437\n",
            "           1       0.37      0.17      0.24       110\n",
            "\n",
            "    accuracy                           0.78       547\n",
            "   macro avg       0.59      0.55      0.55       547\n",
            "weighted avg       0.73      0.78      0.74       547\n",
            "\n",
            " Epoch Number 3\n",
            " Train. Loss: 0.602 Train. Acc: 67.71%\n",
            " Eval loss , 0.512, eval acc, 78.07 %\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.93      0.86       427\n",
            "           1       0.39      0.17      0.23       120\n",
            "\n",
            "    accuracy                           0.76       547\n",
            "   macro avg       0.60      0.55      0.55       547\n",
            "weighted avg       0.71      0.76      0.72       547\n",
            "\n",
            " Epoch Number 4\n",
            " Train. Loss: 0.587 Train. Acc: 69.55%\n",
            " Eval loss , 0.527, eval acc, 76.68 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mHUUqSS434YB"
      },
      "source": [
        "# #obtain the label list. \r\n",
        "# target_list = []\r\n",
        "# for _,t in eval_dataset:\r\n",
        "#   target_list.append(t)\r\n",
        "\r\n",
        "# #obtain the class weights\r\n",
        "# leng = len(target_list)\r\n",
        "# total_true = sum(target_list)\r\n",
        "# class_count = [leng-total_true, total_true]\r\n",
        "# class_count"
      ],
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KRZib4vB9pq7"
      },
      "source": [
        "# for step,batch in enumerate(eval_loader):\r\n",
        "\r\n",
        "#     data,label = (t for t in batch)\r\n",
        "#     data = data.to(device)\r\n",
        "#     label = label.to(device)\r\n",
        "\r\n",
        "#     pred = model(data)\r\n",
        "#     loss = loss_fn(pred,label)\r\n",
        "#     acc = accuracy(pred,label)\r\n",
        "#     report = get_cfm(pred,label)\r\n",
        "    \r\n",
        "#     epoch_loss += loss.item()\r\n",
        "#     epoch_acc += acc     \r\n",
        "\r\n",
        "        "
      ],
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jy1Db8zvm_fU"
      },
      "source": [
        "    # pred_list =[]\r\n",
        "    # label_list =[]\r\n",
        "    # for step,batch in enumerate(eval_loader):\r\n",
        "    #     data,label = (t for t in batch)\r\n",
        "    #     data = data.to(device)\r\n",
        "    #     label = label.to(device)\r\n",
        "\r\n",
        "    #     pred = model(data)\r\n",
        "    #     loss = loss_fn(pred,label)\r\n",
        "    #     acc = accuracy(pred,label)\r\n",
        "    #     if step ==0:\r\n",
        "    #         pred_list=pred.cpu().detach().numpy()\r\n",
        "    #         label_list = label.cpu().detach().numpy()\r\n",
        "    #     if step != 0:\r\n",
        "    #         pred_list = np.concatenate([pred_list,pred.cpu().detach().numpy()],axis =0)\r\n",
        "    #         label_list = np.concatenate([label_list,label.cpu().detach().numpy()],axis =0)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "    # report = get_cfm(torch.tensor(pred_list),torch.tensor(label_list))\r\n",
        "    # print (report)\r\n"
      ],
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6RRyt4SystDX",
        "outputId": "a2bec302-9acf-413c-98ae-3d3dd0c3f213"
      },
      "source": [
        "target = torch.empty(3).random_(2)\r\n",
        "target"
      ],
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0., 0., 1.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 131
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ojTvT9eqxR5b"
      },
      "source": [
        "# Torch Model 1.1 - FeedForward with CE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WLJPZbgaxZaG"
      },
      "source": [
        "#let's first try a simple FC NN\r\n",
        "class FFNN (nn.Module):\r\n",
        "    def __init__(self, input_size, num_classes,num_hidden, hidden_dim ):\r\n",
        "        super().__init__()\r\n",
        "\r\n",
        "        assert num_hidden > 0 \r\n",
        "\r\n",
        "        self.fc1 = nn.Linear(input_size,hidden_dim)\r\n",
        "        self.fc2 = nn.Linear(hidden_dim,hidden_dim)\r\n",
        "\r\n",
        "        self.hidden_layers = nn.ModuleList([])\r\n",
        "        self.hidden_layers.append (self.fc1)\r\n",
        "        for i in range (num_hidden -1 ):\r\n",
        "            self.hidden_layers.append(self.fc2)\r\n",
        "        \r\n",
        "        self.final_layer = nn.Linear (hidden_dim,num_classes)\r\n",
        "        self.dropout = nn.Dropout()\r\n",
        "        self.relu = nn.ReLU()\r\n",
        "\r\n",
        "    def forward(self,x):\r\n",
        "        for hidden_layer in self.hidden_layers:\r\n",
        "            x = hidden_layer(x)\r\n",
        "            x = self.dropout(x)\r\n",
        "            x = self.relu(x)\r\n",
        "        \r\n",
        "        out = self.final_layer(x)\r\n",
        "         #why is it -1, ok, coz it's batch x input x class. \r\n",
        "\r\n",
        "        return out\r\n"
      ],
      "execution_count": 159,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AMlcM_qozOB0"
      },
      "source": [
        "#try initiating the model\r\n",
        "INPUT_SIZE = 19\r\n",
        "NUM_CLASSES = 2\r\n",
        "NUM_HIDDEN = 2\r\n",
        "HIDDEN_DIM = 512\r\n",
        "\r\n",
        "model = FFNN(INPUT_SIZE, NUM_CLASSES, NUM_HIDDEN, HIDDEN_DIM)\r\n"
      ],
      "execution_count": 160,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sTZe1h5BzRp5"
      },
      "source": [
        "#define loss fuction and optimizer\r\n",
        "optimizer = optim.Adam(model.parameters())\r\n",
        "loss_fn = nn.CrossEntropyLoss()\r\n"
      ],
      "execution_count": 161,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9aPZQU0u0GuL"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\n",
        "model = model.to(device)\r\n",
        "loss_fn = loss_fn.to(device)"
      ],
      "execution_count": 162,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08VJ2ZgqzVc4"
      },
      "source": [
        "#now define the training and evaluation loop\r\n",
        "def train(model, dataloader, optimizer, loss_fn):\r\n",
        "    model.train()\r\n",
        "\r\n",
        "    epoch_loss= 0\r\n",
        "    epoch_acc = 0\r\n",
        "\r\n",
        "    for step,batch in enumerate(dataloader):\r\n",
        "        optimizer.zero_grad()\r\n",
        "\r\n",
        "        data, label = (t for t in batch)\r\n",
        "        data = data.to(device)\r\n",
        "        label = label.to(device)\r\n",
        "\r\n",
        "        pred = model(data)\r\n",
        "        loss = loss_fn(pred,label)\r\n",
        "\r\n",
        "        loss.backward()\r\n",
        "        optimizer.step()\r\n",
        "\r\n",
        "        epoch_loss += loss.item()\r\n",
        "\r\n",
        "    return epoch_loss/len(dataloader)\r\n",
        "        \r\n",
        "\r\n"
      ],
      "execution_count": 163,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0cFc_FIzsRA"
      },
      "source": [
        "def evaluate(model,dataloader,loss_fn):\r\n",
        "    model.eval()\r\n",
        "\r\n",
        "    epoch_loss = 0 \r\n",
        "    epoch_acc = 0 \r\n",
        "\r\n",
        "    pred_list =[]\r\n",
        "    label_list =[]\r\n",
        "    for step,batch in enumerate(dataloader):\r\n",
        "        data,label = (t for t in batch)\r\n",
        "        data = data.to(device)\r\n",
        "        label = label.to(device)\r\n",
        "\r\n",
        "        pred = model(data)\r\n",
        "        loss = loss_fn(pred,label)\r\n",
        "        \r\n",
        "\r\n",
        "        epoch_loss += loss.item()\r\n",
        "\r\n",
        "\r\n",
        "    return epoch_loss/len(dataloader)"
      ],
      "execution_count": 164,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ap6hirwVzvcC",
        "outputId": "84505724-dc75-40db-b677-e01db7fdff06"
      },
      "source": [
        "N_EPOCHS = 10\r\n",
        "\r\n",
        "for i in range(N_EPOCHS):\r\n",
        "    train_loss = train(model,train_loader, optimizer, loss_fn)\r\n",
        "    \r\n",
        "    eval_loss = evaluate(model,eval_loader, loss_fn)\r\n",
        "    \r\n",
        "    print (f' Epoch Number {i}') \r\n",
        "    print (f' Train. Loss: {train_loss:.3f} ')\r\n",
        "    print (f' Eval loss , {eval_loss:.3f} ')\r\n",
        "    # print(f'\\t Train. Loss: {train_loss:.3f} |  Train. Acc: {train_acc*100:.2f}%')\r\n",
        "    # print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
      ],
      "execution_count": 165,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Epoch Number 0\n",
            " Train. Loss: 0.664 \n",
            " Eval loss , 0.591 \n",
            " Epoch Number 1\n",
            " Train. Loss: 0.622 \n",
            " Eval loss , 0.504 \n",
            " Epoch Number 2\n",
            " Train. Loss: 0.617 \n",
            " Eval loss , 0.569 \n",
            " Epoch Number 3\n",
            " Train. Loss: 0.579 \n",
            " Eval loss , 0.488 \n",
            " Epoch Number 4\n",
            " Train. Loss: 0.601 \n",
            " Eval loss , 0.527 \n",
            " Epoch Number 5\n",
            " Train. Loss: 0.573 \n",
            " Eval loss , 0.472 \n",
            " Epoch Number 6\n",
            " Train. Loss: 0.603 \n",
            " Eval loss , 0.544 \n",
            " Epoch Number 7\n",
            " Train. Loss: 0.581 \n",
            " Eval loss , 0.543 \n",
            " Epoch Number 8\n",
            " Train. Loss: 0.602 \n",
            " Eval loss , 0.536 \n",
            " Epoch Number 9\n",
            " Train. Loss: 0.585 \n",
            " Eval loss , 0.487 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bf_54Mrvw1aO"
      },
      "source": [
        "# Torch Model 2 - Feedforward Classification with F1 loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8hcgMTYw7h6"
      },
      "source": [
        "#let's first try a simple FC NN\r\n",
        "class FFNN_F1 (nn.Module):\r\n",
        "    def __init__(self, input_size, num_classes,num_hidden, hidden_dim ):\r\n",
        "        super().__init__()\r\n",
        "\r\n",
        "        assert num_hidden > 0 \r\n",
        "\r\n",
        "        self.fc1 = nn.Linear(input_size,hidden_dim)\r\n",
        "        self.fc2 = nn.Linear(hidden_dim,hidden_dim)\r\n",
        "\r\n",
        "        self.hidden_layers = nn.ModuleList([])\r\n",
        "        self.hidden_layers.append (self.fc1)\r\n",
        "        for i in range (num_hidden -1 ):\r\n",
        "            self.hidden_layers.append(self.fc2)\r\n",
        "        \r\n",
        "        self.final_layer = nn.Linear (hidden_dim,num_classes)\r\n",
        "        self.dropout = nn.Dropout()\r\n",
        "        self.relu = nn.ReLU()\r\n",
        "\r\n",
        "    def forward(self,x):\r\n",
        "        for hidden_layer in self.hidden_layers:\r\n",
        "            x = hidden_layer(x)\r\n",
        "            x = self.dropout(x)\r\n",
        "            x = self.relu(x)\r\n",
        "        \r\n",
        "        out = self.final_layer(x)\r\n",
        "               \r\n",
        "        return out\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5v_3EXVoYrtc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JNYld1FHw7fq",
        "outputId": "e7f2f390-5bf2-4ece-df2f-bf8a53da78f0"
      },
      "source": [
        "#try initiating the model\r\n",
        "INPUT_SIZE = 19\r\n",
        "NUM_CLASSES = 2\r\n",
        "NUM_HIDDEN = 2\r\n",
        "HIDDEN_DIM = 512\r\n",
        "\r\n",
        "model = FFNN_F1(INPUT_SIZE, NUM_CLASSES, NUM_HIDDEN, HIDDEN_DIM)\r\n",
        "\r\n",
        "model\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FFNN_F1(\n",
              "  (fc1): Linear(in_features=19, out_features=512, bias=True)\n",
              "  (fc2): Linear(in_features=512, out_features=512, bias=True)\n",
              "  (hidden_layers): ModuleList(\n",
              "    (0): Linear(in_features=19, out_features=512, bias=True)\n",
              "    (1): Linear(in_features=512, out_features=512, bias=True)\n",
              "  )\n",
              "  (final_layer): Linear(in_features=512, out_features=2, bias=True)\n",
              "  (dropout): Dropout(p=0.5, inplace=False)\n",
              "  (relu): ReLU()\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jKh99uxBY6ie",
        "outputId": "bc4d55f7-9c5d-4f0b-b15b-defcfd488aa4"
      },
      "source": [
        "#print model parameters\r\n",
        "def count_parameters(model):\r\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\r\n",
        "\r\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 273,922 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xb7-RwCBw7dp",
        "outputId": "b2d9ad1d-e510-492e-d5d7-d56bcbc5862f"
      },
      "source": [
        "#define optimizer, loss function\r\n",
        "class F1_Loss(nn.Module):\r\n",
        "    '''Calculate F1 score. Can work with gpu tensors\r\n",
        "    \r\n",
        "    The original implmentation is written by Michal Haltuf on Kaggle.\r\n",
        "    \r\n",
        "    Returns\r\n",
        "    -------\r\n",
        "    torch.Tensor\r\n",
        "        `ndim` == 1. epsilon <= val <= 1\r\n",
        "    \r\n",
        "    Reference\r\n",
        "    ---------\r\n",
        "    - https://www.kaggle.com/rejpalcz/best-loss-function-for-f1-score-metric\r\n",
        "    - https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score\r\n",
        "    - https://discuss.pytorch.org/t/calculating-precision-recall-and-f1-score-in-case-of-multi-label-classification/28265/6\r\n",
        "    - http://www.ryanzhang.info/python/writing-your-own-loss-function-module-for-pytorch/\r\n",
        "    '''\r\n",
        "    def __init__(self, epsilon=1e-7):\r\n",
        "        super().__init__()\r\n",
        "        self.epsilon = epsilon\r\n",
        "        \r\n",
        "    def forward(self, y_pred, y_true,):\r\n",
        "        assert y_pred.ndim == 2\r\n",
        "        assert y_true.ndim == 1\r\n",
        "        y_true = F.one_hot(y_true, 2).to(torch.float32)\r\n",
        "        y_pred = F.softmax(y_pred, dim=1)\r\n",
        "        \r\n",
        "        tp = (y_true * y_pred).sum(dim=0).to(torch.float32)\r\n",
        "        tn = ((1 - y_true) * (1 - y_pred)).sum(dim=0).to(torch.float32)\r\n",
        "        fp = ((1 - y_true) * y_pred).sum(dim=0).to(torch.float32)\r\n",
        "        fn = (y_true * (1 - y_pred)).sum(dim=0).to(torch.float32)\r\n",
        "\r\n",
        "        precision = tp / (tp + fp + self.epsilon)\r\n",
        "        recall = tp / (tp + fn + self.epsilon)\r\n",
        "\r\n",
        "        f1 = 2* (precision*recall) / (precision + recall + self.epsilon)\r\n",
        "        f1 = f1.clamp(min=self.epsilon, max=1-self.epsilon)\r\n",
        "        return 1 - f1.mean()\r\n",
        "\r\n",
        "\r\n",
        "params = model.parameters()\r\n",
        "optimizer = optim.Adam(params)\r\n",
        "loss_fn =F1_Loss()\r\n",
        "\r\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\n",
        "model = model.to(device)\r\n",
        "loss_fn = loss_fn.to(device)\r\n",
        "\r\n",
        "print (device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8hgaRdJEw7ak"
      },
      "source": [
        "def accuracy(preds, y):\r\n",
        "    \"\"\"\r\n",
        "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    #round predictions to the closest integer\r\n",
        "    prediciton = F.softmax(preds, dim =1)\r\n",
        "    _, pred = torch.max(prediciton, 1)\r\n",
        "    correct = (pred == y).float() #convert into float for division \r\n",
        "    acc = correct.sum() / len(correct)\r\n",
        "    return acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0M9YnRqa1_l"
      },
      "source": [
        "#can i define a function to calculate the f1 score/confusion matrix? \r\n",
        "def get_cfm (preds, label):\r\n",
        "    pred = F.softmax(preds, dim =1)\r\n",
        "    _,pred_label = torch.max(pred,dim=1)\r\n",
        "    m = classification_report(pred_label.cpu(), label.cpu())\r\n",
        "    return m"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3kn6Ua6KZZYF"
      },
      "source": [
        "#now define the training and evaluation loop\r\n",
        "def train(model, dataloader, optimizer, loss_fn):\r\n",
        "    model.train()\r\n",
        "\r\n",
        "    epoch_loss= 0\r\n",
        "    epoch_acc = 0\r\n",
        "\r\n",
        "    for step,batch in enumerate(dataloader):\r\n",
        "        optimizer.zero_grad()\r\n",
        "\r\n",
        "        data, label = (t for t in batch)\r\n",
        "        data = data.to(device)\r\n",
        "        label = label.to(device)\r\n",
        "\r\n",
        "        pred = model(data)\r\n",
        "        loss = loss_fn(pred,label)\r\n",
        "        acc  = accuracy(pred,label)\r\n",
        "\r\n",
        "        loss.backward()\r\n",
        "        optimizer.step()\r\n",
        "\r\n",
        "        epoch_loss += loss.item()\r\n",
        "        epoch_acc += acc\r\n",
        "\r\n",
        "    return epoch_loss/len(dataloader), epoch_acc/len(dataloader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bUWmYf6mZZVI"
      },
      "source": [
        "def evaluate(model,dataloader,loss_fn):\r\n",
        "    model.eval()\r\n",
        "\r\n",
        "    epoch_loss = 0 \r\n",
        "    epoch_acc = 0 \r\n",
        "\r\n",
        "    pred_list =[]\r\n",
        "    label_list =[]\r\n",
        "    for step,batch in enumerate(dataloader):\r\n",
        "        data,label = (t for t in batch)\r\n",
        "        data = data.to(device)\r\n",
        "        label = label.to(device)\r\n",
        "\r\n",
        "        pred = model(data)\r\n",
        "        loss = loss_fn(pred,label)\r\n",
        "        acc = accuracy(pred,label)\r\n",
        "        \r\n",
        "        #append the prediction result and show it per epoch later. \r\n",
        "        if step ==0:     \r\n",
        "            pred_list=pred.cpu().detach().numpy()\r\n",
        "            label_list = label.cpu().detach().numpy()\r\n",
        "        if step != 0:\r\n",
        "            pred_list = np.concatenate([pred_list,pred.cpu().detach().numpy()],axis =0)\r\n",
        "            label_list = np.concatenate([label_list,label.cpu().detach().numpy()],axis =0)\r\n",
        "\r\n",
        "        epoch_loss += loss.item()\r\n",
        "        epoch_acc += acc\r\n",
        "\r\n",
        "    report = get_cfm(torch.tensor(pred_list),torch.tensor(label_list))\r\n",
        "    print (report)\r\n",
        "    return epoch_loss/len(dataloader), epoch_acc/len(dataloader)\r\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PvIYSXJCZZSe",
        "outputId": "fe276acd-01e6-41f9-9c45-9ce877d28456"
      },
      "source": [
        "N_EPOCHS = 10\r\n",
        "\r\n",
        "for i in range(N_EPOCHS):\r\n",
        "    train_loss,train_acc = train(model,train_loader, optimizer, loss_fn)\r\n",
        "    \r\n",
        "    eval_loss, eval_acc = evaluate(model,eval_loader, loss_fn)\r\n",
        "    \r\n",
        "    print (f' Epoch Number {i}') \r\n",
        "    print (f' Train. Loss: {train_loss:.3f} Train. Acc: {train_acc*100:.2f}%')\r\n",
        "    print (f' Eval loss , {eval_loss:.3f}, eval acc, {eval_acc*100:.2f} %')\r\n",
        "    # print(f'\\t Train. Loss: {train_loss:.3f} |  Train. Acc: {train_acc*100:.2f}%')\r\n",
        "    # print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.92      0.90       474\n",
            "           1       0.27      0.19      0.23        73\n",
            "\n",
            "    accuracy                           0.82       547\n",
            "   macro avg       0.58      0.56      0.56       547\n",
            "weighted avg       0.80      0.82      0.81       547\n",
            "\n",
            " Epoch Number 0\n",
            " Train. Loss: 0.481 Train. Acc: 57.39%\n",
            " Eval loss , 0.571, eval acc, 82.76 %\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.93      0.88       444\n",
            "           1       0.39      0.19      0.26       103\n",
            "\n",
            "    accuracy                           0.79       547\n",
            "   macro avg       0.61      0.56      0.57       547\n",
            "weighted avg       0.75      0.79      0.76       547\n",
            "\n",
            " Epoch Number 1\n",
            " Train. Loss: 0.427 Train. Acc: 62.00%\n",
            " Eval loss , 0.521, eval acc, 79.63 %\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.73      0.94      0.82       383\n",
            "           1       0.55      0.17      0.26       164\n",
            "\n",
            "    accuracy                           0.71       547\n",
            "   macro avg       0.64      0.56      0.54       547\n",
            "weighted avg       0.67      0.71      0.65       547\n",
            "\n",
            " Epoch Number 2\n",
            " Train. Loss: 0.353 Train. Acc: 68.11%\n",
            " Eval loss , 0.534, eval acc, 71.96 %\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.93      0.86       424\n",
            "           1       0.43      0.18      0.25       123\n",
            "\n",
            "    accuracy                           0.76       547\n",
            "   macro avg       0.61      0.56      0.56       547\n",
            "weighted avg       0.71      0.76      0.72       547\n",
            "\n",
            " Epoch Number 3\n",
            " Train. Loss: 0.331 Train. Acc: 69.19%\n",
            " Eval loss , 0.505, eval acc, 77.00 %\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.93      0.86       426\n",
            "           1       0.39      0.17      0.23       121\n",
            "\n",
            "    accuracy                           0.76       547\n",
            "   macro avg       0.59      0.55      0.54       547\n",
            "weighted avg       0.71      0.76      0.72       547\n",
            "\n",
            " Epoch Number 4\n",
            " Train. Loss: 0.304 Train. Acc: 71.08%\n",
            " Eval loss , 0.502, eval acc, 76.51 %\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.71      0.94      0.81       375\n",
            "           1       0.57      0.17      0.26       172\n",
            "\n",
            "    accuracy                           0.70       547\n",
            "   macro avg       0.64      0.55      0.54       547\n",
            "weighted avg       0.67      0.70      0.64       547\n",
            "\n",
            " Epoch Number 5\n",
            " Train. Loss: 0.324 Train. Acc: 67.98%\n",
            " Eval loss , 0.520, eval acc, 70.78 %\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.93      0.85       419\n",
            "           1       0.41      0.16      0.23       128\n",
            "\n",
            "    accuracy                           0.75       547\n",
            "   macro avg       0.60      0.55      0.54       547\n",
            "weighted avg       0.70      0.75      0.71       547\n",
            "\n",
            " Epoch Number 6\n",
            " Train. Loss: 0.320 Train. Acc: 69.38%\n",
            " Eval loss , 0.505, eval acc, 75.78 %\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.93      0.85       417\n",
            "           1       0.41      0.16      0.23       130\n",
            "\n",
            "    accuracy                           0.75       547\n",
            "   macro avg       0.60      0.54      0.54       547\n",
            "weighted avg       0.69      0.75      0.70       547\n",
            "\n",
            " Epoch Number 7\n",
            " Train. Loss: 0.319 Train. Acc: 69.08%\n",
            " Eval loss , 0.507, eval acc, 75.44 %\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.93      0.85       417\n",
            "           1       0.43      0.17      0.24       130\n",
            "\n",
            "    accuracy                           0.75       547\n",
            "   macro avg       0.61      0.55      0.55       547\n",
            "weighted avg       0.70      0.75      0.71       547\n",
            "\n",
            " Epoch Number 8\n",
            " Train. Loss: 0.300 Train. Acc: 70.80%\n",
            " Eval loss , 0.508, eval acc, 75.78 %\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.75      0.94      0.84       397\n",
            "           1       0.53      0.18      0.27       150\n",
            "\n",
            "    accuracy                           0.73       547\n",
            "   macro avg       0.64      0.56      0.55       547\n",
            "weighted avg       0.69      0.73      0.68       547\n",
            "\n",
            " Epoch Number 9\n",
            " Train. Loss: 0.302 Train. Acc: 70.76%\n",
            " Eval loss , 0.513, eval acc, 73.90 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XqSIcZPr9qCY"
      },
      "source": [
        "# Torch Model 3 - Feedforward Regresssion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U02qHiN79wNe"
      },
      "source": [
        "#let's first try a simple FC NN\n",
        "class FFNN_regression (nn.Module):\n",
        "    def __init__(self, input_size,num_hidden, hidden_dim ):\n",
        "        super().__init__()\n",
        "\n",
        "        assert num_hidden > 0 \n",
        "\n",
        "        self.fc1 = nn.Linear(input_size,hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim,hidden_dim)\n",
        "\n",
        "        self.hidden_layers = nn.ModuleList([])\n",
        "        self.hidden_layers.append (self.fc1)\n",
        "        for i in range (num_hidden -1 ):\n",
        "            self.hidden_layers.append(self.fc2)\n",
        "        \n",
        "        self.final_layer = nn.Linear (hidden_dim,1)\n",
        "        self.dropout = nn.Dropout()\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self,x):\n",
        "        for hidden_layer in self.hidden_layers:\n",
        "            x = hidden_layer(x)\n",
        "            x = self.dropout(x)\n",
        "            x = self.relu(x)\n",
        "        \n",
        "        out = self.final_layer(x)\n",
        "        \n",
        "        return out\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oOkl8d-M-EtB"
      },
      "source": [
        "#try initiating the model\n",
        "INPUT_SIZE = 18\n",
        "NUM_HIDDEN = 4\n",
        "HIDDEN_DIM = 1024\n",
        "\n",
        "model = FFNN_regression(INPUT_SIZE, NUM_HIDDEN, HIDDEN_DIM)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gE-ZITBl-EqZ"
      },
      "source": [
        "#define loss fuction and optimizer\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "loss_fn = nn.L1Loss()\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "loss_fn = loss_fn.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qQhSlaHL-Eit"
      },
      "source": [
        "#now define the training and evaluation loop\n",
        "def train(model, dataloader, optimizer, loss_fn):\n",
        "    model.train()\n",
        "\n",
        "    epoch_loss= 0\n",
        "\n",
        "    for step,batch in enumerate(dataloader):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        data, label = (t for t in batch)\n",
        "        data = data.to(device)\n",
        "        label = label.to(device)\n",
        "\n",
        "        pred = model(data).squeeze(1)\n",
        "        loss = loss_fn(pred,label)\n",
        "\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss/len(dataloader)\n",
        "        \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1LTWnox_rJK"
      },
      "source": [
        "def evaluate(model,dataloader,loss_fn):\n",
        "    model.eval()\n",
        "\n",
        "    epoch_loss = 0 \n",
        "\n",
        "    for step,batch in enumerate(dataloader):\n",
        "        data,label = (t for t in batch)\n",
        "        data = data.to(device)\n",
        "        label = label.to(device)\n",
        "\n",
        "        pred = model(data).squeeze(1)\n",
        "        loss = loss_fn(pred,label)\n",
        "\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss/len(dataloader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JGq3iPf3-lO1",
        "outputId": "55b6a105-3143-4c36-e4dc-7c85e1a66556"
      },
      "source": [
        "N_EPOCHS = 20 \n",
        "\n",
        "for i in range(N_EPOCHS):\n",
        "    train_loss = train(model,train_loader, optimizer, loss_fn)\n",
        "    eval_loss = evaluate(model,eval_loader, loss_fn)\n",
        "    \n",
        "    print (f' Epoch Number {i}') \n",
        "    print (f' Train. Loss: {train_loss:.5f} ')\n",
        "    print (f' Eval loss , {eval_loss:.5f} ')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Epoch Number 0\n",
            " Train. Loss: 0.01819 \n",
            " Eval loss , 0.01586 \n",
            " Epoch Number 1\n",
            " Train. Loss: 0.01819 \n",
            " Eval loss , 0.01614 \n",
            " Epoch Number 2\n",
            " Train. Loss: 0.01786 \n",
            " Eval loss , 0.01623 \n",
            " Epoch Number 3\n",
            " Train. Loss: 0.01790 \n",
            " Eval loss , 0.01642 \n",
            " Epoch Number 4\n",
            " Train. Loss: 0.01800 \n",
            " Eval loss , 0.01593 \n",
            " Epoch Number 5\n",
            " Train. Loss: 0.01772 \n",
            " Eval loss , 0.01546 \n",
            " Epoch Number 6\n",
            " Train. Loss: 0.01777 \n",
            " Eval loss , 0.01570 \n",
            " Epoch Number 7\n",
            " Train. Loss: 0.01737 \n",
            " Eval loss , 0.01564 \n",
            " Epoch Number 8\n",
            " Train. Loss: 0.01764 \n",
            " Eval loss , 0.01640 \n",
            " Epoch Number 9\n",
            " Train. Loss: 0.01744 \n",
            " Eval loss , 0.01590 \n",
            " Epoch Number 10\n",
            " Train. Loss: 0.01722 \n",
            " Eval loss , 0.01566 \n",
            " Epoch Number 11\n",
            " Train. Loss: 0.01740 \n",
            " Eval loss , 0.01632 \n",
            " Epoch Number 12\n",
            " Train. Loss: 0.01794 \n",
            " Eval loss , 0.01632 \n",
            " Epoch Number 13\n",
            " Train. Loss: 0.01759 \n",
            " Eval loss , 0.01537 \n",
            " Epoch Number 14\n",
            " Train. Loss: 0.01768 \n",
            " Eval loss , 0.01596 \n",
            " Epoch Number 15\n",
            " Train. Loss: 0.01705 \n",
            " Eval loss , 0.01582 \n",
            " Epoch Number 16\n",
            " Train. Loss: 0.01724 \n",
            " Eval loss , 0.01615 \n",
            " Epoch Number 17\n",
            " Train. Loss: 0.01743 \n",
            " Eval loss , 0.01585 \n",
            " Epoch Number 18\n",
            " Train. Loss: 0.01743 \n",
            " Eval loss , 0.01560 \n",
            " Epoch Number 19\n",
            " Train. Loss: 0.01714 \n",
            " Eval loss , 0.01593 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PyEdBEQh_ayw"
      },
      "source": [
        "The loss seems way too small??? am I calculating it correctly???\n",
        "I'm going to use L1loss instead of L2 loss and see how it goes. \n",
        "It seems that the class imbalance is too dominant, I'll have to work around it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EVf57rdX_GQh",
        "outputId": "3241d689-b33d-4b95-c43c-af6153234831"
      },
      "source": [
        "for i,batch in enumerate(train_loader):\n",
        "    data,label = (t for t in batch)\n",
        "    print (i)\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GboHeo62BWBW",
        "outputId": "df0a8af7-4279-4a38-db5f-9a85f08658b7"
      },
      "source": [
        "data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 2.9815e-04,  7.7955e-02, -4.8741e-04,  ...,  7.8442e-02,\n",
              "          0.0000e+00,  0.0000e+00],\n",
              "        [ 1.8731e-03,  4.4611e-02, -2.0406e-03,  ...,  4.6651e-02,\n",
              "          0.0000e+00,  0.0000e+00],\n",
              "        [ 4.7553e-04,  6.1652e-03, -3.5175e-02,  ...,  4.1340e-02,\n",
              "          0.0000e+00,  0.0000e+00],\n",
              "        ...,\n",
              "        [-2.4258e-04,  3.4845e-02, -2.3194e-02,  ...,  5.8039e-02,\n",
              "          0.0000e+00,  0.0000e+00],\n",
              "        [-6.1106e-04,  9.8598e-04, -8.4670e-02,  ...,  8.5656e-02,\n",
              "          0.0000e+00,  0.0000e+00],\n",
              "        [ 7.6427e-03,  1.4638e-02, -3.6971e-02,  ...,  5.1609e-02,\n",
              "          0.0000e+00,  1.0000e+00]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 290
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6v5viZ5fBxE5",
        "outputId": "4a23d3c0-223a-4650-e376-a19c80c97d28"
      },
      "source": [
        "label"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0.0446,  0.0062,  0.0658,  0.0292,  0.2194,  0.0073,  0.0116,  0.0205,\n",
              "         0.0400,  0.0313,  0.0649,  0.0670,  0.0324,  0.0123,  0.0553,  0.0617,\n",
              "         0.0361,  0.0074,  0.0025,  0.0077,  0.0162,  0.0138,  0.0834,  0.1036,\n",
              "         0.0322,  0.0200,  0.0510,  0.0026,  0.0269,  0.0284,  0.0473,  0.0076,\n",
              "         0.0175,  0.0042,  0.0025,  0.0010,  0.0856,  0.0034,  0.0291,  0.0619,\n",
              "         0.0367,  0.0090,  0.0314,  0.0052,  0.0188,  0.0058,  0.0669,  0.0360,\n",
              "         0.0193,  0.0024,  0.0005,  0.0035,  0.0165,  0.0270,  0.0061, -0.0006,\n",
              "         0.0310,  0.0150,  0.0017,  0.0374,  0.0348,  0.0010,  0.0146,  0.0331])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 291
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n3swifsyBysK",
        "outputId": "e3be6c4d-f594-4733-aad6-04a049b6ab0c"
      },
      "source": [
        "model(data.to(device)).squeeze()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.0306, 0.0253, 0.0240, 0.0256, 0.0248, 0.0387, 0.0230, 0.0273, 0.0294,\n",
              "        0.0302, 0.0312, 0.0329, 0.0207, 0.0208, 0.0152, 0.0203, 0.0126, 0.0092,\n",
              "        0.0113, 0.0096, 0.0081, 0.0083, 0.0082, 0.0177, 0.0202, 0.0215, 0.0082,\n",
              "        0.0101, 0.0153, 0.0084, 0.0109, 0.0119, 0.0129, 0.0082, 0.0082, 0.0078,\n",
              "        0.0074, 0.0177, 0.0110, 0.0160, 0.0200, 0.0211, 0.0205, 0.0203, 0.0196,\n",
              "        0.0192, 0.0197, 0.0226, 0.0210, 0.0200, 0.0206, 0.0205, 0.0193, 0.0161,\n",
              "        0.0170, 0.0176, 0.0211, 0.0222, 0.0179, 0.0064, 0.0064, 0.0088, 0.0134,\n",
              "        0.0227], device='cuda:0', grad_fn=<SqueezeBackward0>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 292
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zp13tzHLB1mx",
        "outputId": "a27cd2bf-3d43-4de8-d2d3-a36645bc10a9"
      },
      "source": [
        "loss_fn(model(data.to(device)).squeeze(),label.to(device))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.0214, device='cuda:0', grad_fn=<L1LossBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 293
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6_e3F0DCGp2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_E-oMKnKN-L9"
      },
      "source": [
        "#Torch Model 4 - LSTM Classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X5LDTZiSN9jL"
      },
      "source": [
        ""
      ],
      "execution_count": 382,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uhvmCMcpOi48"
      },
      "source": [
        "The previous 3 models used the almost the same dataloader because the similar input format, for LSTM I will need to make some adjustments. And perhaps try out 2 scenarios with and w/o the weighted sampler. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4EPDbh22O5Xn"
      },
      "source": [
        "class LSTMModel(nn.Module):\r\n",
        "    def __init__(self, input_size, num_classes,  num_hidden, hidden_dim):\r\n",
        "        super().__init__()\r\n",
        "        #num_layers = num_layers\r\n",
        "        # self.input_size = input_size\r\n",
        "        # self.hidden_size = hidden_size\r\n",
        "        self.lstm = nn.LSTM(input_size, hidden_dim, num_hidden, batch_first=True, bidirectional=False, dropout=0.5)\r\n",
        "        self.ff = nn.Linear(hidden_dim, num_classes)\r\n",
        "        self.dropout = nn.Dropout()\r\n",
        "\r\n",
        "        #need to change the loss function. \r\n",
        "    def forward(self, input):\r\n",
        "        output, (hn, cn) = self.lstm(input)\r\n",
        "        hn = self.dropout(hn[-1,:,:])\r\n",
        "        return self.ff(hn)"
      ],
      "execution_count": 383,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Y0n47b1Qj2I"
      },
      "source": [
        "INPUT_SIZE = 19\r\n",
        "NUM_CLASSES = 2\r\n",
        "NUM_HIDDEN = 2\r\n",
        "HIDDEN_DIM = 512\r\n",
        "\r\n",
        "model = LSTMModel(INPUT_SIZE, NUM_CLASSES, NUM_HIDDEN, HIDDEN_DIM)"
      ],
      "execution_count": 384,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zfj9ru919ubE"
      },
      "source": [
        "class FocalLoss(nn.Module):\r\n",
        "    def __init__(self, gamma=0, alpha=None, size_average=True):\r\n",
        "        super(FocalLoss, self).__init__()\r\n",
        "        self.gamma = gamma\r\n",
        "        self.alpha = alpha\r\n",
        "        if isinstance(alpha,(float,int)): self.alpha = torch.Tensor([alpha,1-alpha])\r\n",
        "        if isinstance(alpha,list): self.alpha = torch.Tensor(alpha)\r\n",
        "        self.size_average = size_average\r\n",
        "\r\n",
        "    def forward(self, input, target):\r\n",
        "        if input.dim()>2:\r\n",
        "            input = input.view(input.size(0),input.size(1),-1)  # N,C,H,W => N,C,H*W\r\n",
        "            input = input.transpose(1,2)    # N,C,H*W => N,H*W,C\r\n",
        "            input = input.contiguous().view(-1,input.size(2))   # N,H*W,C => N*H*W,C\r\n",
        "        target = target.view(-1,1)\r\n",
        "\r\n",
        "        logpt = F.log_softmax(input)\r\n",
        "        logpt = logpt.gather(1,target)\r\n",
        "        logpt = logpt.view(-1)\r\n",
        "        pt = Variable(logpt.data.exp())\r\n",
        "\r\n",
        "        if self.alpha is not None:\r\n",
        "            if self.alpha.type()!=input.data.type():\r\n",
        "                self.alpha = self.alpha.type_as(input.data)\r\n",
        "            at = self.alpha.gather(0,target.data.view(-1))\r\n",
        "            logpt = logpt * Variable(at)\r\n",
        "\r\n",
        "        loss = -1 * (1-pt)**self.gamma * logpt\r\n",
        "        if self.size_average: return loss.mean()\r\n",
        "        else: return loss.sum()"
      ],
      "execution_count": 385,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "93C2J3V1Ud8c",
        "outputId": "c2f0ffbb-957f-45ca-b575-562194e9af31"
      },
      "source": [
        "model"
      ],
      "execution_count": 386,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LSTMModel(\n",
              "  (lstm): LSTM(19, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
              "  (ff): Linear(in_features=512, out_features=2, bias=True)\n",
              "  (dropout): Dropout(p=0.5, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 386
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SFfo_4kuUpcS",
        "outputId": "77e75f41-397a-45b4-8e7c-b4a20aca6611"
      },
      "source": [
        "#print model parameters\r\n",
        "def count_parameters(model):\r\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\r\n",
        "\r\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')\r\n"
      ],
      "execution_count": 387,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 3,193,858 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p0hdVHycWRIi"
      },
      "source": [
        "#define loss fuction and optimizer\r\n",
        "optimizer = optim.Adam(model.parameters())\r\n",
        "#loss_fn = nn.NLLLoss()\r\n",
        "loss_fn= FocalLoss(gamma=0,alpha=0.2)"
      ],
      "execution_count": 388,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fnCJxYz-WX_z"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\n",
        "model = model.to(device)\r\n",
        "loss_fn = loss_fn.to(device)"
      ],
      "execution_count": 389,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5T6Pr2z9UvD5"
      },
      "source": [
        "def accuracy(preds, y):\r\n",
        "    \"\"\"\r\n",
        "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    #round predictions to the closest integer\r\n",
        "    prediciton = F.softmax(preds, dim =1)\r\n",
        "    _, pred = torch.max(prediciton, 1)\r\n",
        "    correct = (pred == y).float() #convert into float for division \r\n",
        "    acc = correct.sum() / len(correct)\r\n",
        "    return acc"
      ],
      "execution_count": 390,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8mJpUVIWdSI"
      },
      "source": [
        "#can i define a function to calculate the f1 score/confusion matrix? \r\n",
        "def get_cfm (preds, label):\r\n",
        "    pred = F.softmax(preds, dim =1)\r\n",
        "    _,pred_label = torch.max(pred,dim=1)\r\n",
        "    m = classification_report(pred_label.cpu(), label.cpu())\r\n",
        "    return m"
      ],
      "execution_count": 391,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TpJK5wkYWhN8"
      },
      "source": [
        "#now define the training and evaluation loop\r\n",
        "def train(model, dataloader, optimizer, loss_fn):\r\n",
        "    model.train()\r\n",
        "\r\n",
        "    epoch_loss= 0\r\n",
        "    epoch_acc = 0\r\n",
        "\r\n",
        "    for step,batch in enumerate(dataloader):\r\n",
        "        optimizer.zero_grad()\r\n",
        "\r\n",
        "        data, label = (t for t in batch)\r\n",
        "        data = data.to(device)\r\n",
        "        label = label.to(device)\r\n",
        "\r\n",
        "        pred = model(data)\r\n",
        "        loss = loss_fn(pred,label)\r\n",
        "        acc  = accuracy(pred,label)\r\n",
        "\r\n",
        "        #append the prediction result and show it per epoch later. \r\n",
        "        if step ==0:     \r\n",
        "            pred_list=pred.cpu().detach().numpy()\r\n",
        "            label_list = label.cpu().detach().numpy()\r\n",
        "        if step != 0:\r\n",
        "            pred_list = np.concatenate([pred_list,pred.cpu().detach().numpy()],axis =0)\r\n",
        "            label_list = np.concatenate([label_list,label.cpu().detach().numpy()],axis =0)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "        loss.backward()\r\n",
        "        optimizer.step()\r\n",
        "\r\n",
        "        epoch_loss += loss.item()\r\n",
        "        epoch_acc += acc\r\n",
        "    report = get_cfm(torch.tensor(pred_list),torch.tensor(label_list))\r\n",
        "    print (report)\r\n",
        "    return epoch_loss/len(dataloader), epoch_acc/len(dataloader)\r\n",
        "\r\n"
      ],
      "execution_count": 392,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ls3DvxraWkgw"
      },
      "source": [
        "def evaluate(model,dataloader,loss_fn):\r\n",
        "    model.eval()\r\n",
        "\r\n",
        "    epoch_loss = 0 \r\n",
        "    epoch_acc = 0 \r\n",
        "\r\n",
        "    pred_list =[]\r\n",
        "    label_list =[]\r\n",
        "    for step,batch in enumerate(dataloader):\r\n",
        "        data,label = (t for t in batch)\r\n",
        "        data = data.to(device)\r\n",
        "        label = label.to(device)\r\n",
        "\r\n",
        "        pred = model(data)\r\n",
        "        loss = loss_fn(pred,label)\r\n",
        "        acc = accuracy(pred,label)\r\n",
        "        \r\n",
        "        #append the prediction result and show it per epoch later. \r\n",
        "        if step ==0:     \r\n",
        "            pred_list=pred.cpu().detach().numpy()\r\n",
        "            label_list = label.cpu().detach().numpy()\r\n",
        "        if step != 0:\r\n",
        "            pred_list = np.concatenate([pred_list,pred.cpu().detach().numpy()],axis =0)\r\n",
        "            label_list = np.concatenate([label_list,label.cpu().detach().numpy()],axis =0)\r\n",
        "\r\n",
        "        epoch_loss += loss.item()\r\n",
        "        epoch_acc += acc\r\n",
        "\r\n",
        "    report = get_cfm(torch.tensor(pred_list),torch.tensor(label_list))\r\n",
        "    print (report)\r\n",
        "    return epoch_loss/len(dataloader), epoch_acc/len(dataloader)"
      ],
      "execution_count": 393,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H6fc8cJzWnjN",
        "outputId": "dd81e8c0-dcaf-407d-e424-77c3128c3da1"
      },
      "source": [
        "N_EPOCHS = 15\r\n",
        "\r\n",
        "for i in range(N_EPOCHS):\r\n",
        "    train_loss,train_acc = train(model,train_loader, optimizer, loss_fn)\r\n",
        "    \r\n",
        "    eval_loss, eval_acc = evaluate(model,eval_loader, loss_fn)\r\n",
        "    \r\n",
        "    print (f' Epoch Number {i}') \r\n",
        "    print (f' Train. Loss: {train_loss:.3f} Train. Acc: {train_acc*100:.2f}%')\r\n",
        "    print (f' Eval loss , {eval_loss:.3f}, eval acc, {eval_acc*100:.2f} %')\r\n",
        "    # print(f'\\t Train. Loss: {train_loss:.3f} |  Train. Acc: {train_acc*100:.2f}%')\r\n",
        "    # print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
      ],
      "execution_count": 394,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:17: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.87      0.92      1219\n",
            "           1       0.03      0.14      0.04        29\n",
            "\n",
            "    accuracy                           0.85      1248\n",
            "   macro avg       0.50      0.50      0.48      1248\n",
            "weighted avg       0.95      0.85      0.90      1248\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.91      0.95       537\n",
            "           1       0.00      0.00      0.00         0\n",
            "\n",
            "    accuracy                           0.91       537\n",
            "   macro avg       0.50      0.45      0.48       537\n",
            "weighted avg       1.00      0.91      0.95       537\n",
            "\n",
            " Epoch Number 0\n",
            " Train. Loss: 0.191 Train. Acc: 85.70%\n",
            " Eval loss , 0.170, eval acc, 90.88 %\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.87      0.93      1248\n",
            "           1       0.00      0.00      0.00         0\n",
            "\n",
            "    accuracy                           0.87      1248\n",
            "   macro avg       0.50      0.44      0.47      1248\n",
            "weighted avg       1.00      0.87      0.93      1248\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.91      0.95       537\n",
            "           1       0.00      0.00      0.00         0\n",
            "\n",
            "    accuracy                           0.91       537\n",
            "   macro avg       0.50      0.45      0.48       537\n",
            "weighted avg       1.00      0.91      0.95       537\n",
            "\n",
            " Epoch Number 1\n",
            " Train. Loss: 0.188 Train. Acc: 87.34%\n",
            " Eval loss , 0.168, eval acc, 90.88 %\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.87      0.93      1248\n",
            "           1       0.00      0.00      0.00         0\n",
            "\n",
            "    accuracy                           0.87      1248\n",
            "   macro avg       0.50      0.44      0.47      1248\n",
            "weighted avg       1.00      0.87      0.93      1248\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.91      0.95       537\n",
            "           1       0.00      0.00      0.00         0\n",
            "\n",
            "    accuracy                           0.91       537\n",
            "   macro avg       0.50      0.45      0.48       537\n",
            "weighted avg       1.00      0.91      0.95       537\n",
            "\n",
            " Epoch Number 2\n",
            " Train. Loss: 0.189 Train. Acc: 87.34%\n",
            " Eval loss , 0.165, eval acc, 90.88 %\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.87      0.93      1248\n",
            "           1       0.00      0.00      0.00         0\n",
            "\n",
            "    accuracy                           0.87      1248\n",
            "   macro avg       0.50      0.44      0.47      1248\n",
            "weighted avg       1.00      0.87      0.93      1248\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.91      0.95       537\n",
            "           1       0.00      0.00      0.00         0\n",
            "\n",
            "    accuracy                           0.91       537\n",
            "   macro avg       0.50      0.45      0.48       537\n",
            "weighted avg       1.00      0.91      0.95       537\n",
            "\n",
            " Epoch Number 3\n",
            " Train. Loss: 0.185 Train. Acc: 87.34%\n",
            " Eval loss , 0.164, eval acc, 90.88 %\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.88      0.93      1222\n",
            "           1       0.05      0.31      0.09        26\n",
            "\n",
            "    accuracy                           0.86      1248\n",
            "   macro avg       0.52      0.59      0.51      1248\n",
            "weighted avg       0.96      0.86      0.91      1248\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.91      0.91       480\n",
            "           1       0.20      0.18      0.19        57\n",
            "\n",
            "    accuracy                           0.84       537\n",
            "   macro avg       0.55      0.55      0.55       537\n",
            "weighted avg       0.83      0.84      0.83       537\n",
            "\n",
            " Epoch Number 4\n",
            " Train. Loss: 0.184 Train. Acc: 86.56%\n",
            " Eval loss , 0.165, eval acc, 84.45 %\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.88      0.91      1166\n",
            "           1       0.12      0.23      0.16        82\n",
            "\n",
            "    accuracy                           0.84      1248\n",
            "   macro avg       0.53      0.56      0.53      1248\n",
            "weighted avg       0.89      0.84      0.86      1248\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.91      0.93       499\n",
            "           1       0.16      0.21      0.18        38\n",
            "\n",
            "    accuracy                           0.86       537\n",
            "   macro avg       0.55      0.56      0.55       537\n",
            "weighted avg       0.88      0.86      0.87       537\n",
            "\n",
            " Epoch Number 5\n",
            " Train. Loss: 0.181 Train. Acc: 83.91%\n",
            " Eval loss , 0.155, eval acc, 87.06 %\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.90      0.88      1058\n",
            "           1       0.31      0.26      0.29       190\n",
            "\n",
            "    accuracy                           0.80      1248\n",
            "   macro avg       0.59      0.58      0.58      1248\n",
            "weighted avg       0.79      0.80      0.79      1248\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.92      0.90       465\n",
            "           1       0.25      0.18      0.21        72\n",
            "\n",
            "    accuracy                           0.82       537\n",
            "   macro avg       0.57      0.55      0.55       537\n",
            "weighted avg       0.79      0.82      0.81       537\n",
            "\n",
            " Epoch Number 6\n",
            " Train. Loss: 0.168 Train. Acc: 80.31%\n",
            " Eval loss , 0.163, eval acc, 82.89 %\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.89      0.89      1080\n",
            "           1       0.29      0.27      0.28       168\n",
            "\n",
            "    accuracy                           0.81      1248\n",
            "   macro avg       0.59      0.58      0.59      1248\n",
            "weighted avg       0.81      0.81      0.81      1248\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.91      0.90       474\n",
            "           1       0.20      0.16      0.18        63\n",
            "\n",
            "    accuracy                           0.82       537\n",
            "   macro avg       0.54      0.54      0.54       537\n",
            "weighted avg       0.81      0.82      0.82       537\n",
            "\n",
            " Epoch Number 7\n",
            " Train. Loss: 0.177 Train. Acc: 81.41%\n",
            " Eval loss , 0.157, eval acc, 83.41 %\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.90      0.88      1052\n",
            "           1       0.31      0.26      0.28       196\n",
            "\n",
            "    accuracy                           0.79      1248\n",
            "   macro avg       0.59      0.58      0.58      1248\n",
            "weighted avg       0.78      0.79      0.79      1248\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.92      0.87       439\n",
            "           1       0.29      0.15      0.20        98\n",
            "\n",
            "    accuracy                           0.78       537\n",
            "   macro avg       0.56      0.54      0.54       537\n",
            "weighted avg       0.73      0.78      0.75       537\n",
            "\n",
            " Epoch Number 8\n",
            " Train. Loss: 0.172 Train. Acc: 79.84%\n",
            " Eval loss , 0.160, eval acc, 79.07 %\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.89      0.88      1063\n",
            "           1       0.28      0.24      0.26       185\n",
            "\n",
            "    accuracy                           0.80      1248\n",
            "   macro avg       0.58      0.57      0.57      1248\n",
            "weighted avg       0.78      0.80      0.79      1248\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.91      0.87       442\n",
            "           1       0.25      0.14      0.18        95\n",
            "\n",
            "    accuracy                           0.78       537\n",
            "   macro avg       0.54      0.53      0.52       537\n",
            "weighted avg       0.73      0.78      0.75       537\n",
            "\n",
            " Epoch Number 9\n",
            " Train. Loss: 0.173 Train. Acc: 79.92%\n",
            " Eval loss , 0.158, eval acc, 78.90 %\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.89      0.88      1052\n",
            "           1       0.31      0.25      0.28       196\n",
            "\n",
            "    accuracy                           0.79      1248\n",
            "   macro avg       0.59      0.57      0.58      1248\n",
            "weighted avg       0.78      0.79      0.78      1248\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.92      0.89       457\n",
            "           1       0.25      0.16      0.20        80\n",
            "\n",
            "    accuracy                           0.80       537\n",
            "   macro avg       0.56      0.54      0.54       537\n",
            "weighted avg       0.77      0.80      0.79       537\n",
            "\n",
            " Epoch Number 10\n",
            " Train. Loss: 0.166 Train. Acc: 79.69%\n",
            " Eval loss , 0.155, eval acc, 81.50 %\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.75      0.93      0.83       870\n",
            "           1       0.64      0.27      0.38       378\n",
            "\n",
            "    accuracy                           0.73      1248\n",
            "   macro avg       0.70      0.60      0.61      1248\n",
            "weighted avg       0.72      0.73      0.69      1248\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.92      0.90       467\n",
            "           1       0.24      0.17      0.20        70\n",
            "\n",
            "    accuracy                           0.82       537\n",
            "   macro avg       0.56      0.54      0.55       537\n",
            "weighted avg       0.80      0.82      0.81       537\n",
            "\n",
            " Epoch Number 11\n",
            " Train. Loss: 0.166 Train. Acc: 73.91%\n",
            " Eval loss , 0.164, eval acc, 82.89 %\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.91      0.87      1002\n",
            "           1       0.44      0.28      0.34       246\n",
            "\n",
            "    accuracy                           0.79      1248\n",
            "   macro avg       0.64      0.60      0.61      1248\n",
            "weighted avg       0.76      0.79      0.77      1248\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.91      0.87       447\n",
            "           1       0.24      0.13      0.17        90\n",
            "\n",
            "    accuracy                           0.78       537\n",
            "   macro avg       0.54      0.52      0.52       537\n",
            "weighted avg       0.74      0.78      0.76       537\n",
            "\n",
            " Epoch Number 12\n",
            " Train. Loss: 0.163 Train. Acc: 79.06%\n",
            " Eval loss , 0.165, eval acc, 79.42 %\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.92      0.86       967\n",
            "           1       0.49      0.28      0.36       281\n",
            "\n",
            "    accuracy                           0.77      1248\n",
            "   macro avg       0.65      0.60      0.61      1248\n",
            "weighted avg       0.74      0.77      0.75      1248\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.92      0.89       454\n",
            "           1       0.25      0.16      0.19        83\n",
            "\n",
            "    accuracy                           0.80       537\n",
            "   macro avg       0.56      0.54      0.54       537\n",
            "weighted avg       0.76      0.80      0.78       537\n",
            "\n",
            " Epoch Number 13\n",
            " Train. Loss: 0.160 Train. Acc: 77.73%\n",
            " Eval loss , 0.161, eval acc, 80.98 %\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.92      0.85       929\n",
            "           1       0.54      0.27      0.36       319\n",
            "\n",
            "    accuracy                           0.76      1248\n",
            "   macro avg       0.67      0.60      0.61      1248\n",
            "weighted avg       0.72      0.76      0.72      1248\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.92      0.89       463\n",
            "           1       0.24      0.16      0.19        74\n",
            "\n",
            "    accuracy                           0.81       537\n",
            "   macro avg       0.55      0.54      0.54       537\n",
            "weighted avg       0.78      0.81      0.80       537\n",
            "\n",
            " Epoch Number 14\n",
            " Train. Loss: 0.159 Train. Acc: 76.02%\n",
            " Eval loss , 0.163, eval acc, 82.19 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1rtD7Guh-g5"
      },
      "source": [
        "why is the loss negative here, did I mess up with the NLLloss? I passed the softmax into the NLLloss correctly ya? \r\n",
        "Ah, because i used softmax, it should be log_softmax instead. \r\n",
        "\r\n",
        " what about the actual result here. maybe try a few more epochs? \r\n",
        "\r\n",
        " The results is not exactly stable ?\r\n",
        " It's actually a common theme, throughout all the 4 models though .\r\n",
        "\r\n",
        " Hard to say, the recall of true label is not very good in evaluation set, although the accuracy is going up. The recall rate in trainning set seems decent though. ummm. \r\n",
        "\r\n",
        " What if I want to focus on the recall rate, maybe the focal loss could come into play here ???? maybe it can address the class imbalance better than over/undersampling? "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oEFnPR3BiFYw"
      },
      "source": [
        ""
      ],
      "execution_count": 394,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6pN-Is6Lv5OE"
      },
      "source": [
        "# Torch Model 5 - Feedforward with Focal Loss\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Emj_r3CNwBzL"
      },
      "source": [
        "Now that I understand a little bit more about Cross-entropy and the concept of focal loss, let me try it out on the most basic network and see if it works. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJhkY-kkwAlO"
      },
      "source": [
        "#first, define focal loss?\r\n",
        "#there are a few variations...let's try "
      ],
      "execution_count": 294,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZxNAbE63TNq"
      },
      "source": [
        "class FocalLoss(nn.Module):\r\n",
        "    def __init__(self, gamma=0, alpha=None, size_average=True):\r\n",
        "        super(FocalLoss, self).__init__()\r\n",
        "        self.gamma = gamma\r\n",
        "        self.alpha = alpha\r\n",
        "        if isinstance(alpha,(float,int)): self.alpha = torch.Tensor([alpha,1-alpha])\r\n",
        "        if isinstance(alpha,list): self.alpha = torch.Tensor(alpha)\r\n",
        "        self.size_average = size_average\r\n",
        "\r\n",
        "    def forward(self, input, target):\r\n",
        "        if input.dim()>2:\r\n",
        "            input = input.view(input.size(0),input.size(1),-1)  # N,C,H,W => N,C,H*W\r\n",
        "            input = input.transpose(1,2)    # N,C,H*W => N,H*W,C\r\n",
        "            input = input.contiguous().view(-1,input.size(2))   # N,H*W,C => N*H*W,C\r\n",
        "        target = target.view(-1,1)\r\n",
        "\r\n",
        "        logpt = F.log_softmax(input)\r\n",
        "        logpt = logpt.gather(1,target)\r\n",
        "        logpt = logpt.view(-1)\r\n",
        "        pt = Variable(logpt.data.exp())\r\n",
        "\r\n",
        "        if self.alpha is not None:\r\n",
        "            if self.alpha.type()!=input.data.type():\r\n",
        "                self.alpha = self.alpha.type_as(input.data)\r\n",
        "            at = self.alpha.gather(0,target.data.view(-1))\r\n",
        "            logpt = logpt * Variable(at)\r\n",
        "\r\n",
        "        loss = -1 * (1-pt)**self.gamma * logpt\r\n",
        "        if self.size_average: return loss.mean()\r\n",
        "        else: return loss.sum()"
      ],
      "execution_count": 295,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oUyoMJCYpPEL"
      },
      "source": [
        "loss_fn = FocalLoss(gamma=0, alpha=0.2)\r\n"
      ],
      "execution_count": 296,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVSyzRMwpZo2"
      },
      "source": [
        "#let's first try a simple FC NN\r\n",
        "class FFNN (nn.Module):\r\n",
        "    def __init__(self, input_size, num_classes,num_hidden, hidden_dim ):\r\n",
        "        super().__init__()\r\n",
        "\r\n",
        "        assert num_hidden > 0 \r\n",
        "\r\n",
        "        self.fc1 = nn.Linear(input_size,hidden_dim)\r\n",
        "        self.fc2 = nn.Linear(hidden_dim,hidden_dim)\r\n",
        "\r\n",
        "        self.hidden_layers = nn.ModuleList([])\r\n",
        "        self.hidden_layers.append (self.fc1)\r\n",
        "        for i in range (num_hidden -1 ):\r\n",
        "            self.hidden_layers.append(self.fc2)\r\n",
        "        \r\n",
        "        self.final_layer = nn.Linear (hidden_dim,num_classes)\r\n",
        "        self.dropout = nn.Dropout()\r\n",
        "        self.relu = nn.ReLU()\r\n",
        "\r\n",
        "    def forward(self,x):\r\n",
        "        for hidden_layer in self.hidden_layers:\r\n",
        "            x = hidden_layer(x)\r\n",
        "            x = self.dropout(x)\r\n",
        "            x = self.relu(x)\r\n",
        "        \r\n",
        "        out = self.final_layer(x)\r\n",
        "\r\n",
        "        return out\r\n"
      ],
      "execution_count": 297,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TEfWUpxVp9fD"
      },
      "source": [
        "#try initiating the model\r\n",
        "INPUT_SIZE = 19\r\n",
        "NUM_CLASSES = 2\r\n",
        "NUM_HIDDEN = 2\r\n",
        "HIDDEN_DIM = 512\r\n",
        "\r\n",
        "model = FFNN(INPUT_SIZE, NUM_CLASSES, NUM_HIDDEN, HIDDEN_DIM)"
      ],
      "execution_count": 298,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sAkvOEmWqBWz"
      },
      "source": [
        "#define loss fuction and optimizer\r\n",
        "optimizer = optim.Adam(model.parameters())\r\n"
      ],
      "execution_count": 299,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I3nNaR4eqHJ_"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\n",
        "model = model.to(device)\r\n",
        "loss_fn = loss_fn.to(device)"
      ],
      "execution_count": 300,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dqbOEELoqJp-"
      },
      "source": [
        "#define a function to calcualte prediction accuracy\r\n",
        "def accuracy(pred,label):\r\n",
        "    _,pred_label = torch.max(pred,dim=1)\r\n",
        "    correct = (pred_label==label).float()\r\n",
        "    accuracy = correct.sum()/len(correct)\r\n",
        "    return accuracy\r\n",
        "\r\n",
        "#can i define a function to calculate the f1 score/confusion matrix? \r\n",
        "def get_cfm (pred, label):\r\n",
        "    _,pred_label = torch.max(pred,dim=1)\r\n",
        "    m = classification_report(pred_label.cpu(), label.cpu())\r\n",
        "    return m\r\n",
        "\r\n",
        "\r\n",
        "    "
      ],
      "execution_count": 301,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q50awQL1qSm-"
      },
      "source": [
        "#now define the training and evaluation loop\r\n",
        "def train(model, dataloader, optimizer, loss_fn):\r\n",
        "    model.train()\r\n",
        "\r\n",
        "    epoch_loss= 0\r\n",
        "    epoch_acc = 0\r\n",
        "\r\n",
        "    for step,batch in enumerate(dataloader):\r\n",
        "        optimizer.zero_grad()\r\n",
        "\r\n",
        "        data, label = (t for t in batch)\r\n",
        "        data = data.to(device)\r\n",
        "        label = label.to(device)\r\n",
        "\r\n",
        "        pred = model(data)\r\n",
        "        loss = loss_fn(pred,label)\r\n",
        "        acc  = accuracy(pred,label)\r\n",
        "\r\n",
        "        loss.backward()\r\n",
        "        optimizer.step()\r\n",
        "\r\n",
        "        epoch_loss += loss.item()\r\n",
        "        epoch_acc += acc\r\n",
        "\r\n",
        "    return epoch_loss/len(dataloader), epoch_acc/len(dataloader)\r\n",
        "        "
      ],
      "execution_count": 302,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O0-LO9tVqVa0"
      },
      "source": [
        "def evaluate(model,dataloader,loss_fn):\r\n",
        "    model.eval()\r\n",
        "\r\n",
        "    epoch_loss = 0 \r\n",
        "    epoch_acc = 0 \r\n",
        "\r\n",
        "    pred_list =[]\r\n",
        "    label_list =[]\r\n",
        "    for step,batch in enumerate(dataloader):\r\n",
        "        data,label = (t for t in batch)\r\n",
        "        data = data.to(device)\r\n",
        "        label = label.to(device)\r\n",
        "\r\n",
        "        pred = model(data)\r\n",
        "        loss = loss_fn(pred,label)\r\n",
        "        acc = accuracy(pred,label)\r\n",
        "        \r\n",
        "        #append the prediction result and show it per epoch later. \r\n",
        "        if step ==0:     \r\n",
        "            pred_list=pred.cpu().detach().numpy()\r\n",
        "            label_list = label.cpu().detach().numpy()\r\n",
        "        if step != 0:\r\n",
        "            pred_list = np.concatenate([pred_list,pred.cpu().detach().numpy()],axis =0)\r\n",
        "            label_list = np.concatenate([label_list,label.cpu().detach().numpy()],axis =0)\r\n",
        "\r\n",
        "        epoch_loss += loss.item()\r\n",
        "        epoch_acc += acc\r\n",
        "\r\n",
        "    report = get_cfm(torch.tensor(pred_list),torch.tensor(label_list))\r\n",
        "    print (report)\r\n",
        "    return epoch_loss/len(dataloader), epoch_acc/len(dataloader)\r\n",
        "        "
      ],
      "execution_count": 303,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MWlqJYPHqXrY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4baefe90-55b0-424e-a113-6a0ee50b729c"
      },
      "source": [
        "N_EPOCHS = 20\r\n",
        "\r\n",
        "for i in range(N_EPOCHS):\r\n",
        "    train_loss,train_acc = train(model,train_loader, optimizer, loss_fn)\r\n",
        "    \r\n",
        "    eval_loss, eval_acc = evaluate(model,eval_loader, loss_fn)\r\n",
        "    \r\n",
        "    print (f' Epoch Number {i}') \r\n",
        "    print (f' Train. Loss: {train_loss:.3f} Train. Acc: {train_acc*100:.2f}%')\r\n",
        "    print (f' Eval loss , {eval_loss:.3f}, eval acc, {eval_acc*100:.2f} %')\r\n",
        "    # print(f'\\t Train. Loss: {train_loss:.3f} |  Train. Acc: {train_acc*100:.2f}%')\r\n",
        "    # print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
      ],
      "execution_count": 304,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:17: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.91      0.95       547\n",
            "           1       0.00      0.00      0.00         0\n",
            "\n",
            "    accuracy                           0.91       547\n",
            "   macro avg       0.50      0.45      0.48       547\n",
            "weighted avg       1.00      0.91      0.95       547\n",
            "\n",
            " Epoch Number 0\n",
            " Train. Loss: 0.210 Train. Acc: 83.39%\n",
            " Eval loss , 0.156, eval acc, 90.71 %\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.92      0.92       498\n",
            "           1       0.18      0.18      0.18        49\n",
            "\n",
            "    accuracy                           0.85       547\n",
            "   macro avg       0.55      0.55      0.55       547\n",
            "weighted avg       0.85      0.85      0.85       547\n",
            "\n",
            " Epoch Number 1\n",
            " Train. Loss: 0.183 Train. Acc: 82.53%\n",
            " Eval loss , 0.164, eval acc, 85.19 %\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.91      0.92       502\n",
            "           1       0.14      0.16      0.15        45\n",
            "\n",
            "    accuracy                           0.85       547\n",
            "   macro avg       0.53      0.53      0.53       547\n",
            "weighted avg       0.86      0.85      0.85       547\n",
            "\n",
            " Epoch Number 2\n",
            " Train. Loss: 0.181 Train. Acc: 84.68%\n",
            " Eval loss , 0.154, eval acc, 85.19 %\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.92      0.92       498\n",
            "           1       0.18      0.18      0.18        49\n",
            "\n",
            "    accuracy                           0.85       547\n",
            "   macro avg       0.55      0.55      0.55       547\n",
            "weighted avg       0.85      0.85      0.85       547\n",
            "\n",
            " Epoch Number 3\n",
            " Train. Loss: 0.174 Train. Acc: 83.93%\n",
            " Eval loss , 0.151, eval acc, 85.19 %\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.92      0.92       495\n",
            "           1       0.20      0.19      0.19        52\n",
            "\n",
            "    accuracy                           0.85       547\n",
            "   macro avg       0.56      0.55      0.56       547\n",
            "weighted avg       0.85      0.85      0.85       547\n",
            "\n",
            " Epoch Number 4\n",
            " Train. Loss: 0.170 Train. Acc: 83.03%\n",
            " Eval loss , 0.148, eval acc, 85.01 %\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.92      0.91       482\n",
            "           1       0.24      0.18      0.21        65\n",
            "\n",
            "    accuracy                           0.83       547\n",
            "   macro avg       0.56      0.55      0.56       547\n",
            "weighted avg       0.81      0.83      0.82       547\n",
            "\n",
            " Epoch Number 5\n",
            " Train. Loss: 0.165 Train. Acc: 82.21%\n",
            " Eval loss , 0.147, eval acc, 83.45 %\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.92      0.90       474\n",
            "           1       0.29      0.21      0.24        73\n",
            "\n",
            "    accuracy                           0.83       547\n",
            "   macro avg       0.59      0.56      0.57       547\n",
            "weighted avg       0.80      0.83      0.81       547\n",
            "\n",
            " Epoch Number 6\n",
            " Train. Loss: 0.165 Train. Acc: 81.86%\n",
            " Eval loss , 0.146, eval acc, 83.11 %\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.93      0.90       471\n",
            "           1       0.31      0.21      0.25        76\n",
            "\n",
            "    accuracy                           0.83       547\n",
            "   macro avg       0.60      0.57      0.58       547\n",
            "weighted avg       0.80      0.83      0.81       547\n",
            "\n",
            " Epoch Number 7\n",
            " Train. Loss: 0.158 Train. Acc: 79.72%\n",
            " Eval loss , 0.146, eval acc, 82.93 %\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.92      0.90       466\n",
            "           1       0.31      0.20      0.24        81\n",
            "\n",
            "    accuracy                           0.82       547\n",
            "   macro avg       0.59      0.56      0.57       547\n",
            "weighted avg       0.79      0.82      0.80       547\n",
            "\n",
            " Epoch Number 8\n",
            " Train. Loss: 0.158 Train. Acc: 80.65%\n",
            " Eval loss , 0.146, eval acc, 82.06 %\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.93      0.89       461\n",
            "           1       0.33      0.20      0.25        86\n",
            "\n",
            "    accuracy                           0.81       547\n",
            "   macro avg       0.60      0.56      0.57       547\n",
            "weighted avg       0.78      0.81      0.79       547\n",
            "\n",
            " Epoch Number 9\n",
            " Train. Loss: 0.156 Train. Acc: 80.97%\n",
            " Eval loss , 0.146, eval acc, 81.69 %\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.93      0.89       461\n",
            "           1       0.33      0.20      0.25        86\n",
            "\n",
            "    accuracy                           0.81       547\n",
            "   macro avg       0.60      0.56      0.57       547\n",
            "weighted avg       0.78      0.81      0.79       547\n",
            "\n",
            " Epoch Number 10\n",
            " Train. Loss: 0.153 Train. Acc: 78.59%\n",
            " Eval loss , 0.147, eval acc, 81.69 %\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.92      0.89       462\n",
            "           1       0.31      0.19      0.24        85\n",
            "\n",
            "    accuracy                           0.81       547\n",
            "   macro avg       0.59      0.56      0.56       547\n",
            "weighted avg       0.78      0.81      0.79       547\n",
            "\n",
            " Epoch Number 11\n",
            " Train. Loss: 0.149 Train. Acc: 79.84%\n",
            " Eval loss , 0.147, eval acc, 81.51 %\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.93      0.89       459\n",
            "           1       0.35      0.20      0.26        88\n",
            "\n",
            "    accuracy                           0.81       547\n",
            "   macro avg       0.61      0.57      0.58       547\n",
            "weighted avg       0.78      0.81      0.79       547\n",
            "\n",
            " Epoch Number 12\n",
            " Train. Loss: 0.154 Train. Acc: 77.65%\n",
            " Eval loss , 0.146, eval acc, 81.69 %\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.93      0.89       457\n",
            "           1       0.33      0.19      0.24        90\n",
            "\n",
            "    accuracy                           0.80       547\n",
            "   macro avg       0.59      0.56      0.56       547\n",
            "weighted avg       0.77      0.80      0.78       547\n",
            "\n",
            " Epoch Number 13\n",
            " Train. Loss: 0.149 Train. Acc: 79.09%\n",
            " Eval loss , 0.147, eval acc, 80.99 %\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.93      0.89       459\n",
            "           1       0.33      0.19      0.24        88\n",
            "\n",
            "    accuracy                           0.81       547\n",
            "   macro avg       0.60      0.56      0.57       547\n",
            "weighted avg       0.77      0.81      0.79       547\n",
            "\n",
            " Epoch Number 14\n",
            " Train. Loss: 0.152 Train. Acc: 79.01%\n",
            " Eval loss , 0.147, eval acc, 81.34 %\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.93      0.89       462\n",
            "           1       0.33      0.20      0.25        85\n",
            "\n",
            "    accuracy                           0.81       547\n",
            "   macro avg       0.60      0.56      0.57       547\n",
            "weighted avg       0.78      0.81      0.79       547\n",
            "\n",
            " Epoch Number 15\n",
            " Train. Loss: 0.153 Train. Acc: 78.12%\n",
            " Eval loss , 0.146, eval acc, 81.86 %\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.93      0.89       458\n",
            "           1       0.33      0.19      0.24        89\n",
            "\n",
            "    accuracy                           0.81       547\n",
            "   macro avg       0.59      0.56      0.57       547\n",
            "weighted avg       0.77      0.81      0.78       547\n",
            "\n",
            " Epoch Number 16\n",
            " Train. Loss: 0.154 Train. Acc: 78.47%\n",
            " Eval loss , 0.147, eval acc, 81.17 %\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.93      0.89       460\n",
            "           1       0.33      0.20      0.25        87\n",
            "\n",
            "    accuracy                           0.81       547\n",
            "   macro avg       0.60      0.56      0.57       547\n",
            "weighted avg       0.78      0.81      0.79       547\n",
            "\n",
            " Epoch Number 17\n",
            " Train. Loss: 0.152 Train. Acc: 77.18%\n",
            " Eval loss , 0.147, eval acc, 81.51 %\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.93      0.89       458\n",
            "           1       0.33      0.19      0.24        89\n",
            "\n",
            "    accuracy                           0.81       547\n",
            "   macro avg       0.59      0.56      0.57       547\n",
            "weighted avg       0.77      0.81      0.78       547\n",
            "\n",
            " Epoch Number 18\n",
            " Train. Loss: 0.152 Train. Acc: 78.55%\n",
            " Eval loss , 0.147, eval acc, 81.17 %\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.93      0.89       460\n",
            "           1       0.33      0.20      0.25        87\n",
            "\n",
            "    accuracy                           0.81       547\n",
            "   macro avg       0.60      0.56      0.57       547\n",
            "weighted avg       0.78      0.81      0.79       547\n",
            "\n",
            " Epoch Number 19\n",
            " Train. Loss: 0.152 Train. Acc: 78.24%\n",
            " Eval loss , 0.147, eval acc, 81.51 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XZJe2EJ_tOT8",
        "outputId": "1b48a27f-00c4-4693-86ad-fcfbf1f7e3d1"
      },
      "source": [
        "a = torch.range(1,6)"
      ],
      "execution_count": 305,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tmoM5BBvtU0q",
        "outputId": "20d6cb67-e5eb-4e60-8d63-4a8494d536b2"
      },
      "source": [
        "a"
      ],
      "execution_count": 306,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1., 2., 3., 4., 5., 6.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 306
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FxRsRxnZtQmL"
      },
      "source": [
        ""
      ],
      "execution_count": 306,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCU6zdnstXVH"
      },
      "source": [
        ""
      ],
      "execution_count": 306,
      "outputs": []
    }
  ]
}